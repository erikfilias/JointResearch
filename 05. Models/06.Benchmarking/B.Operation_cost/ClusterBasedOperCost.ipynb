{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook generate the line benefits from the operational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "# from kneed import KneeLocator\n",
    "from pyomo.environ      import *\n",
    "from pyomo.opt          import SolverFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Defining the clustering functions\n",
    "def KMeansMethod(OptClusters, Y_sklearn, _path_0, _path_1, CaseName_0, CaseName_1, table, data):\n",
    "    # Running the K-means with the optimal number of clusters. Setting up the initializer and random state.\n",
    "    kmeans_pca = KMeans(n_clusters=OptClusters, init='k-means++', random_state=42)\n",
    "    kmeans_pca.fit(Y_sklearn)\n",
    "    df_segm_pca_kmeans = pd.concat([table.reset_index(drop=True), pd.DataFrame(Y_sklearn)], axis=1)\n",
    "    df_segm_pca_kmeans.columns.values[-3:] = ['Component 1', 'Component 2', 'Component 3']\n",
    "    df_segm_pca_kmeans['Segment K-means PCA'] = kmeans_pca.labels_\n",
    "    # Saving the cluster on the NetworkCommitment CSV file\n",
    "    df_segm_pca_kmeans.to_csv(_path_0 + '/oT_Result_NetworkCommitment_ReducedCost_Clusters_kmeans_' + CaseName_0 + '.csv', sep=',')\n",
    "    # Storing clusters in the first table\n",
    "    table['Segment K-means PCA'] = kmeans_pca.labels_\n",
    "    table = table.reset_index()\n",
    "    table = table.set_index(['Scenario', 'Period', 'Day', 'Month', 'Segment K-means PCA'])\n",
    "    # Stacking the table to also have the lines as index\n",
    "    df = table.stack()\n",
    "    df = df.reset_index()\n",
    "    # Adding a new column with the cluster for each LoadLevel\n",
    "    data['Segment K-means PCA'] = np.where(data['Variable'] == df['Variable'],\n",
    "                                                    df['Segment K-means PCA'], df['Segment K-means PCA'])\n",
    "    # Adding the duration to each LoadLevel\n",
    "    data['Duration'] = 1\n",
    "    # Renaming the cluster with respective name adopting in openTEPES project\n",
    "    data['Stage'] = data['Segment K-means PCA'].map({0: 'st1', 1: 'st2', 2: 'st3', 3: 'st4', 4: 'st5',\n",
    "                                                                       5: 'st6', 6: 'st7', 7: 'st8', 8: 'st9',\n",
    "                                                                       9: 'st10',\n",
    "                                                                       10: 'st11', 11: 'st12', 12: 'st13', 13: 'st14',\n",
    "                                                                       14: 'st15',\n",
    "                                                                       15: 'st16', 16: 'st17', 17: 'st18', 18: 'st19',\n",
    "                                                                       19: 'st20',\n",
    "                                                                       20: 'st21', 21: 'st22', 22: 'st23', 23: 'st24',\n",
    "                                                                       24: 'st25',\n",
    "                                                                       25: 'st26', 26: 'st27', 27: 'st28', 28: 'st29',\n",
    "                                                                       29: 'st30',\n",
    "                                                                       30: 'st31', 31: 'st32', 32: 'st33', 33: 'st34',\n",
    "                                                                       34: 'st35',\n",
    "                                                                       35: 'st36', 36: 'st37', 37: 'st38', 38: 'st39',\n",
    "                                                                       39: 'st40',\n",
    "                                                                       40: 'st41', 41: 'st42', 42: 'st43', 43: 'st44',\n",
    "                                                                       44: 'st45',\n",
    "                                                                       45: 'st46', 46: 'st47', 47: 'st48', 48: 'st49',\n",
    "                                                                       49: 'st50'})\n",
    "    # Getting only the relevant information to build the new CSV file in CaseName_ByStages\n",
    "    data = data[\n",
    "        ['Scenario', 'Period', 'LoadLevel', 'Stage', 'InitialNode', 'FinalNode', 'Circuit', 'Value']]\n",
    "    data.to_csv(_path_0 + '/oT_Test_ReducedCost_Clusters_' + CaseName_0 + '.csv', sep=',')\n",
    "    # Shaping the dataframe to be saved in CSV files\n",
    "    TableToFile = pd.pivot_table(data, values='Value', index=['LoadLevel', 'Stage', 'Duration'],\n",
    "                                 columns=['InitialNode', 'FinalNode', 'Circuit'], fill_value=0)\n",
    "    TableToFile = TableToFile.reset_index()\n",
    "    # TableToFile['Duration'] = 1\n",
    "    # Creating the dataframe to generate oT_Data_Duration\n",
    "    dfDuration = pd.DataFrame(0, index=TableToFile.index, columns=['LoadLevel', 'Duration', 'Stage'])\n",
    "    dfDuration['LoadLevel'] = TableToFile['LoadLevel']\n",
    "    dfDuration['Duration'] = TableToFile['Duration']\n",
    "    dfDuration['Stage'] = TableToFile['Stage']\n",
    "    dfDuration.to_csv(_path_1 + '/oT_Data_Duration_' + CaseName_1 + '.csv', sep=',', index=False)\n",
    "    # Identifying the Stages\n",
    "    Stages = dfDuration.Stage.unique()\n",
    "    Stages = np.sort(Stages)\n",
    "    # Creating the dataframe to generate oT_Data_Stages\n",
    "    dfa = pd.DataFrame({'Weight': dfDuration['Stage']})\n",
    "    dfa = dfa['Weight'].value_counts()\n",
    "    dfa = dfa / 24\n",
    "    dfa = dfa.sort_index()\n",
    "    dfStages = pd.DataFrame(dfa.values, index=dfa.index, columns=['Weight'])\n",
    "    dfStages.to_csv(_path_1 + '/oT_Data_Stage_' + CaseName_1 + '.csv', sep=',')\n",
    "    # Creating the dataframe to generate oT_Dict_Stages\n",
    "    dict_Stages = pd.DataFrame(Stages, columns=['Stage'])\n",
    "    dict_Stages.to_csv(_path_1 + '/oT_Dict_Stage_' + CaseName_1 + '.csv', sep=',', index=False)\n",
    "\n",
    "\n",
    "def KMedoidsMethod(OptClusters, Y_sklearn, _path_0, _path_1, CaseName_0, CaseName_1, table, data):\n",
    "    # Running the K-means with the optimal number of clusters. Setting up the initializer and random state.\n",
    "    kmedoids_pca = KMedoids(metric=\"euclidean\", n_clusters=OptClusters, init=\"heuristic\", max_iter=2, random_state=42)\n",
    "    kmedoids_pca.fit(Y_sklearn)\n",
    "    df_segm_pca_kmedoids = pd.concat([table.reset_index(drop=True), pd.DataFrame(Y_sklearn)], axis=1)\n",
    "    df_segm_pca_kmedoids.columns.values[-3:] = ['Component 1', 'Component 2', 'Component 3']\n",
    "    df_segm_pca_kmedoids['Segment K-medoids PCA'] = kmedoids_pca.labels_\n",
    "    # Saving the cluster on the NetworkCommitment CSV file\n",
    "    df_segm_pca_kmedoids.to_csv(_path_0 + '/oT_Result_NetworkCommitment_ReducedCost_Clusters_kmedoids_' + CaseName_0 + '.csv', sep=',')\n",
    "    # Storing clusters in the first table\n",
    "    table['Segment K-medoids PCA'] = kmedoids_pca.labels_\n",
    "    table = table.reset_index()\n",
    "    table = table.set_index(['Day', 'Month', 'Segment K-medoids PCA'])\n",
    "    # Stacking the table to also have the lines as index\n",
    "    df = table.stack()\n",
    "    df = df.reset_index()\n",
    "    # Adding a new column with the cluster for each LoadLevel\n",
    "    data['Segment K-medoids PCA'] = np.where(data['Variable'] == df['Variable'], df['Segment K-medoids PCA'], df['Segment K-medoids PCA'])\n",
    "    # Adding the duration to each LoadLevel\n",
    "    data['Duration'] = 0\n",
    "    # Renaming the cluster with respective name adopting in openTEPES project\n",
    "    data['Stage'] = data['Segment K-medoids PCA'].map({0: 'st1', 1: 'st2', 2: 'st3', 3: 'st4', 4: 'st5',\n",
    "                                                                         5: 'st6', 6: 'st7', 7: 'st8', 8: 'st9', 9: 'st10',\n",
    "                                                                         10: 'st11', 11: 'st12', 12: 'st13', 13: 'st14', 14: 'st15',\n",
    "                                                                         15: 'st16', 16: 'st17', 17: 'st18', 18: 'st19', 19: 'st20',\n",
    "                                                                         20: 'st21', 21: 'st22', 22: 'st23', 23: 'st24', 24: 'st25',\n",
    "                                                                         25: 'st26', 26: 'st27', 27: 'st28', 28: 'st29', 29: 'st30',\n",
    "                                                                         30: 'st31', 31: 'st32', 32: 'st33', 33: 'st34', 34: 'st35',\n",
    "                                                                         35: 'st36', 36: 'st37', 37: 'st38', 38: 'st39', 39: 'st40',\n",
    "                                                                         40: 'st41', 41: 'st42', 42: 'st43', 43: 'st44', 44: 'st45',\n",
    "                                                                         45: 'st46', 46: 'st47', 47: 'st48', 48: 'st49', 49: 'st50',\n",
    "                                                                         50: 'st51', 51: 'st52', 52: 'st53', 53: 'st54', 54: 'st55',\n",
    "                                                                         55: 'st56', 56: 'st57', 57: 'st58', 58: 'st59', 59: 'st60',\n",
    "                                                                         60: 'st61', 61: 'st62', 62: 'st63', 63: 'st64', 64: 'st65',\n",
    "                                                                         65: 'st66', 66: 'st67', 67: 'st68', 68: 'st69', 69: 'st70',\n",
    "                                                                         70: 'st71', 71: 'st72', 72: 'st73', 73: 'st74', 74: 'st75',\n",
    "                                                                         75: 'st76', 76: 'st77', 77: 'st78', 78: 'st79', 79: 'st80',\n",
    "                                                                         80: 'st81', 81: 'st82', 82: 'st83', 83: 'st84', 84: 'st85',\n",
    "                                                                         85: 'st86', 86: 'st87', 87: 'st88', 88: 'st89', 89: 'st90',\n",
    "                                                                         90: 'st91', 91: 'st92', 92: 'st93', 93: 'st94', 94: 'st95',\n",
    "                                                                         95: 'st96', 96: 'st97', 97: 'st98', 98: 'st99', 99: 'st100',\n",
    "                                                                         100: 'st101', 101: 'st102', 102: 'st103', 103: 'st104', 104: 'st105',\n",
    "                                                                         105: 'st106', 106: 'st107', 107: 'st108', 108: 'st109', 109: 'st110',\n",
    "                                                                         110: 'st111', 111: 'st112', 112: 'st113', 113: 'st114', 114: 'st115',\n",
    "                                                                         115: 'st116', 116: 'st117', 117: 'st118', 118: 'st119', 119: 'st120',\n",
    "                                                                         120: 'st121', 121: 'st122', 122: 'st123', 123: 'st124', 124: 'st125',\n",
    "                                                                         125: 'st126', 126: 'st127', 127: 'st128', 128: 'st129', 129: 'st130',\n",
    "                                                                         130: 'st131', 131: 'st132', 132: 'st133', 133: 'st134', 134: 'st135',\n",
    "                                                                         135: 'st136', 136: 'st137', 137: 'st138', 138: 'st139', 139: 'st140',\n",
    "                                                                         140: 'st141', 141: 'st142', 142: 'st143', 143: 'st144', 144: 'st145',\n",
    "                                                                         145: 'st146', 146: 'st147', 147: 'st148', 148: 'st149', 149: 'st150'})\n",
    "\n",
    "    #\n",
    "    idx = kmedoids_pca.medoid_indices_\n",
    "    dfDayToStage = pd.DataFrame(idx, columns=['Day'])\n",
    "    dfDayToStage = dfDayToStage + 1\n",
    "    for k in dfDayToStage.index:\n",
    "        data.loc[data['Day'] == dfDayToStage['Day'][k], 'Duration'] = 1\n",
    "\n",
    "    # Getting only the relevant information to build the new CSV file in CaseName_ByStages\n",
    "    # data = data[\n",
    "        # ['Scenario', 'Period', 'LoadLevel', 'Stage', 'InitialNode', 'FinalNode', 'Circuit', 'Duration', 'Value']]\n",
    "    data = data[\n",
    "        ['LoadLevel', 'Stage', 'Execution', 'Duration', 'Value']]\n",
    "    data.to_csv(_path_0 + '/oT_Test_ReducedCost_Clusters_' + CaseName_0 + '.csv', sep=',')\n",
    "    # Shaping the dataframe to be saved in CSV files\n",
    "    # TableToFile = pd.pivot_table(data, values='Value', index=['LoadLevel', 'Stage', 'Duration'],\n",
    "    #                              columns=['InitialNode', 'FinalNode', 'Circuit'], fill_value=0)\n",
    "    TableToFile = pd.pivot_table(data, values='Value', index=['LoadLevel', 'Stage', 'Duration'],\n",
    "                                 columns=['Execution'], fill_value=0)\n",
    "    TableToFile = TableToFile.reset_index()\n",
    "    # Creating the dataframe to generate oT_Data_Duration\n",
    "    dfDuration = pd.DataFrame(0, index=TableToFile.index, columns=['LoadLevel', 'Duration', 'Stage'])\n",
    "    dfDuration['LoadLevel'] = TableToFile['LoadLevel']\n",
    "    dfDuration['Duration'] = TableToFile['Duration']\n",
    "    dfDuration['Stage'] = TableToFile['Stage']\n",
    "    dfDuration.to_csv(_path_1 + '/oT_Data_Duration_' + CaseName_1 + '.csv', sep=',', index=False)\n",
    "    # Identifying the Stages\n",
    "    Stages = dfDuration.Stage.unique()\n",
    "    Stages = np.sort(Stages)\n",
    "    # Creating the dataframe to generate oT_Data_Stages\n",
    "    dfa = pd.DataFrame({'Weight': dfDuration['Stage']})\n",
    "    dfa = dfa['Weight'].value_counts()\n",
    "    dfa = dfa/24\n",
    "    dfa = dfa.sort_index()\n",
    "    dfStages = pd.DataFrame(dfa.values, index=dfa.index, columns=['Weight'])\n",
    "    dfStages.to_csv(_path_1 + '/oT_Data_Stage_' + CaseName_1 + '.csv', sep=',')\n",
    "    # Creating the dataframe to generate oT_Dict_Stages\n",
    "    dict_Stages = pd.DataFrame(Stages, columns=['Stage'])\n",
    "    dict_Stages.to_csv(_path_1 + '/oT_Dict_Stage_' + CaseName_1 + '.csv', sep=',', index=False)\n",
    "\n",
    "    return kmedoids_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Setting up the path a cases\n",
    "DirName  = os.getcwd()\n",
    "\n",
    "CSV_name =  'oT_Result_GenerationCost_3-bus'\n",
    "\n",
    "CaseName_Base     = '3-bus'\n",
    "CaseName_ByStages = CaseName_Base + '_ByStages'\n",
    "\n",
    "_path_0 = os.path.join(DirName, CaseName_Base)\n",
    "_path_1 = os.path.join(DirName, CaseName_ByStages)\n",
    "\n",
    "StartTime = time.time()\n",
    "\n",
    "#%% Selecting the maximum number of cluster to plot\n",
    "max_cluster = 300\n",
    "#%% Selecting the optimal number of cluster and defining the clustering method (0: k-means; 1:k-medoids)\n",
    "# opt_cluster = 150\n",
    "cluster_method = 1\n",
    "\n",
    "output_directory = DirName + '/' + CaseName_ByStages + '/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(CSV_name+'.csv', index_col=0)\n",
    "diff_df_1 = df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading Sets from CSV\n",
    "dictSets = DataPortal()\n",
    "dictSets.load(filename=_path_0+'/1.Set'+'/oT_Dict_LoadLevel_'   +CaseName_Base+'.csv', set='n'   , format='set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df_1['LoadLevel'] = dictSets['n' ]\n",
    "diff_df_1.set_index('LoadLevel', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the columns demand, wind, solar, hydro\n",
    "diff_df_1 = diff_df_1[['MEUR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1 = diff_df_1.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1.index.names = ['LoadLevel', 'Execution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the column name\n",
    "ddf_1 = ddf_1.to_frame(name='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1 = ddf_1.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1['Date'] = ddf_1['LoadLevel']\n",
    "ddf_1['Date'] = ddf_1['Date'].str.slice(0, -6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1['Date'] = pd.to_datetime(ddf_1['Date'], format='%m-%d %H:%M:%S', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting day of year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1['Hour' ] = ddf_1['Date'].dt.hour\n",
    "ddf_1['Day'  ] = ddf_1['Date'].dt.dayofyear\n",
    "ddf_1['Week' ] = ddf_1['Date'].dt.isocalendar().week\n",
    "ddf_1['Month'] = ddf_1['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation new ID considering each line, and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1['Variable'] = ddf_1['Execution'] + '_' + ddf_1['Hour'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(ddf_1, values='Value', index=['Month', 'Day'], columns=['Variable'], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table.set_index(['Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data table into data X and class labels y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = table.iloc[:,1:len(table.columns)+1].values\n",
    "y = table.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigendecomposition of the raw data based on the correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Eigenpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, which can confirmed by the following two lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ev in eig_vecs:\n",
    "#     np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "# print('Everything ok!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common approach is to rank the eigenvalues from highest to lowest in order choose the top k eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is the so-called \"explained variance,\" which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "cum = 0\n",
    "while cum < 97:\n",
    "    cum += var_exp[i]\n",
    "    print(cum, i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.axvline(x=i, label='line at x = {}'.format(i), c='r')\n",
    "    plt.bar(range(len(table.columns)-1), var_exp, alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.step(range(len(table.columns)-1), cum_var_exp, where='mid',\n",
    "             label='cumulative explained variance')\n",
    "    \n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "plt.savefig(_path_1+'/Fig1.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distribution in relevant components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_pca = sklearnPCA(n_components=i)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "    for lab in labels:\n",
    "#         plt.scatter(Y_sklearn[y==lab, 0],\n",
    "#                     Y_sklearn[y==lab, 1],\n",
    "#                     label=lab)\n",
    "        zdata = Y_sklearn[y==lab, 2]\n",
    "        xdata = Y_sklearn[y==lab, 0]\n",
    "        ydata = Y_sklearn[y==lab, 1]\n",
    "        ax.scatter3D(xdata, ydata, zdata, c=zdata, label=lab, cmap='twilight');\n",
    "#     plt.xlabel('Principal Component 1')\n",
    "#     plt.ylabel('Principal Component 2')\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "    ax.set_zlabel('Principal Component 3')\n",
    "    ax.legend(loc=\"best\")\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.tight_layout()\n",
    "    plt.savefig(_path_1+'/Fig2.png', format='png', dpi=1200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(sklearn_pca.n_components_)\n",
    "plt.bar(features, sklearn_pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "plt.savefig(_path_1+'/Fig3.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(Y_sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_components.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = range(1, max_cluster)\n",
    "inertias = []\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    model.fit(PCA_components.iloc[:,:i])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)\n",
    "    \n",
    "plt.plot(ks, inertias, '-o', color='black')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.savefig(_path_1+'/Fig4.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_cluster = 0\n",
    "for k in range(len(inertias)-1):\n",
    "    diff = abs(inertias[k]-inertias[k+1])/inertias[k]*100\n",
    "    if diff > 0.1:\n",
    "        opt_cluster += 1\n",
    "    else:\n",
    "        break\n",
    "print(\"Optimal number of clusters: \", opt_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.axvline(x=opt_cluster, label='Optimal number of clusters at k = {}'.format(opt_cluster), c='r')\n",
    "    # plt.bar(range(len(table.columns)-1), var_exp, alpha=0.5, align='center',\n",
    "            # label='individual explained variance')\n",
    "    plt.step(range(len(inertias)), inertias, where='mid',\n",
    "             label='cumulative inertia')\n",
    "    \n",
    "    plt.ylabel('Inertia')\n",
    "    plt.xlabel('Number of clusters, k')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "plt.savefig(_path_1+'/Fig5.png', format='png', dpi=1200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### %opt_cluster Five clusters are chosen, so we run K-medoids with number of clusters equals four.\n",
    "### Same initializer and random state as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Clustering method\n",
    "print(\"Clustering\")\n",
    "if cluster_method == 0:\n",
    "    KMeansMethod(opt_cluster, Y_sklearn, _path_0, _path_1, CaseName_Base, CaseName_ByStages, table, data)\n",
    "elif cluster_method == 1:\n",
    "    results = KMedoidsMethod(opt_cluster, Y_sklearn, _path_0, _path_1, CaseName_Base, CaseName_ByStages, table, ddf_1)\n",
    "print('End of the process...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
