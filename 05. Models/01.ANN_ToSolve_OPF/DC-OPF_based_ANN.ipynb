{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network to solve the DC-OPF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# split a univariate sequence into samples\n",
    "def split_sequenceUStep(sequence, n_steps_in):\n",
    "    X, y = list(), list()\n",
    "    # X, y = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the study case\n",
    "CaseName = '3-bus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_dict_gen   = pd.read_csv('01.Data/'+ CaseName+'/ANN_Dict_Generation_'        +CaseName+'.csv', header=0, index_col=[0])\n",
    "df_data_gen   = pd.read_csv('01.Data/'+ CaseName+'/ANN_Data_Generation_'        +CaseName+'.csv', header=0, index_col=[0])\n",
    "df_demand     = pd.read_csv('01.Data/'+ CaseName+'/ANN_Data_Demand_'            +CaseName+'.csv', header=0, index_col=[0,1,2])\n",
    "df_generation = pd.read_csv('01.Data/'+ CaseName+'/ANN_Result_GenerationEnergy_'+CaseName+'.csv', header=0, index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_gen['MinimumPower']['CCGT_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand     = df_demand.stack().reset_index().pivot_table(index=['level_2'], columns=['level_3'], values=0, aggfunc='sum')\n",
    "df_generation = df_generation.reset_index().pivot_table(index=['LoadLevel'], columns='Unit', values='GWh', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_demand, df_generation], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "# df_data = (df_data - df_data.min()) / (df_data.max() - df_data.min())\n",
    "df_data['Node_1'] /= 1000\n",
    "df_data['Node_2'] /= 1000\n",
    "df_data['Node_3'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GWh to MWh\n",
    "df_data *= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the demand dataset\n",
    "df_data['Node_1'] = (df_data['Node_1'] - df_data['Node_1'].min()) / (df_data['Node_1'].max() - df_data['Node_1'].min())\n",
    "df_data['Node_2'] = (df_data['Node_2'] - df_data['Node_2'].min()) / (df_data['Node_2'].max() - df_data['Node_2'].min())\n",
    "df_data['Node_3'] = (df_data['Node_3'] - df_data['Node_3'].min()) / (df_data['Node_3'].max() - df_data['Node_3'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the generation dataset\n",
    "df_data['CCGT_1'] = (df_data['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) / (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1'])\n",
    "df_data['CCGT_2'] = (df_data['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) / (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (demand) and outputs (generation)\n",
    "X = df_data.iloc[:,:3].values\n",
    "y = df_data.iloc[:,3:6].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "train_inputs  = torch.from_numpy(X_train)\n",
    "train_targets = torch.from_numpy(y_train)\n",
    "test_inputs   = torch.from_numpy(X_test)\n",
    "test_targets  = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class aNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        torch.nn.init.kaiming_uniform_(self.hidden_layer1.weight, a=0)\n",
    "        self.hidden_layer2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_layer3 = torch.nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size3, output_size)\n",
    "\n",
    "        # define the device to use (GPU or CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = torch.relu(self.hidden_layer1(input))\n",
    "        hidden2 = torch.relu(self.hidden_layer2(hidden1))\n",
    "        # hidden3 = torch.relu(self.hidden_layer3(hidden2))\n",
    "        hidden3 = self.hidden_layer3(hidden2)\n",
    "        output = self.output_layer(hidden3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = aNN(input_size=3, hidden_size1=32, hidden_size2=16, hidden_size3=8, output_size=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.24193961918354034\n",
      "Epoch 20, Train Loss: 0.19388499855995178\n",
      "Epoch 30, Train Loss: 0.14367425441741943\n",
      "Epoch 40, Train Loss: 0.09151341021060944\n",
      "Epoch 50, Train Loss: 0.04249993711709976\n",
      "Epoch 60, Train Loss: 0.010557758621871471\n",
      "Epoch 70, Train Loss: 0.004760975483804941\n",
      "Epoch 80, Train Loss: 0.005270394030958414\n",
      "Epoch 90, Train Loss: 0.0033212141133844852\n",
      "Epoch 100, Train Loss: 0.0028468521777540445\n",
      "Epoch 110, Train Loss: 0.002501013921573758\n",
      "Epoch 120, Train Loss: 0.002184591954573989\n",
      "Epoch 130, Train Loss: 0.0020009991712868214\n",
      "Epoch 140, Train Loss: 0.0018385141156613827\n",
      "Epoch 150, Train Loss: 0.0016961144283413887\n",
      "Epoch 160, Train Loss: 0.0015440280549228191\n",
      "Epoch 170, Train Loss: 0.001384914037771523\n",
      "Epoch 180, Train Loss: 0.0012518754228949547\n",
      "Epoch 190, Train Loss: 0.0011693834094330668\n",
      "Epoch 200, Train Loss: 0.0011090554762631655\n",
      "Epoch 210, Train Loss: 0.00104990741237998\n",
      "Epoch 220, Train Loss: 0.0009954383131116629\n",
      "Epoch 230, Train Loss: 0.0009470279328525066\n",
      "Epoch 240, Train Loss: 0.000902898667845875\n",
      "Epoch 250, Train Loss: 0.0008614803664386272\n",
      "Epoch 260, Train Loss: 0.0008228167425841093\n",
      "Epoch 270, Train Loss: 0.0007873820723034441\n",
      "Epoch 280, Train Loss: 0.0007551722810603678\n",
      "Epoch 290, Train Loss: 0.0007247932953760028\n",
      "Epoch 300, Train Loss: 0.0006955064600333571\n",
      "Epoch 310, Train Loss: 0.000667244428768754\n",
      "Epoch 320, Train Loss: 0.0006397182587534189\n",
      "Epoch 330, Train Loss: 0.0006127777742221951\n",
      "Epoch 340, Train Loss: 0.000586566049605608\n",
      "Epoch 350, Train Loss: 0.00056093605235219\n",
      "Epoch 360, Train Loss: 0.0005358299240469933\n",
      "Epoch 370, Train Loss: 0.0005114829400554299\n",
      "Epoch 380, Train Loss: 0.0004878538893535733\n",
      "Epoch 390, Train Loss: 0.0004649820039048791\n",
      "Epoch 400, Train Loss: 0.0004428690590430051\n",
      "Epoch 410, Train Loss: 0.0004216023371554911\n",
      "Epoch 420, Train Loss: 0.00040129193803295493\n",
      "Epoch 430, Train Loss: 0.00038189711631275713\n",
      "Epoch 440, Train Loss: 0.00036341327358968556\n",
      "Epoch 450, Train Loss: 0.00034332452923990786\n",
      "Epoch 460, Train Loss: 0.0003205473185516894\n",
      "Epoch 470, Train Loss: 0.0003010231303051114\n",
      "Epoch 480, Train Loss: 0.0002836641506291926\n",
      "Epoch 490, Train Loss: 0.00026819182676263154\n",
      "Epoch 500, Train Loss: 0.0002542832517065108\n",
      "Epoch 510, Train Loss: 0.00024148821830749512\n",
      "Epoch 520, Train Loss: 0.00022962653019931167\n",
      "Epoch 530, Train Loss: 0.00021861439745407552\n",
      "Epoch 540, Train Loss: 0.00020830216817557812\n",
      "Epoch 550, Train Loss: 0.0001987049909075722\n",
      "Epoch 560, Train Loss: 0.0001897920446936041\n",
      "Epoch 570, Train Loss: 0.00018145571812056005\n",
      "Epoch 580, Train Loss: 0.00017362031212542206\n",
      "Epoch 590, Train Loss: 0.00016623776173219085\n",
      "Epoch 600, Train Loss: 0.00015920308942440897\n",
      "Epoch 610, Train Loss: 0.00015228400297928602\n",
      "Epoch 620, Train Loss: 0.0001443499932065606\n",
      "Epoch 630, Train Loss: 0.00013569917064160109\n",
      "Epoch 640, Train Loss: 0.00012768506712745875\n",
      "Epoch 650, Train Loss: 0.00011975895904470235\n",
      "Epoch 660, Train Loss: 0.00010267709876643494\n",
      "Epoch 670, Train Loss: 8.534592780051753e-05\n",
      "Epoch 680, Train Loss: 7.174207712523639e-05\n",
      "Epoch 690, Train Loss: 6.0501592088257894e-05\n",
      "Epoch 700, Train Loss: 5.156906263437122e-05\n",
      "Epoch 710, Train Loss: 4.450170308700763e-05\n",
      "Epoch 720, Train Loss: 3.893985558534041e-05\n",
      "Epoch 730, Train Loss: 3.4748245525406674e-05\n",
      "Epoch 740, Train Loss: 3.142664354527369e-05\n",
      "Epoch 750, Train Loss: 2.8916809242218733e-05\n",
      "Epoch 760, Train Loss: 2.6891419111052528e-05\n",
      "Epoch 770, Train Loss: 2.5218731025233865e-05\n",
      "Epoch 780, Train Loss: 2.3827065888326615e-05\n",
      "Epoch 790, Train Loss: 2.266998853883706e-05\n",
      "Epoch 800, Train Loss: 2.171570849895943e-05\n",
      "Epoch 810, Train Loss: 2.0906416466459632e-05\n",
      "Epoch 820, Train Loss: 2.0330926417955197e-05\n",
      "Epoch 830, Train Loss: 1.96221808437258e-05\n",
      "Epoch 840, Train Loss: 1.906116449390538e-05\n",
      "Epoch 850, Train Loss: 1.8555898350314237e-05\n",
      "Epoch 860, Train Loss: 1.80734114110237e-05\n",
      "Epoch 870, Train Loss: 1.7600395949557424e-05\n",
      "Epoch 880, Train Loss: 1.714701284072362e-05\n",
      "Epoch 890, Train Loss: 1.670750862103887e-05\n",
      "Epoch 900, Train Loss: 1.6450287148472853e-05\n",
      "Epoch 910, Train Loss: 1.5978997907950543e-05\n",
      "Epoch 920, Train Loss: 1.553269976284355e-05\n",
      "Epoch 930, Train Loss: 1.5187993994913995e-05\n",
      "Epoch 940, Train Loss: 1.4815291251579765e-05\n",
      "Epoch 950, Train Loss: 1.446679380023852e-05\n",
      "Epoch 960, Train Loss: 1.4124142580840271e-05\n",
      "Epoch 970, Train Loss: 1.3784650946035981e-05\n",
      "Epoch 980, Train Loss: 1.3438803762255702e-05\n",
      "Epoch 990, Train Loss: 1.394479022565065e-05\n",
      "Epoch 1000, Train Loss: 1.284291829506401e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    train_predictions = model(train_inputs.float())\n",
    "    train_loss = torch.nn.MSELoss()(train_predictions.float().squeeze(), train_targets.float())\n",
    "\n",
    "    # Backward pass\n",
    "    # optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the training loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1991730389127042e-05\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_predictions = model(test_inputs.float())\n",
    "test_loss = torch.nn.MSELoss()(test_predictions.float().squeeze(), test_targets.float())\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.2500e-03,  9.5506e-01],\n",
       "        [ 3.4167e-03,  9.8608e-01],\n",
       "        [ 4.1871e-03,  9.1085e-01],\n",
       "        ...,\n",
       "        [-4.6503e-03,  7.5379e-01],\n",
       "        [ 8.7485e-04,  9.2976e-01],\n",
       "        [ 4.3493e-03,  9.9816e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test predictions and test output to NumPy arrays\n",
    "test_targets = test_targets.detach().numpy()\n",
    "test_predictions_s = test_predictions.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets_df = pd.DataFrame(test_targets, columns=['G1_target','G2_target'])\n",
    "test_targets_df['G1_target'] = test_targets_df['G1_target'] * (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) + df_data_gen['MinimumPower']['CCGT_1']\n",
    "test_targets_df['G2_target'] = test_targets_df['G2_target'] * (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) + df_data_gen['MinimumPower']['CCGT_2']\n",
    "test_targets_df.to_csv('test_targets.csv', index=False)\n",
    "test_predictions_df = pd.DataFrame(test_predictions_s, columns=['G1_estimate','G2_estimate'])\n",
    "test_predictions_df['G1_estimate'] = test_predictions_df['G1_estimate'] * (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) + df_data_gen['MinimumPower']['CCGT_1']\n",
    "test_predictions_df['G2_estimate'] = test_predictions_df['G2_estimate'] * (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) + df_data_gen['MinimumPower']['CCGT_2']\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame({'LoadLevel': pd.date_range(start='2023-05-04 00:00:00', periods=len(test_predictions_df), freq='H')})\n",
    "frames = [time_df, test_predictions_df, test_targets_df]\n",
    "result = pd.concat(frames, axis=1).set_index('LoadLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = result.stack().reset_index().rename(columns={'level_1':'Demand', 0:'Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "lines = (\n",
    "    alt.Chart(source)\n",
    "    .mark_line()\n",
    "    .encode(x=\"LoadLevel\", y=\"Value\", color=\"Demand\")\n",
    ").properties(width=1500, height=500)\n",
    "lines.save('Plot.html', embed_options={'renderer':'svg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An extensive graph of the model using torchviz\n",
    "# from torchviz import make_dot\n",
    "\n",
    "# dot = make_dot(test_predictions, params=dict(model.named_parameters()))\n",
    "# dot.render(\"rnn_torchviz\", format=\"png\")\n",
    "# dot.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:12544): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    }
   ],
   "source": [
    "# plot the model graph using torchview\n",
    "from torchview import draw_graph\n",
    "model_graph = draw_graph(model, input_size=(test_inputs.size()), device='meta', save_graph=True, graph_name='model_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an array of 20 with random integer values between 0 and 100\n",
    "x = np.random.randint(0, 100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49, 80, 79,  4, 90, 53, 27, 60, 18, 35, 31, 12, 65, 65, 64, 13, 65,\n",
       "       85, 19, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
