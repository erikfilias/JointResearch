{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network to solve the DC-OPF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# split a univariate sequence into samples\n",
    "def split_sequenceUStep(sequence, n_steps_in):\n",
    "    X, y = list(), list()\n",
    "    # X, y = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the study case\n",
    "CaseName = '3-bus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_dict_gen   = pd.read_csv('01.Data/'+ CaseName+'/ANN_Dict_Generation_'        +CaseName+'.csv', header=0, index_col=[0])\n",
    "df_data_gen   = pd.read_csv('01.Data/'+ CaseName+'/ANN_Data_Generation_'        +CaseName+'.csv', header=0, index_col=[0])\n",
    "df_demand     = pd.read_csv('01.Data/'+ CaseName+'/ANN_Data_Demand_'            +CaseName+'.csv', header=0, index_col=[0,1,2])\n",
    "df_generation = pd.read_csv('01.Data/'+ CaseName+'/ANN_Result_GenerationEnergy_'+CaseName+'.csv', header=0, index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_gen['MinimumPower']['CCGT_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand     = df_demand.stack().reset_index().pivot_table(index=['level_2'], columns=['level_3'], values=0, aggfunc='sum')\n",
    "df_generation = df_generation.reset_index().pivot_table(index=['LoadLevel'], columns='Unit', values='GWh', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_demand, df_generation], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "# df_data = (df_data - df_data.min()) / (df_data.max() - df_data.min())\n",
    "df_data['Node_1'] /= 1000\n",
    "df_data['Node_2'] /= 1000\n",
    "df_data['Node_3'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GWh to MWh\n",
    "df_data *= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the demand dataset\n",
    "df_data['Node_1'] = (df_data['Node_1'] - df_data['Node_1'].min()) / (df_data['Node_1'].max() - df_data['Node_1'].min())\n",
    "df_data['Node_2'] = (df_data['Node_2'] - df_data['Node_2'].min()) / (df_data['Node_2'].max() - df_data['Node_2'].min())\n",
    "df_data['Node_3'] = (df_data['Node_3'] - df_data['Node_3'].min()) / (df_data['Node_3'].max() - df_data['Node_3'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the generation dataset\n",
    "df_data['CCGT_1'] = (df_data['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) / (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1'])\n",
    "df_data['CCGT_2'] = (df_data['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) / (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (demand) and outputs (generation)\n",
    "X = df_data.iloc[:,:3].values\n",
    "y = df_data.iloc[:,3:6].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "train_inputs  = torch.from_numpy(X_train)\n",
    "train_targets = torch.from_numpy(y_train)\n",
    "test_inputs   = torch.from_numpy(X_test)\n",
    "test_targets  = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class aNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        torch.nn.init.kaiming_uniform_(self.hidden_layer1.weight, a=0)\n",
    "        self.hidden_layer2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_layer3 = torch.nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size3, output_size)\n",
    "\n",
    "        # define the device to use (GPU or CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = torch.relu(self.hidden_layer1(input))\n",
    "        hidden2 = torch.relu(self.hidden_layer2(hidden1))\n",
    "        # hidden3 = torch.relu(self.hidden_layer3(hidden2))\n",
    "        hidden3 = self.hidden_layer3(hidden2)\n",
    "        output = self.output_layer(hidden3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = aNN(input_size=3, hidden_size1=32, hidden_size2=16, hidden_size3=8, output_size=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.5714046955108643\n",
      "Epoch 20, Train Loss: 0.4193643033504486\n",
      "Epoch 30, Train Loss: 0.2795654237270355\n",
      "Epoch 40, Train Loss: 0.15618497133255005\n",
      "Epoch 50, Train Loss: 0.06562673300504684\n",
      "Epoch 60, Train Loss: 0.020392518490552902\n",
      "Epoch 70, Train Loss: 0.013558031059801579\n",
      "Epoch 80, Train Loss: 0.013473832048475742\n",
      "Epoch 90, Train Loss: 0.010369607247412205\n",
      "Epoch 100, Train Loss: 0.008646944537758827\n",
      "Epoch 110, Train Loss: 0.007585547398775816\n",
      "Epoch 120, Train Loss: 0.0065331063233315945\n",
      "Epoch 130, Train Loss: 0.005678690504282713\n",
      "Epoch 140, Train Loss: 0.004988003056496382\n",
      "Epoch 150, Train Loss: 0.004419767763465643\n",
      "Epoch 160, Train Loss: 0.003971650265157223\n",
      "Epoch 170, Train Loss: 0.003623749129474163\n",
      "Epoch 180, Train Loss: 0.003354820189997554\n",
      "Epoch 190, Train Loss: 0.003144751535728574\n",
      "Epoch 200, Train Loss: 0.00297424104064703\n",
      "Epoch 210, Train Loss: 0.0028308280743658543\n",
      "Epoch 220, Train Loss: 0.0027059337589889765\n",
      "Epoch 230, Train Loss: 0.002594149438664317\n",
      "Epoch 240, Train Loss: 0.002492316300049424\n",
      "Epoch 250, Train Loss: 0.00239846995100379\n",
      "Epoch 260, Train Loss: 0.0023110697511583567\n",
      "Epoch 270, Train Loss: 0.002229044446721673\n",
      "Epoch 280, Train Loss: 0.002151230815798044\n",
      "Epoch 290, Train Loss: 0.002077042357996106\n",
      "Epoch 300, Train Loss: 0.0020054078195244074\n",
      "Epoch 310, Train Loss: 0.0019334880635142326\n",
      "Epoch 320, Train Loss: 0.0018508901121094823\n",
      "Epoch 330, Train Loss: 0.001749929622747004\n",
      "Epoch 340, Train Loss: 0.0016620067181065679\n",
      "Epoch 350, Train Loss: 0.0015913431998342276\n",
      "Epoch 360, Train Loss: 0.0015190787380561233\n",
      "Epoch 370, Train Loss: 0.00144957413431257\n",
      "Epoch 380, Train Loss: 0.0013830759562551975\n",
      "Epoch 390, Train Loss: 0.0013215267099440098\n",
      "Epoch 400, Train Loss: 0.0012650337303057313\n",
      "Epoch 410, Train Loss: 0.001213054172694683\n",
      "Epoch 420, Train Loss: 0.0011646912898868322\n",
      "Epoch 430, Train Loss: 0.0011193600948899984\n",
      "Epoch 440, Train Loss: 0.0010769710643216968\n",
      "Epoch 450, Train Loss: 0.0010370465461164713\n",
      "Epoch 460, Train Loss: 0.000999321579001844\n",
      "Epoch 470, Train Loss: 0.0009632977307774127\n",
      "Epoch 480, Train Loss: 0.0009284439729526639\n",
      "Epoch 490, Train Loss: 0.0008945564040914178\n",
      "Epoch 500, Train Loss: 0.0008613582467660308\n",
      "Epoch 510, Train Loss: 0.0008286935044452548\n",
      "Epoch 520, Train Loss: 0.0007960719522088766\n",
      "Epoch 530, Train Loss: 0.0007632334600202739\n",
      "Epoch 540, Train Loss: 0.0007303232559934258\n",
      "Epoch 550, Train Loss: 0.0006972577539272606\n",
      "Epoch 560, Train Loss: 0.0006641984218731523\n",
      "Epoch 570, Train Loss: 0.0006313645862974226\n",
      "Epoch 580, Train Loss: 0.0005988531629554927\n",
      "Epoch 590, Train Loss: 0.00056709029013291\n",
      "Epoch 600, Train Loss: 0.0005361367366276681\n",
      "Epoch 610, Train Loss: 0.0005060832481831312\n",
      "Epoch 620, Train Loss: 0.00047719082795083523\n",
      "Epoch 630, Train Loss: 0.00044951343443244696\n",
      "Epoch 640, Train Loss: 0.00042284393566660583\n",
      "Epoch 650, Train Loss: 0.0003971215628553182\n",
      "Epoch 660, Train Loss: 0.00037246779538691044\n",
      "Epoch 670, Train Loss: 0.000349056237610057\n",
      "Epoch 680, Train Loss: 0.00032680557342246175\n",
      "Epoch 690, Train Loss: 0.00030569834052585065\n",
      "Epoch 700, Train Loss: 0.00028581140213645995\n",
      "Epoch 710, Train Loss: 0.00026719961897470057\n",
      "Epoch 720, Train Loss: 0.00024969998048618436\n",
      "Epoch 730, Train Loss: 0.0002332901203772053\n",
      "Epoch 740, Train Loss: 0.00021796501823700964\n",
      "Epoch 750, Train Loss: 0.00020357976609375328\n",
      "Epoch 760, Train Loss: 0.00019017419253941625\n",
      "Epoch 770, Train Loss: 0.0001777223078534007\n",
      "Epoch 780, Train Loss: 0.00016606564167886972\n",
      "Epoch 790, Train Loss: 0.0001552132744109258\n",
      "Epoch 800, Train Loss: 0.00014515187649521977\n",
      "Epoch 810, Train Loss: 0.00013579332153312862\n",
      "Epoch 820, Train Loss: 0.00012706982670351863\n",
      "Epoch 830, Train Loss: 0.00011886862921528518\n",
      "Epoch 840, Train Loss: 0.00011096447997260839\n",
      "Epoch 850, Train Loss: 0.00010320191358914599\n",
      "Epoch 860, Train Loss: 9.552387200528756e-05\n",
      "Epoch 870, Train Loss: 8.807489211903885e-05\n",
      "Epoch 880, Train Loss: 8.136711403494701e-05\n",
      "Epoch 890, Train Loss: 7.573739276267588e-05\n",
      "Epoch 900, Train Loss: 7.091915176715702e-05\n",
      "Epoch 910, Train Loss: 6.656856567133218e-05\n",
      "Epoch 920, Train Loss: 6.27021145191975e-05\n",
      "Epoch 930, Train Loss: 5.9359830629546195e-05\n",
      "Epoch 940, Train Loss: 5.64501024200581e-05\n",
      "Epoch 950, Train Loss: 5.3928557463223115e-05\n",
      "Epoch 960, Train Loss: 5.174359102966264e-05\n",
      "Epoch 970, Train Loss: 4.9823200242826715e-05\n",
      "Epoch 980, Train Loss: 4.811348844668828e-05\n",
      "Epoch 990, Train Loss: 4.6572135033784434e-05\n",
      "Epoch 1000, Train Loss: 4.516791523201391e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    train_predictions = model(train_inputs.float())\n",
    "    train_loss = torch.nn.MSELoss()(train_predictions.float().squeeze(), train_targets.float())\n",
    "\n",
    "    # Backward pass\n",
    "    # optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the training loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.4005199015373364e-05\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_predictions = model(test_inputs.float())\n",
    "test_loss = torch.nn.MSELoss()(test_predictions.float().squeeze(), test_targets.float())\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0132,  0.6575],\n",
       "        [ 0.0061,  0.9973],\n",
       "        [ 0.1690,  0.9952],\n",
       "        ...,\n",
       "        [-0.0029,  0.9301],\n",
       "        [-0.0031,  0.8616],\n",
       "        [ 0.0910,  1.0022]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test predictions and test output to NumPy arrays\n",
    "test_targets = test_targets.detach().numpy()\n",
    "test_predictions = test_predictions.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets_df = pd.DataFrame(test_targets, columns=['G1_target','G2_target'])\n",
    "test_targets_df['G1_target'] = test_targets_df['G1_target'] * (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) + df_data_gen['MinimumPower']['CCGT_1']\n",
    "test_targets_df['G2_target'] = test_targets_df['G2_target'] * (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) + df_data_gen['MinimumPower']['CCGT_2']\n",
    "test_targets_df.to_csv('test_targets.csv', index=False)\n",
    "test_predictions_df = pd.DataFrame(test_predictions, columns=['G1_estimate','G2_estimate'])\n",
    "test_predictions_df['G1_estimate'] = test_predictions_df['G1_estimate'] * (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) + df_data_gen['MinimumPower']['CCGT_1']\n",
    "test_predictions_df['G2_estimate'] = test_predictions_df['G2_estimate'] * (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) + df_data_gen['MinimumPower']['CCGT_2']\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame({'LoadLevel': pd.date_range(start='2023-05-04 00:00:00', periods=len(test_predictions_df), freq='H')})\n",
    "frames = [time_df, test_predictions_df, test_targets_df]\n",
    "result = pd.concat(frames, axis=1).set_index('LoadLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = result.stack().reset_index().rename(columns={'level_1':'Demand', 0:'Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "lines = (\n",
    "    alt.Chart(source)\n",
    "    .mark_line()\n",
    "    .encode(x=\"LoadLevel\", y=\"Value\", color=\"Demand\")\n",
    ").properties(width=1500, height=500)\n",
    "lines.save('Plot.html', embed_options={'renderer':'svg'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
