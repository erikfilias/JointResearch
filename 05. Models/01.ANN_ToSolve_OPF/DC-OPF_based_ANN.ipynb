{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A neural network to solve the DC-OPF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import torch\n",
    "from torchviz import make_dot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# split a univariate sequence into samples\n",
    "def split_sequenceUStep(sequence, n_steps_in):\n",
    "    X, y = list(), list()\n",
    "    # X, y = defaultdict(list), defaultdict(list)\n",
    "    \n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        \n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "        \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the study case\n",
    "CaseName = '3-bus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_dict_gen   = pd.read_csv('01.Data/'+ CaseName+'/ANN_Dict_Generation_'        +CaseName+'.csv', header=0, index_col=[0])\n",
    "df_data_gen   = pd.read_csv('01.Data/'+ CaseName+'/ANN_Data_Generation_'        +CaseName+'.csv', header=0, index_col=[0])\n",
    "df_demand     = pd.read_csv('01.Data/'+ CaseName+'/ANN_Data_Demand_'            +CaseName+'.csv', header=0, index_col=[0,1,2])\n",
    "df_generation = pd.read_csv('01.Data/'+ CaseName+'/ANN_Result_GenerationEnergy_'+CaseName+'.csv', header=0, index_col=[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_gen['MinimumPower']['CCGT_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand     = df_demand.stack().reset_index().pivot_table(index=['level_2'], columns=['level_3'], values=0, aggfunc='sum')\n",
    "df_generation = df_generation.reset_index().pivot_table(index=['LoadLevel'], columns='Unit', values='GWh', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.concat([df_demand, df_generation], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "# df_data = (df_data - df_data.min()) / (df_data.max() - df_data.min())\n",
    "df_data['Node_1'] /= 1000\n",
    "df_data['Node_2'] /= 1000\n",
    "df_data['Node_3'] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from GWh to MWh\n",
    "df_data *= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the demand dataset\n",
    "df_data['Node_1'] = (df_data['Node_1'] - df_data['Node_1'].min()) / (df_data['Node_1'].max() - df_data['Node_1'].min())\n",
    "df_data['Node_2'] = (df_data['Node_2'] - df_data['Node_2'].min()) / (df_data['Node_2'].max() - df_data['Node_2'].min())\n",
    "df_data['Node_3'] = (df_data['Node_3'] - df_data['Node_3'].min()) / (df_data['Node_3'].max() - df_data['Node_3'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the generation dataset\n",
    "df_data['CCGT_1'] = (df_data['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) / (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1'])\n",
    "df_data['CCGT_2'] = (df_data['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) / (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (demand) and outputs (generation)\n",
    "X = df_data.iloc[:,:3].values\n",
    "y = df_data.iloc[:,3:6].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "train_inputs  = torch.from_numpy(X_train)\n",
    "train_targets = torch.from_numpy(y_train)\n",
    "test_inputs   = torch.from_numpy(X_test)\n",
    "test_targets  = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class aNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_layer1 = torch.nn.Linear(input_size, hidden_size1)\n",
    "        torch.nn.init.kaiming_uniform_(self.hidden_layer1.weight, a=0)\n",
    "        self.hidden_layer2 = torch.nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.hidden_layer3 = torch.nn.Linear(hidden_size2, hidden_size3)\n",
    "        self.output_layer = torch.nn.Linear(hidden_size3, output_size)\n",
    "\n",
    "        # define the device to use (GPU or CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden1 = torch.relu(self.hidden_layer1(input))\n",
    "        hidden2 = torch.relu(self.hidden_layer2(hidden1))\n",
    "        # hidden3 = torch.relu(self.hidden_layer3(hidden2))\n",
    "        hidden3 = self.hidden_layer3(hidden2)\n",
    "        output = self.output_layer(hidden3)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer\n",
    "model = aNN(input_size=3, hidden_size1=32, hidden_size2=16, hidden_size3=8, output_size=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 0.2950093448162079\n",
      "Epoch 20, Train Loss: 0.2402355670928955\n",
      "Epoch 30, Train Loss: 0.18687888979911804\n",
      "Epoch 40, Train Loss: 0.13246048986911774\n",
      "Epoch 50, Train Loss: 0.07768283784389496\n",
      "Epoch 60, Train Loss: 0.030700836330652237\n",
      "Epoch 70, Train Loss: 0.0064253793098032475\n",
      "Epoch 80, Train Loss: 0.004464925732463598\n",
      "Epoch 90, Train Loss: 0.004392119124531746\n",
      "Epoch 100, Train Loss: 0.002982495818287134\n",
      "Epoch 110, Train Loss: 0.0027553467079997063\n",
      "Epoch 120, Train Loss: 0.0024625477381050587\n",
      "Epoch 130, Train Loss: 0.0021653929725289345\n",
      "Epoch 140, Train Loss: 0.001939917798154056\n",
      "Epoch 150, Train Loss: 0.0017709741368889809\n",
      "Epoch 160, Train Loss: 0.0016351070953533053\n",
      "Epoch 170, Train Loss: 0.0015163097996264696\n",
      "Epoch 180, Train Loss: 0.0014137565158307552\n",
      "Epoch 190, Train Loss: 0.0013234467478469014\n",
      "Epoch 200, Train Loss: 0.0012428901391103864\n",
      "Epoch 210, Train Loss: 0.0011697628069669008\n",
      "Epoch 220, Train Loss: 0.0011025756830349565\n",
      "Epoch 230, Train Loss: 0.0010402281768620014\n",
      "Epoch 240, Train Loss: 0.000982037279754877\n",
      "Epoch 250, Train Loss: 0.000927308457903564\n",
      "Epoch 260, Train Loss: 0.0008755370508879423\n",
      "Epoch 270, Train Loss: 0.0008262203191407025\n",
      "Epoch 280, Train Loss: 0.0007791618700139225\n",
      "Epoch 290, Train Loss: 0.0007340186275541782\n",
      "Epoch 300, Train Loss: 0.0006904898909851909\n",
      "Epoch 310, Train Loss: 0.0006484595942310989\n",
      "Epoch 320, Train Loss: 0.0006075706332921982\n",
      "Epoch 330, Train Loss: 0.0005673014093190432\n",
      "Epoch 340, Train Loss: 0.0005270367837511003\n",
      "Epoch 350, Train Loss: 0.0004867004172410816\n",
      "Epoch 360, Train Loss: 0.0004456100577954203\n",
      "Epoch 370, Train Loss: 0.0004032338038086891\n",
      "Epoch 380, Train Loss: 0.0003607279504649341\n",
      "Epoch 390, Train Loss: 0.0003205420798622072\n",
      "Epoch 400, Train Loss: 0.0002838670916389674\n",
      "Epoch 410, Train Loss: 0.00025143803213723004\n",
      "Epoch 420, Train Loss: 0.00022347382036969066\n",
      "Epoch 430, Train Loss: 0.00019955205789301544\n",
      "Epoch 440, Train Loss: 0.00017891437164507806\n",
      "Epoch 450, Train Loss: 0.0001607797312317416\n",
      "Epoch 460, Train Loss: 0.0001447623799322173\n",
      "Epoch 470, Train Loss: 0.00013065333769191056\n",
      "Epoch 480, Train Loss: 0.00011822736269095913\n",
      "Epoch 490, Train Loss: 0.0001073298990377225\n",
      "Epoch 500, Train Loss: 9.773118654266e-05\n",
      "Epoch 510, Train Loss: 8.9250024757348e-05\n",
      "Epoch 520, Train Loss: 8.176686969818547e-05\n",
      "Epoch 530, Train Loss: 7.516854384448379e-05\n",
      "Epoch 540, Train Loss: 6.935700366739184e-05\n",
      "Epoch 550, Train Loss: 6.425371975637972e-05\n",
      "Epoch 560, Train Loss: 5.975398744340055e-05\n",
      "Epoch 570, Train Loss: 5.5781478295102715e-05\n",
      "Epoch 580, Train Loss: 5.225926361163147e-05\n",
      "Epoch 590, Train Loss: 4.91353239340242e-05\n",
      "Epoch 600, Train Loss: 4.635480581782758e-05\n",
      "Epoch 610, Train Loss: 4.385647480376065e-05\n",
      "Epoch 620, Train Loss: 4.157034709351137e-05\n",
      "Epoch 630, Train Loss: 3.923783515347168e-05\n",
      "Epoch 640, Train Loss: 3.724951602634974e-05\n",
      "Epoch 650, Train Loss: 3.5444543755147606e-05\n",
      "Epoch 660, Train Loss: 3.379447298357263e-05\n",
      "Epoch 670, Train Loss: 3.227148044970818e-05\n",
      "Epoch 680, Train Loss: 3.086657670792192e-05\n",
      "Epoch 690, Train Loss: 2.9563976568169892e-05\n",
      "Epoch 700, Train Loss: 2.826651507348288e-05\n",
      "Epoch 710, Train Loss: 2.6957417503581382e-05\n",
      "Epoch 720, Train Loss: 2.577783197921235e-05\n",
      "Epoch 730, Train Loss: 2.4810109607642516e-05\n",
      "Epoch 740, Train Loss: 2.3950769900693558e-05\n",
      "Epoch 750, Train Loss: 2.3067044821800664e-05\n",
      "Epoch 760, Train Loss: 2.1970106899971142e-05\n",
      "Epoch 770, Train Loss: 2.093834882543888e-05\n",
      "Epoch 780, Train Loss: 1.9989032807643525e-05\n",
      "Epoch 790, Train Loss: 1.9177819922333583e-05\n",
      "Epoch 800, Train Loss: 1.8476323020877317e-05\n",
      "Epoch 810, Train Loss: 1.7848162315203808e-05\n",
      "Epoch 820, Train Loss: 1.725492074911017e-05\n",
      "Epoch 830, Train Loss: 1.668438380875159e-05\n",
      "Epoch 840, Train Loss: 1.6134732504724525e-05\n",
      "Epoch 850, Train Loss: 1.5592775525874458e-05\n",
      "Epoch 860, Train Loss: 1.507329670857871e-05\n",
      "Epoch 870, Train Loss: 1.4593058040190954e-05\n",
      "Epoch 880, Train Loss: 1.4158182239043526e-05\n",
      "Epoch 890, Train Loss: 1.3761741683993023e-05\n",
      "Epoch 900, Train Loss: 1.3397677321336232e-05\n",
      "Epoch 910, Train Loss: 1.305740988755133e-05\n",
      "Epoch 920, Train Loss: 1.2737000361084938e-05\n",
      "Epoch 930, Train Loss: 1.2430403330654372e-05\n",
      "Epoch 940, Train Loss: 1.213436644320609e-05\n",
      "Epoch 950, Train Loss: 1.1848521353385877e-05\n",
      "Epoch 960, Train Loss: 1.156903545052046e-05\n",
      "Epoch 970, Train Loss: 1.1298850949970074e-05\n",
      "Epoch 980, Train Loss: 1.1038687262043823e-05\n",
      "Epoch 990, Train Loss: 1.0786028724396601e-05\n",
      "Epoch 1000, Train Loss: 1.0538535207160749e-05\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    train_predictions = model(train_inputs.float())\n",
    "    train_loss = torch.nn.MSELoss()(train_predictions.float().squeeze(), train_targets.float())\n",
    "\n",
    "    # Backward pass\n",
    "    # optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the training loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.099209111998789e-05\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_predictions = model(test_inputs.float())\n",
    "test_loss = torch.nn.MSELoss()(test_predictions.float().squeeze(), test_targets.float())\n",
    "print(f'Test Loss: {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3736e-01,  9.9929e-01],\n",
       "        [-5.7791e-04,  8.0667e-01],\n",
       "        [ 2.4603e-02,  9.9406e-01],\n",
       "        ...,\n",
       "        [-4.3851e-04,  7.3671e-01],\n",
       "        [ 1.7682e-01,  9.9639e-01],\n",
       "        [-3.4897e-03,  9.9056e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test predictions and test output to NumPy arrays\n",
    "test_targets = test_targets.detach().numpy()\n",
    "test_predictions_s = test_predictions.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets_df = pd.DataFrame(test_targets, columns=['G1_target','G2_target'])\n",
    "test_targets_df['G1_target'] = test_targets_df['G1_target'] * (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) + df_data_gen['MinimumPower']['CCGT_1']\n",
    "test_targets_df['G2_target'] = test_targets_df['G2_target'] * (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) + df_data_gen['MinimumPower']['CCGT_2']\n",
    "test_targets_df.to_csv('test_targets.csv', index=False)\n",
    "test_predictions_df = pd.DataFrame(test_predictions_s, columns=['G1_estimate','G2_estimate'])\n",
    "test_predictions_df['G1_estimate'] = test_predictions_df['G1_estimate'] * (df_data_gen['MaximumPower']['CCGT_1'] - df_data_gen['MinimumPower']['CCGT_1']) + df_data_gen['MinimumPower']['CCGT_1']\n",
    "test_predictions_df['G2_estimate'] = test_predictions_df['G2_estimate'] * (df_data_gen['MaximumPower']['CCGT_2'] - df_data_gen['MinimumPower']['CCGT_2']) + df_data_gen['MinimumPower']['CCGT_2']\n",
    "test_predictions_df.to_csv('test_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame({'LoadLevel': pd.date_range(start='2023-05-04 00:00:00', periods=len(test_predictions_df), freq='H')})\n",
    "frames = [time_df, test_predictions_df, test_targets_df]\n",
    "result = pd.concat(frames, axis=1).set_index('LoadLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = result.stack().reset_index().rename(columns={'level_1':'Demand', 0:'Value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "lines = (\n",
    "    alt.Chart(source)\n",
    "    .mark_line()\n",
    "    .encode(x=\"LoadLevel\", y=\"Value\", color=\"Demand\")\n",
    ").properties(width=1500, height=500)\n",
    "lines.save('Plot.html', embed_options={'renderer':'svg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## An extensive graph of the model using torchviz\n",
    "# from torchviz import make_dot\n",
    "\n",
    "# dot = make_dot(test_predictions, params=dict(model.named_parameters()))\n",
    "# dot.render(\"rnn_torchviz\", format=\"png\")\n",
    "# dot.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(dot.exe:21088): Pango-WARNING **: couldn't load font \"Linux libertine Not-Rotated 10\", falling back to \"Sans Not-Rotated 10\", expect ugly output.\n"
     ]
    }
   ],
   "source": [
    "# plot the model graph using torchview\n",
    "from torchview import draw_graph\n",
    "model_graph = draw_graph(model, input_size=(test_inputs.size()), device='meta', save_graph=True, graph_name='model_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
