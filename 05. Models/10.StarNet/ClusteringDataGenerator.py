# This script generates the data for the clustering task.
# The data is generated by executing the neural network and using the input data of the StarNet model.

#%% Imports
import argparse
import itertools
import math
import os
import time
import torch
import itertools  as it
import NN_classes as NN
import numpy      as np
import pandas     as pd
from   pyomo.environ import DataPortal, Set

#%% Defining metadata
parser = argparse.ArgumentParser(description='Introducing main parameters...')
parser.add_argument('--case',   type=str, default=None)
parser.add_argument('--dir',    type=str, default=None)
parser.add_argument('--solver', type=str, default=None)

# Setting the path to the data
DIR    = os.path.dirname(__file__)
CASE   = '9n'
SOLVER = 'gurobi'

# Defining the main function
def main():
    args = parser.parse_args()
    if args.dir is None:
        args.dir    = input('Input Dir    Name (Default {}): '.format(DIR))
        if args.dir == '':
            args.dir = DIR
    if args.case is None:
        args.case   = input('Input Case   Name (Default {}): '.format(CASE))
        if args.case == '':
            args.case = CASE
    if args.solver is None:
        args.solver = input('Input Solver Name (Default {}): '.format(SOLVER))
        if args.solver == '':
            args.solver = SOLVER
    print(args.case)
    print(args.dir)
    print(args.solver)
    import sys
    print(sys.argv)
    print(args)

    #%% Reading data from CSV files
    _path      = os.path.join(args.dir, args.case)
    # Initial time counter for all the code
    initial_time = time.time()
    df = PINT_approach(args.dir, args.case)

    df.to_csv(_path + '/3.Out' + '/oT_Benefit_Line_PINT_' + args.case + '.csv')

def PINT_approach(DirName, CaseName):
    # building the path to the data
    _path = os.path.join(DirName, CaseName)
    # initial time counter for PINT approach
    initial_time = time.time()
    start_time   = time.time()
    # reading dictionaries from CSV files
    dictSets = DataPortal()
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Period_'    +CaseName+'.csv'), set='pp')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Scenario_'  +CaseName+'.csv'), set='sc')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_LoadLevel_' +CaseName+'.csv'), set='nn')

    # combined sets
    dict_ps   = list(it.product(dictSets['pp'], dictSets['sc']))
    dict_psn  = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn']))

    # reading the data with existing network
    df_Network = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
    df_Network = df_Network.replace(0.0, float('nan'))
    dict_la = [(ni,nf,cc) for (ni,nf,cc) in df_Network.index if df_Network['Reactance'][ni,nf,cc] != 0.0 and df_Network['TTC'][ni,nf,cc] > 0.0 and df_Network['InitialPeriod'][ni,nf,cc] <= dictSets['pp'][-1] and df_Network['FinalPeriod'][ni,nf,cc] >= dictSets['pp'][0]]
    dict_le = [(ni,nf,cc) for (ni,nf,cc) in dict_la if df_Network['BinaryInvestment'][ni,nf,cc] != 'Yes']
    dict_lc = [(ni,nf,cc) for (ni,nf,cc) in dict_la if df_Network['BinaryInvestment'][ni,nf,cc] == 'Yes']

    # dataframe with the line benefits generated by the PINT approach
    df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(dict_la), index=pd.MultiIndex.from_tuples(dict_psn))

    # generating the dataset for the existing network
    print('Getting the dataset using only existing lines')
    df_Network_existing = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
    df_Network_existing = df_Network_existing.replace(0.0, float('nan'))
    df_Network_existing = df_Network_existing.drop(dict_lc, axis=0)
    print('Existing Network indices', len(df_Network_existing.index))
    df_Network_existing.to_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'))
    df_existing = Input_Dataset_Generator(DirName, CaseName)

    # restoring the original network
    print('Restoring the original network')
    print('Original Network indices', len(df_Network.index))
    df_Network.to_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'))
    print(f'Time for getting the dataset using only existing lines: {round(time.time() - start_time)} s')

    counter1 = 0
    # getting the line benefits per candidate line
    for (ni,nf,cc) in dict_lc:
        start_time = time.time()
        print("――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――")
        print(f"Line {ni} {nf} {cc}")
        print("――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――")
        df_Network_candidate = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
        df_Network_candidate = df_Network_candidate.replace(0.0, float('nan'))
        elines = [(ni, nf, cc) for (ni, nf, cc) in dict_la if df_Network['BinaryInvestment'][ni, nf, cc] != 'Yes']
        elines.append((ni, nf, cc))

        df_Network_candidate.loc[(ni, nf, cc), 'InitialPeriod'  ] = 2020
        df_Network_candidate.loc[(ni, nf, cc), 'Sensitivity'    ] = "Yes"
        df_Network_candidate.loc[(ni, nf, cc), 'InvestmentFixed'] = 1

        # selecting the lines that will be keep
        df_Network_candidate = df_Network_candidate.loc[elines]
        print(f'Number of lines to be considered: {len(df_Network_candidate.index)}')

        # Saving the CSV file with the current network
        df_Network_candidate.to_csv(_path + '/2.Par/oT_Data_Network_'+CaseName+'.csv')

        # getting the dataset for the especific candidate line
        df_line = Input_Dataset_Generator(DirName, CaseName)
        df_line.to_csv(_path + '/2.Par/oT_Data_Line_'+CaseName+'.csv')

        # getting the line benefits
        for nn in dictSets['nn']:
            df_eline = df_existing.loc[nn]
            df_cline = df_line.loc[nn]
            x1_eline = torch.from_numpy(df_eline.values)
            x2_cline = torch.from_numpy(df_cline.values)
            InputSizes  = x1_eline.shape[0]
            HiddenSizes = []
            HiddenSizes.extend([int(math.sqrt(InputSizes)), int(math.sqrt(math.sqrt(InputSizes)))])
            OutputSizes = 1
            model    = NN.ObjectiveEstimator_ANN_2hidden_layer(input_size   = InputSizes,
                                                               hidden_sizes = HiddenSizes,
                                                               output_size  = OutputSizes)
            model.load_state_dict(torch.load('model_OE_2h_12e_0.01lr_0dor.pth'))
            model.eval()
            y1_eline = model(x1_eline.float())
            y2_cline = model(x2_cline.float())

            # Convert the estimations to NumPy arrays
            line_benefit_eline = y1_eline.detach().numpy()
            line_benefit_cline = y2_cline.detach().numpy()

            # getting the line benefits
            df.loc[(dictSets['pp'], dictSets['sc'], nn), (ni, nf, cc)] = line_benefit_cline - line_benefit_eline


        # restoring the original network
        print('Restoring the original network')
        df_Network.to_csv(os.path.join(_path, '2.Par', 'oT_Data_Network_' + CaseName + '.csv'))
        print(f'Time for getting the dataset for a particular line: {round(time.time() - start_time)} s')

        counter1 += 1
        print("――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――")
        print(f'Remaining lines: {len(dict_lc)-counter1}')
        print("――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――――")




    final_time = time.time() - initial_time
    print('Total time for PINT approach:             ', round(final_time), 's')

    return df

def Input_Dataset_Generator(DirName, CaseName):
    # Initial time counter for the input dataset generation
    _path = os.path.join(DirName, CaseName)
    initial_time = time.time()
    start_time   = time.time()
    #%% Reading dictionaries from CSV files
    dictSets = DataPortal()
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Generation_'+CaseName+'.csv'), set='gg')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_LoadLevel_' +CaseName+'.csv'), set='nn')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Node_'      +CaseName+'.csv'), set='nd')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Period_'    +CaseName+'.csv'), set='pp')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Scenario_'  +CaseName+'.csv'), set='sc')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Technology_'+CaseName+'.csv'), set='tg')

    #%% Reading mappings from CSV files
    df_gen = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Generation_'+CaseName+'.csv'), index_col=[0])
    map_gen = df_gen['Technology'].to_dict()

    # Dynamic sets
    dict_techs = [tg for tg in dictSets['tg'] if tg in ['Oil','Coal','Gas','Nuclear','Hydro','Solar','Wind']]
    dict_gens  = [gg for gg in dictSets['gg'] if map_gen[gg] in dict_techs]
    # dict_gens  = [gg for gg in dictSets['gg']]

    # combined sets
    dict_ps   = list(it.product(dictSets['pp'], dictSets['sc']))
    dict_psn  = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn']))
    dict_psng = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn'], dict_gens))

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Reading    dictionaries               ... ', round(reading_sets_time), 's')

    #%% Reading data from CSV files
    # reading active power demand data
    df_org_demand = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Demand_'+CaseName+'.csv'), index_col=[0,1,2])
    df_org_demand = df_org_demand.stack().to_frame(name='Value').rename_axis(['Period','Scenario','LoadLevel','Variable'], axis=0)
    # df_org_demand['Dataset'] = 'ElectricityDemand'
    # df_org_demand = df_org_demand.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')
    df_org_demand = df_org_demand.reset_index().pivot_table(index=['LoadLevel'], columns=['Variable'], values='Value')

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Generating the electricity demand data... ', round(reading_sets_time), 's')

    # reading active power generation data
    dict_rated_max_power = df_gen['MaximumPower'].to_frame(name='Value')
    pRatedMaxPower       = dict_rated_max_power.loc[dict_gens]
    pVariableMaxPower    = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_VariableMaxGeneration_'+CaseName+'.csv'), index_col=[0,1,2])
    pVariableMaxPower    = pVariableMaxPower[dict_gens]

    pVariableMaxPower   = pVariableMaxPower.replace(0.0, float('nan'))
    pMaxPower           = pd.DataFrame([pRatedMaxPower['Value']]*len(dict_psn), index=pd.MultiIndex.from_tuples(dict_psn), columns=dict_gens)
    pMaxPower           = pMaxPower.reindex        (sorted(pMaxPower.columns        ), axis=1)
    pVariableMaxPower   = pVariableMaxPower.reindex(sorted(pVariableMaxPower.columns), axis=1)
    pMaxPower           = pVariableMaxPower.where         (pVariableMaxPower < pMaxPower, other=pMaxPower)
    pMaxPower           = pMaxPower.where                 (pMaxPower         > 0.0,       other=0.0)
    df_org_max_power    = pMaxPower.stack().to_frame(name='Value').rename_axis(['Period','Scenario','LoadLevel','Variable'], axis=0)
    # df_org_max_power['Dataset'] = 'MaxPowerGeneration'
    # df_org_max_power    = df_org_max_power.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')
    df_org_max_power    = df_org_max_power.reset_index().pivot_table(index=['LoadLevel'], columns=['Variable'], values='Value')

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Generating the max. power gen. data   ... ', round(reading_sets_time), 's')

    # reading Y matrix data
    df_Network = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
    df_Network = df_Network.replace(0.0, float('nan'))

    # number of nodes
    size  = len(dictSets['nd'])
    # Convert the set to a list
    nodes = list(dictSets['nd'])
    nodes.sort()
    admittance_matrix = np.zeros((size, size), dtype=complex)
    # Iterate over each row in the DataFrame and populate the admittance matrix
    dict_la = [(ni,nf,cc) for (ni,nf,cc) in df_Network.index if df_Network['Reactance'][ni,nf,cc] != 0.0 and df_Network['TTC'][ni,nf,cc] > 0.0 and df_Network['InitialPeriod'][ni,nf,cc] <= dictSets['pp'][-1] and df_Network['FinalPeriod'][ni,nf,cc] >= dictSets['pp'][0]]
    print(dict_la)
    for (ni,nf,cc) in dict_la:
        index_1 = 0
        index_2 = 0
        reactance   = df_Network['Reactance' ][ni,nf,cc]
        resistance  = df_Network['Resistance'][ni,nf,cc]
        tap         = df_Network['Tap'       ][ni,nf,cc]

        # find the index of the nodes in the admittance matrix
        index_1 = nodes.index(ni)
        index_2 = nodes.index(nf)

        admittance = 1 / (resistance + reactance * 1j)
        admittance_matrix[index_1][index_2] = admittance_matrix[index_1][index_2] + admittance * tap
        admittance_matrix[index_2][index_1] = admittance_matrix[index_1][index_2]

    # Calculate the diagonal elements
    for i in range(size):
        for (ni,nf,cc) in dict_la:
            index_1     = 0
            index_2     = 0
            susceptance = df_Network['Susceptance'][ni,nf,cc]
            tap         = df_Network['Tap'        ][ni,nf,cc]
            index_1     = nodes.index(ni)
            index_2     = nodes.index(nf)

            if index_1 == i:
                admittance_matrix[i][i] = admittance_matrix[i][i] + admittance_matrix[index_1][index_2] * tap ** 2 + susceptance * 1j
            elif index_2 == i:
                admittance_matrix[i][i] = admittance_matrix[i][i] + admittance_matrix[index_1][index_2] + susceptance * 1j

    admittance_matrix[np.isnan(admittance_matrix)] = 0
    # df = pd.DataFrame(admittance_matrix).stack().reset_index()
    # df.columns = ['Node1', 'Node2', 'Admittance']
    # df.set_index(['Node1', 'Node2'], inplace=True)
    df_Y_matrix_real = pd.DataFrame(index=pd.MultiIndex.from_tuples(dict_psn))
    df_Y_matrix_imag = pd.DataFrame(index=pd.MultiIndex.from_tuples(dict_psn))

    for (ni,nf) in itertools.product(range(admittance_matrix.shape[0]), range(admittance_matrix.shape[1])):
        # df1 = pd.DataFrame([np.real(df['Admittance'][ni, nf])]*len(dict_psn), columns=['Node_' + str(ni + 1) + '_Node_' + str(nf + 1)] ,index=pd.MultiIndex.from_tuples(dict_psn))
        # df2 = pd.DataFrame([np.imag(df['Admittance'][ni, nf])]*len(dict_psn), columns=['Node_' + str(ni + 1) + '_Node_' + str(nf + 1)] ,index=pd.MultiIndex.from_tuples(dict_psn))
        df1 = pd.DataFrame([np.real(admittance_matrix[ni][nf])]*len(dict_psn), columns=['Node_' + str(ni + 1) + '_Node_' + str(nf + 1)] ,index=pd.MultiIndex.from_tuples(dict_psn))
        df2 = pd.DataFrame([np.imag(admittance_matrix[ni][nf])]*len(dict_psn), columns=['Node_' + str(ni + 1) + '_Node_' + str(nf + 1) + '_I'] ,index=pd.MultiIndex.from_tuples(dict_psn))
        df_Y_matrix_real = pd.concat([df_Y_matrix_real, df1], axis=1)
        df_Y_matrix_imag = pd.concat([df_Y_matrix_imag, df2], axis=1)

    df_Y_matrix_real = df_Y_matrix_real.stack()
    df_Y_matrix_real.index.names = ['Period', 'Scenario', 'LoadLevel', 'Variable']
    df_Y_matrix_real = df_Y_matrix_real.to_frame(name='Value')
    # df_Y_matrix_real['Dataset'] = 'MatrixYReal'
    # df_Y_matrix_real = df_Y_matrix_real.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')
    df_Y_matrix_real = df_Y_matrix_real.reset_index().pivot_table(index=['LoadLevel'], columns=['Variable'], values='Value')

    df_Y_matrix_imag = df_Y_matrix_imag.stack()
    df_Y_matrix_imag.index.names = ['Period', 'Scenario', 'LoadLevel', 'Variable']
    df_Y_matrix_imag = df_Y_matrix_imag.to_frame(name='Value')
    # df_Y_matrix_imag['Dataset'] = 'MatrixYImag'
    # df_Y_matrix_imag = df_Y_matrix_imag.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')
    df_Y_matrix_imag = df_Y_matrix_imag.reset_index().pivot_table(index=['LoadLevel'], columns=['Variable'], values='Value')

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Generating the Y matrix data          ... ', round(reading_sets_time), 's')

    # Merging all the data
    df_input_data = pd.concat([df_org_demand, df_Y_matrix_real, df_Y_matrix_imag, df_org_max_power], axis=1)
    df_input_data = df_input_data.reindex(sorted(df_input_data.columns), axis=1)

    # Normalizing the data
    for col1 in df_input_data.columns:
        min_value = df_input_data[col1].min()
        max_value = df_input_data[col1].max()
        if max_value != min_value:
            df_input_data[col1] = (df_input_data[col1] - min_value) / (max_value - min_value)
        else:
            df_input_data[col1] = df_input_data[col1] / max_value

    reading_sets_time = time.time() - initial_time
    print('Dataset generation                    ... ', round(reading_sets_time), 's')

    return df_input_data

if __name__ == '__main__':
    main()