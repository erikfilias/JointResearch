# This script generates the data for the clustering task.
# The data is generated by executing the neural network and using the input data of the StarNet model.

#%% Imports
import argparse
import os
import time
import itertools as it
import numpy  as np
import pandas as pd
from   pyomo.environ import DataPortal, Set

#%% Defining metadata
parser = argparse.ArgumentParser(description='Introducing main parameters...')
parser.add_argument('--case',   type=str, default=None)
parser.add_argument('--dir',    type=str, default=None)
parser.add_argument('--solver', type=str, default=None)

# Setting the path to the data
DIR    = os.path.dirname(__file__)
CASE   = '9n'
SOLVER = 'gurobi'

# Defining the main function
def main():
    args = parser.parse_args()
    if args.dir is None:
        args.dir    = input('Input Dir    Name (Default {}): '.format(DIR))
        if args.dir == '':
            args.dir = DIR
    if args.case is None:
        args.case   = input('Input Case   Name (Default {}): '.format(CASE))
        if args.case == '':
            args.case = CASE
    if args.solver is None:
        args.solver = input('Input Solver Name (Default {}): '.format(SOLVER))
        if args.solver == '':
            args.solver = SOLVER
    print(args.case)
    print(args.dir)
    print(args.solver)
    import sys
    print(sys.argv)
    print(args)

    #%% Reading data from CSV files
    _path      = os.path.join(args.dir, args.case)
    # Initial time counter for all the code
    initial_time = time.time()
    df = PINT_approach(args.dir, args.case)

def PINT_approach(DirName, CaseName):
    # building the path to the data
    _path = os.path.join(DirName, CaseName)
    # initial time counter for PINT approach
    initial_time = time.time()
    start_time   = time.time()
    # reading dictionaries from CSV files
    dictSets = DataPortal()
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Period_'    +CaseName+'.csv'), set='pp')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Scenario_'  +CaseName+'.csv'), set='sc')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_LoadLevel_' +CaseName+'.csv'), set='nn')

    # combined sets
    dict_ps   = list(it.product(dictSets['pp'], dictSets['sc']))
    dict_psn  = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn']))

    # reading the data with existing network
    df_Network = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
    df_Network = df_Network.replace(0.0, float('nan'))
    dict_la = [(ni,nf,cc) for (ni,nf,cc) in df_Network.index if df_Network['Reactance'][ni,nf,cc] != 0.0 and df_Network['TTC'][ni,nf,cc] > 0.0 and df_Network['InitialPeriod'][ni,nf,cc] <= dictSets['pp'][-1] and df_Network['FinalPeriod'][ni,nf,cc] >= dictSets['pp'][0]]
    dict_le = [(ni,nf,cc) for (ni,nf,cc) in dict_la if df_Network['BinaryInvestment'][ni,nf,cc] != 'Yes']
    dict_lc = [(ni,nf,cc) for (ni,nf,cc) in dict_la if df_Network['BinaryInvestment'][ni,nf,cc] == 'Yes']

    # dataframe with the line benefits generated by the PINT approach
    df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(dict_la), index=pd.MultiIndex.from_tuples(dict_psn))

    # generating the dataset for the existing network
    print('Getting the dataset using only existing lines')
    df_Network_existing = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
    df_Network_existing = df_Network_existing.replace(0.0, float('nan'))
    df_Network_existing = df_Network_existing.drop(dict_lc, axis=0)
    print('Existing Network indices', list(df_Network_existing.index))
    df_Network_existing.to_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'))
    df_existing = Input_Dataset_Generator(DirName, CaseName)

    # restoring the original network
    print('Restoring the original network')
    print('Original Network indices', list(df_Network.index))
    df_Network.to_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'))
    print(f'Time for getting the dataset using only existing lines: {round(time.time() - start_time)} s')
    start_time = time.time()

    # getting the line benefits per candidate line
    # for (ni,nf,cc) in dict_lc:


    final_time = time.time() - initial_time
    print('Total time for PINT approach:             ', round(final_time), 's')

    return df

def Input_Dataset_Generator(DirName, CaseName):
    # Initial time counter for the input dataset generation
    _path = os.path.join(DirName, CaseName)
    initial_time = time.time()
    start_time   = time.time()
    #%% Reading dictionaries from CSV files
    dictSets = DataPortal()
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Generation_'+CaseName+'.csv'), set='gg')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_LoadLevel_' +CaseName+'.csv'), set='nn')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Node_'      +CaseName+'.csv'), set='nd')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Period_'    +CaseName+'.csv'), set='pp')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Scenario_'  +CaseName+'.csv'), set='sc')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Technology_'+CaseName+'.csv'), set='tg')

    #%% Reading mappings from CSV files
    df_gen = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Generation_'+CaseName+'.csv'), index_col=[0])
    map_gen = df_gen['Technology'].to_dict()

    # Dynamic sets
    dict_techs = [tg for tg in dictSets['tg'] if tg in ['Hydro','Solar','Wind']]
    dict_gens  = [gg for gg in dictSets['gg'] if map_gen[gg] in dict_techs]

    # combined sets
    dict_ps   = list(it.product(dictSets['pp'], dictSets['sc']))
    dict_psn  = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn']))
    dict_psng = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn'], dict_gens))

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Reading    dictionaries               ... ', round(reading_sets_time), 's')

    #%% Reading data from CSV files
    # reading active power demand data
    df_org_demand = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Demand_'+CaseName+'.csv'), index_col=[0,1,2])
    df_org_demand = df_org_demand.stack().to_frame(name='Value').rename_axis(['Period','Scenario','LoadLevel','Variable'], axis=0)
    df_org_demand['Dataset'] = 'ElectricityDemand'
    df_org_demand = df_org_demand.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Generating the electricity demand data... ', round(reading_sets_time), 's')

    # reading active power generation data
    dict_rated_max_power = df_gen['MaximumPower'].to_frame(name='Value')
    pRatedMaxPower       = dict_rated_max_power.loc[dict_gens]
    pVariableMaxPower    = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_VariableMaxGeneration_'+CaseName+'.csv'), index_col=[0,1,2])
    pVariableMaxPower    = pVariableMaxPower[dict_gens]

    pVariableMaxPower   = pVariableMaxPower.replace(0.0, float('nan'))
    pMaxPower           = pd.DataFrame([pRatedMaxPower['Value']]*len(dict_psn), index=pd.MultiIndex.from_tuples(dict_psn), columns=dict_gens)
    pMaxPower           = pMaxPower.reindex        (sorted(pMaxPower.columns        ), axis=1)
    pVariableMaxPower   = pVariableMaxPower.reindex(sorted(pVariableMaxPower.columns), axis=1)
    pMaxPower           = pVariableMaxPower.where         (pVariableMaxPower < pMaxPower, other=pMaxPower)
    pMaxPower           = pMaxPower.where                 (pMaxPower         > 0.0,       other=0.0)
    df_org_max_power    = pMaxPower.stack().to_frame(name='Value').rename_axis(['Period','Scenario','LoadLevel','Variable'], axis=0)
    df_org_max_power['Dataset'] = 'MaxPowerGeneration'
    df_org_max_power    = df_org_max_power.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Generating the max. power gen. data   ... ', round(reading_sets_time), 's')

    # reading Y matrix data
    df_Network = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Network_'+CaseName+'.csv'), index_col=[0,1,2])
    df_Network = df_Network.replace(0.0, float('nan'))

    # number of nodes
    size  = len(dictSets['nd'])
    # Convert the set to a list
    nodes = list(dictSets['nd'])
    nodes.sort()
    admittance_matrix = np.zeros((size, size), dtype=complex)
    # Iterate over each row in the DataFrame and populate the admittance matrix
    dict_la = [(ni,nf,cc) for (ni,nf,cc) in df_Network.index if df_Network['Reactance'][ni,nf,cc] != 0.0 and df_Network['TTC'][ni,nf,cc] > 0.0 and df_Network['InitialPeriod'][ni,nf,cc] <= dictSets['pp'][-1] and df_Network['FinalPeriod'][ni,nf,cc] >= dictSets['pp'][0]]
    print(dict_la)
    for (ni, nf, cc) in dict_la:
        index_1 = 0
        index_2 = 0
        reactance   = df_Network['Reactance' ][ni,nf,cc]
        resistance  = df_Network['Resistance'][ni,nf,cc]
        tap         = df_Network['Tap'       ][ni,nf,cc]

        # find the index of the nodes in the admittance matrix
        index_1 = nodes.index(ni)
        index_2 = nodes.index(nf)

        admittance = 1 / (resistance + reactance * 1j)
        admittance_matrix[index_1][index_2] = admittance_matrix[index_1][index_2] + admittance * tap
        admittance_matrix[index_2][index_1] = admittance_matrix[index_1][index_2]

    # Calculate the diagonal elements
    for i in range(size):
        for (ni,nf,cc) in dict_la:
            index_1 = 0
            index_2 = 0
            index_1 = nodes.index(ni)
            index_2 = nodes.index(nf)
            susceptance = df_Network['Susceptance'][ni,nf,cc]
            tap         = df_Network['Tap'        ][ni,nf,cc]
            if index_1 == i:
                admittance_matrix[i][i] = admittance_matrix[i][i] + admittance_matrix[index_1][index_2] * tap ** 2 + susceptance * 1j
            elif index_2 == i:
                admittance_matrix[i][i] = admittance_matrix[i][i] + admittance_matrix[index_1][index_2]            + susceptance * 1j

    df = pd.DataFrame(admittance_matrix).stack().reset_index()
    df.columns = ['Node1', 'Node2', 'Admittance']
    df.set_index(['Node1', 'Node2'], inplace=True)
    df_Y_matrix_real = pd.DataFrame(index=pd.MultiIndex.from_tuples(dict_psn))
    df_Y_matrix_imag = pd.DataFrame(index=pd.MultiIndex.from_tuples(dict_psn))

    for (ni, nf) in df.index:
        df1 = pd.DataFrame([np.real(df['Admittance'][ni, nf])]*len(dict_psn), columns=['Node_' + str(ni + 1) + '_Node_' + str(nf + 1)] ,index=pd.MultiIndex.from_tuples(dict_psn))
        df2 = pd.DataFrame([np.imag(df['Admittance'][ni, nf])]*len(dict_psn), columns=['Node_' + str(ni + 1) + '_Node_' + str(nf + 1)] ,index=pd.MultiIndex.from_tuples(dict_psn))
        df_Y_matrix_real = pd.concat([df_Y_matrix_real, df1], axis=1)
        df_Y_matrix_imag = pd.concat([df_Y_matrix_imag, df2], axis=1)

    df_Y_matrix_real = df_Y_matrix_real.stack()
    df_Y_matrix_real.index.names = ['Period', 'Scenario', 'LoadLevel', 'Variable']
    df_Y_matrix_real = df_Y_matrix_real.to_frame(name='Value')
    df_Y_matrix_real['Dataset'] = 'MatrixYReal'
    df_Y_matrix_real = df_Y_matrix_real.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')

    df_Y_matrix_imag = df_Y_matrix_imag.stack()
    df_Y_matrix_imag.index.names = ['Period', 'Scenario', 'LoadLevel', 'Variable']
    df_Y_matrix_imag = df_Y_matrix_imag.to_frame(name='Value')
    df_Y_matrix_imag['Dataset'] = 'MatrixYImag'
    # df_Y_matrix_imag['Execution'] = execution
    df_Y_matrix_imag = df_Y_matrix_imag.reset_index().pivot_table(index=['LoadLevel'], columns=['Dataset','Variable'], values='Value')

    # for (p, sc, n) in dict_psn:
    #     for (ni, nf) in df.index:
    #         df_Y_matrix.loc[(p, sc, n), 'Node_' + str(ni + 1) + '_Node_' + str(nf + 1)] = df['Admittance'][ni, nf]

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Generating the Y matrix data          ... ', round(reading_sets_time), 's')

    # Merging all the data
    df_input_data = pd.concat([df_org_demand, df_Y_matrix_real, df_Y_matrix_imag, df_org_max_power], axis=1)

    # Normalizing the data
    for col1,col2 in df_input_data.columns:
        min_value = df_input_data[col1,col2].min()
        max_value = df_input_data[col1,col2].max()
        if max_value != min_value:
            df_input_data[col1,col2] = (df_input_data[col1,col2] - min_value) / (max_value - min_value)
        else:
            df_input_data[col1,col2] = df_input_data[col1,col2] / max_value

    reading_sets_time = time.time() - initial_time
    print('Dataset generation                    ... ', round(reading_sets_time), 's')

    return df_input_data

if __name__ == '__main__':
    main()