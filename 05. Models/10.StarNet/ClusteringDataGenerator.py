# This script generates the data for the clustering task.
# The data is generated by executing the neural network and using the input data of the StarNet model.

#%% Imports
import argparse
import os
import time
import itertools as it
import numpy  as np
import pandas as pd
from   pyomo.environ import DataPortal, Set

#%% Defining metadata
parser = argparse.ArgumentParser(description='Introducing main parameters...')
parser.add_argument('--case',   type=str, default=None)
parser.add_argument('--dir',    type=str, default=None)
parser.add_argument('--solver', type=str, default=None)

# Setting the path to the data
DIR    = os.path.dirname(__file__)
CASE   = '9n'
SOLVER = 'gurobi'

# Defining the main function
def main():
    args = parser.parse_args()
    if args.dir is None:
        args.dir    = input('Input Dir    Name (Default {}): '.format(DIR))
        if args.dir == '':
            args.dir = DIR
    if args.case is None:
        args.case   = input('Input Case   Name (Default {}): '.format(CASE))
        if args.case == '':
            args.case = CASE
    if args.solver is None:
        args.solver = input('Input Solver Name (Default {}): '.format(SOLVER))
        if args.solver == '':
            args.solver = SOLVER
    print(args.case)
    print(args.dir)
    print(args.solver)
    import sys
    print(sys.argv)
    print(args)

    #%% Reading data from CSV files
    _path      = os.path.join(args.dir, args.case)
    # Initial time counter for all the code
    initial_time = time.time()
    df = PINT_approach(args.dir, args.case)

def PINT_approach(DirName, CaseName):
    # Initial time counter for all the code
    _path = os.path.join(DirName, CaseName)
    initial_time = time.time()
    start_time   = time.time()
    #%% Reading dictionaries from CSV files
    dictSets = DataPortal()
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Generation_'+CaseName+'.csv'), set='gg')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_LoadLevel_' +CaseName+'.csv'), set='nn')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Node_'      +CaseName+'.csv'), set='nd')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Period_'    +CaseName+'.csv'), set='pp')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Scenario_'  +CaseName+'.csv'), set='sc')
    dictSets.load(filename=os.path.join(_path, '1.Set','oT_Dict_Technology_'+CaseName+'.csv'), set='tg')

    #%% Reading mappings from CSV files
    df_gen = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Generation_'+CaseName+'.csv'), index_col=[0])
    map_gen = df_gen['Technology'].to_dict()

    # Dynamic sets
    dict_techs = [tg for tg in dictSets['tg'] if tg in ['Hydro','Solar','Wind']]
    dict_gens  = [gg for gg in dictSets['gg'] if map_gen[gg] in dict_techs]

    # combined sets
    dict_ps   = list(it.product(dictSets['pp'], dictSets['sc']))
    dict_psn  = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn']))
    dict_psng = list(it.product(dictSets['pp'], dictSets['sc'], dictSets['nn'], dict_gens))

    reading_sets_time = time.time() - start_time
    start_time        = time.time()
    print('Reading    dictionaries               ... ', round(reading_sets_time), 's')

    #%% Reading data from CSV files
    # reading active power demand data
    df_org_demand = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_Demand_'+CaseName+'.csv'), index_col=[0,1,2])
    df_org_demand = df_org_demand.stack().to_frame(name='Value').rename_axis(['Period','Scenario','LoadLevel','variable'], axis=0)
    df_org_demand['Dataset'] = 'ElectricityDemand'
    # df_org_demand['Execution'] = execution

    # reading active power generation data
    dict_rated_max_power = df_gen['MaximumPower'].to_frame(name='Value')
    pRatedMaxPower       = dict_rated_max_power.loc[dict_gens]
    # pRatedMaxPower       = [i for i in dict_rated_max_power if i in dict_gens]
    pVariableMaxPower    = pd.read_csv(os.path.join(_path, '2.Par','oT_Data_VariableMaxGeneration_'+CaseName+'.csv'), index_col=[0,1,2])
    pVariableMaxPower    = pVariableMaxPower[dict_gens]
    # dict_rated_max_power = [i for i in dict_rated_max_power if not (i['id'] == 2)]

    pVariableMaxPower   = pVariableMaxPower.replace(0.0, float('nan'))
    pMaxPower           = pd.DataFrame([pRatedMaxPower['Value']]*len(dict_psn), index=pd.MultiIndex.from_tuples(dict_psn), columns=dict_gens)
    pMaxPower           = pMaxPower.reindex        (sorted(pMaxPower.columns        ), axis=1)
    pVariableMaxPower   = pVariableMaxPower.reindex(sorted(pVariableMaxPower.columns), axis=1)
    pMaxPower           = pVariableMaxPower.where         (pVariableMaxPower < pMaxPower, other=pMaxPower)
    pMaxPower           = pMaxPower.where                 (pMaxPower         > 0.0,       other=0.0)

    return df_org_demand

if __name__ == '__main__':
    main()