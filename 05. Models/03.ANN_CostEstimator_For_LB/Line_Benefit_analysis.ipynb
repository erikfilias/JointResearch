{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc25a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import DataLoading\n",
    "import NN_classes\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b3b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"../Data/3-bus_AC_12w_ext_o_dummy_LCOE/\"\n",
    "all_executions = DataLoading.list_executions(folder=folder,per = period,sc=sc)\n",
    "te_s = 0.3\n",
    "val_s = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865568f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_1_Node_2_cac1_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_1_Node_2_cac2_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_1_Node_2_cac3_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_1_Node_3_cac1_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_1_Node_3_cac2_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_1_Node_3_cac3_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_2_Node_3_cac1_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_2_Node_3_cac2_2030.csv\n",
      "8\n",
      "input_f_sc01_Network_Line_In_Node_2_Node_3_cac3_2030.csv\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dfs_in, dfs_out, dfs_inter = DataLoading.load_data_ext_out(folder, all_executions, period, sc, [\"PowerFlow\"])\n",
    "dfs_inter_j = DataLoading.join_frames_inter_layer(dfs_inter)\n",
    "dfs_inter_j = DataLoading.trim_columns_to_common(dfs_inter_j)\n",
    "# Convert to pytorch tensors\n",
    "ts_in, ts_out, ts_inter = DataLoading.split_tr_val_te_ext_out(dfs_in, dfs_out, dfs_inter_j, all_executions, te_s, val_s)\n",
    "d_ft_in, d_ft_out, d_ft_inter,maxs = DataLoading.concat_and_normalize_ext_out(ts_in, ts_out, ts_inter, all_executions)\n",
    "\n",
    "input_size = dfs_in[\"Network_Existing_Generation_Full\"].shape[1]\n",
    "inter_size = dfs_inter_j[\"Network_Existing_Generation_Full\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9191452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lb_from_dfs_out(dfs_out,execution): \n",
    "    return (dfs_out[\"Network_Existing_Generation_Full\"].sum(axis=1) - dfs_out[execution].sum(axis=1))\n",
    "\n",
    "def calculate_lb_from_ts_out(ts_out,ex):\n",
    "    b= \"Network_Existing_Generation_Full\"\n",
    "    all_ts_out_ex = torch.concat((ts_out[\"train\"][ex],ts_out[\"test\"][ex],ts_out[\"val\"][ex]))\n",
    "    all_ts_out_benchmark = torch.concat((ts_out[\"train\"][b],ts_out[\"test\"][b],ts_out[\"val\"][b]))\n",
    "    return all_ts_out_benchmark-all_ts_out_ex\n",
    "    \n",
    "\n",
    "def find_xthbest_model_params_from_df(df_losses,loss_to_sort,xth_best=1):\n",
    "    return df_losses.sort_values(by =loss_to_sort)[xth_best-1:xth_best]\n",
    "\n",
    "def extract_model_params_from_row(row):\n",
    "    model_type = row.Model_type.item()\n",
    "    model_type = tuple(map(int, model_type.replace(\"(\",\"\").replace(\")\",\"\").split(', ')))\n",
    "    dor = row.Dor.item()\n",
    "    lr = row.Lr.item()\n",
    "    nb_e = row.Epochs.item()\n",
    "\n",
    "    relu_out = row.Relu_out.item()\n",
    "    np =row.Np.item()\n",
    "    bs = row.Batch_size.item()\n",
    "    alpha = row.alpha.item()\n",
    "    MAE = row.MAE.item()\n",
    "    \n",
    "    return {\"Model_type\": model_type,\"nb_e\":nb_e,\"lr\":lr,\"dor\":dor,\"np\":np,\"ro\":relu_out,\"bs\":bs,\"alpha\":alpha,\"MAE\":MAE}\n",
    "    \n",
    "\n",
    "def create_model_and_load_state_from_row(row,input_size,inter_size,hyperloop_name,cluster_run = True):\n",
    "    #First, extract params from row\n",
    "    \n",
    "    model_type = row.Model_type.item()\n",
    "    model_type = tuple(map(int, model_type.replace(\"(\",\"\").replace(\")\",\"\").split(', ')))\n",
    "    dor = row.Dor.item()\n",
    "    lr = row.Lr.item()\n",
    "    nb_e = row.Epochs.item()\n",
    "\n",
    "    relu_out = row.Relu_out.item()\n",
    "    np =row.Np.item()\n",
    "    bs = row.Batch_size.item()\n",
    "    alpha = row.alpha.item()\n",
    "    MAE = row.MAE.item()\n",
    "    \n",
    "    if str(alpha) == \"0.0\": \n",
    "        alpha = \"0\"\n",
    "\n",
    "    if row.Min_val.item(): \n",
    "        mt = \"min_val\"\n",
    "    else: \n",
    "        mt = \"all_epochs\"\n",
    "    \n",
    "    #Then create model of given type\n",
    "    m = NN_classes.create_model(model_type,input_size, dropout_ratio= dor,relu_out =relu_out,inter=True,inter_size=inter_size)\n",
    "    \n",
    "    #Finally, extract model state from dict\n",
    "    \n",
    "    #m_name = f\"OE_{model_type}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro_{bs}bs\"\n",
    "    m_name = f\"OE_{model_type}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}ro_{bs}bs_{alpha}ill_{MAE}MAE\"\n",
    "\n",
    "    if cluster_run:\n",
    "        #m_name = f\"OE_{model_type}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro_{bs}bs\"\n",
    "        path = f\"ResultsClusterRuns/trained_models/{hyperloop_name}/{mt}/model_{m_name}.pth\"\n",
    "    else:\n",
    "        path = f\"trained_models/{hyperloop_name}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "    m.load_state_dict(torch.load(path))\n",
    "    m.eval()\n",
    "    \n",
    "    return m\n",
    "def get_lb_est_and_actual(m,ex,dfs_in,dfs_out): \n",
    "    negf = all_executions[0]\n",
    "    ex_in_e = torch.nan_to_num(dfs_in[ex].to_numpy()/maxs[\"in\"])\n",
    "    ex_in_negf = torch.nan_to_num(dfs_in[negf].to_numpy()/maxs[\"in\"])\n",
    "\n",
    "    prediction_e = m(ex_in_e.float())[0].detach().numpy()\n",
    "    prediction_negf = m(ex_in_negf.float())[0].detach().numpy()\n",
    "\n",
    "    lb_est = prediction_negf- prediction_e\n",
    "    lb_actual = calculate_lb_from_dfs_out(dfs_out,ex).to_numpy()\n",
    "    return lb_est.flatten(),lb_actual.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95264dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 5, 3]\n"
     ]
    }
   ],
   "source": [
    "#Counting ocurences of negative and positive values for all execs: \n",
    "#hyperloop_name = \"RTS24_AC_12w_dummy_0.3_v0.4_PF_sa_rand_0_40\"\n",
    "hyperloop_name =\"3-bus_AC_12w_dummy_0.3_v0.4_PF_LCOE_0_10\"\n",
    "df_losses = pd.read_csv(f\"ResultsClusterRuns/Loss_results_csv/{hyperloop_name}.csv\",index_col=0)\n",
    "#df_losses = pd.read_csv(f\"Loss_results_csv/{hyperloop_name}.csv\",index_col=0)\n",
    "\n",
    "f = df_losses.alpha ==0.16\n",
    "\n",
    "loss_to_sort = \"Te_l_t_mse\"\n",
    "\n",
    "xth_best = 1\n",
    "row = find_xthbest_model_params_from_df(df_losses[f],loss_to_sort,xth_best)\n",
    "m = create_model_and_load_state_from_row(row,input_size,inter_size,hyperloop_name,cluster_run=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f09846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "ex = all_executions[0]\n",
    "\n",
    "all_counts = pd.DataFrame()\n",
    "for ex in all_executions:\n",
    "    lb_est,lb_actual = get_lb_est_and_actual(m,ex,dfs_in,dfs_out)\n",
    "\n",
    "    both_positive_count = np.sum((lb_est > 0) & (lb_actual > 0))\n",
    "    both_negative_count = np.sum((lb_est < 0) & (lb_actual < 0))\n",
    "    opposite_sign_count = np.sum((lb_est * lb_actual) < 0)\n",
    "    BZ = np.sum((lb_est==0) & (lb_actual == 0))\n",
    "    OZ = np.sum((lb_est * lb_actual) == 0)\n",
    "\n",
    "\n",
    "    row_lb = pd.DataFrame({\"exec\":ex,\"BP\":both_positive_count, \"BN\":both_negative_count,\"OS\":opposite_sign_count,\"OZ\":OZ,\"BZ\":BZ,}, index = [i])\n",
    "    all_counts = pd.concat([all_counts,row_lb],axis=0)\n",
    "    i+=1\n",
    "# plt.bar(height = [both_positive_count,both_negative_count,opposite_sign_count],x=[0,1,2],xlabels =[\"both_positive\",\"both_negative\",\"Opposite sign\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db83eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = model_params_dict = extract_model_params_from_row(row)\n",
    "keys = [\"Model_type\",\"nb_e\",\"lr\",\"dor\",\"np\",\"ro\",\"bs\",\"alpha\",\"MAE\"]\n",
    "name = f\"sign_counts_{mpd[keys[0]]}h_{mpd[keys[1]]}e_{mpd[keys[2]]}lr_{mpd[keys[3]]}dor_{mpd[keys[4]]}np_{mpd[keys[5]]}ro_{mpd[keys[6]]}bs_{mpd[keys[7]]}ill_{mpd[keys[8]]}MAE\"\n",
    "all_counts.to_csv(f\"LB_sign_counts/{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7284c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exec    Network_Existing_Generation_FullNetwork_Line_I...\n",
       "BP                                                   3105\n",
       "BN                                                   8086\n",
       "OS                                                   6536\n",
       "OZ                                                   4113\n",
       "BZ                                                   2184\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0063f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((lb_est > 0) & (lb_actual > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da47c4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "536"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((lb_est > 0) & (lb_actual > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d9ba14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.67787075e-05, 1.67414546e-05, 1.56611204e-05, ...,\n",
       "       1.58406794e-04, 1.11535192e-05, 1.40964985e-05], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lb_est > 0) & (lb_actual > 0)\n",
    "lb_est.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
