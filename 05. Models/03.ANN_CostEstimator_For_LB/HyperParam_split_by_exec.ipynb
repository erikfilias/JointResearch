{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb899fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoading\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import NN_classes\n",
    "import training_methods\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"../Data/RTS24_AC_12w\"\n",
    "all_executions = DataLoading.list_executions(folder=\"../Data/RTS24_AC_12w\",per = period,sc=sc)\n",
    "len(all_executions)\n",
    "executions = all_executions[0:40]\n",
    "te_s = 0.3\n",
    "val_s = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aa0d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_102_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_103_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_105_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_104_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_106_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_124_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_104_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_105_N_110_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_106_N_108_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_106_N_110_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_107_N_108_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_108_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_108_N_110_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_109_N_111_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_109_N_112_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_110_N_111_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_110_N_112_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_111_N_113_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_111_N_114_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_112_N_113_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_112_N_123_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_113_N_123_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_114_N_116_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_116_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_121_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_121_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_124_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_116_N_117_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_116_N_119_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_117_N_118_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_117_N_122_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_118_N_121_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_118_N_121_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_119_N_120_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_119_N_120_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_120_N_123_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_120_N_123_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_121_N_122_cac1_2030.csv\n",
      "1227\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6671c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te_by_exec(dfs_in,dfs_out,executions,te_s,val_s,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93163ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out,maxs = DataLoading.concat_and_normalize_split_by_exec(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f72c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "# training_loader = DataLoader(train,batch_size=64)\n",
    "# validation_loader = DataLoader(train,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae78bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0, False, 32, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.437943589944384e-05 valid 3.590268079278758e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.2592551889781663e-06 valid 3.3078749765991233e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.565684311881058e-06 valid 3.524519797792891e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.0639243361929636e-06 valid 3.302867071397486e-06\n",
      "(3, 0, False, 32, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 6.198949386649581e-06 valid 2.6007833184849005e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.3826297971388616e-06 valid 2.57300462180865e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.8333557272393564e-06 valid 3.5989144180348376e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.4927182001834666e-06 valid 4.06504159400356e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.6193706382193982e-06 valid 1.1553961485333275e-06\n",
      "EPOCH 6:\n",
      "LOSS train 9.497186442737839e-07 valid 6.276806630012288e-07\n",
      "EPOCH 7:\n",
      "LOSS train 5.124843583482235e-07 valid 3.141404931739089e-07\n",
      "EPOCH 8:\n",
      "LOSS train 5.549709829581317e-07 valid 2.4751687988100457e-07\n",
      "(3, 0, False, 32, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 4.410360264498181e-05 valid 3.1627757834939985e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.8855758254805276e-06 valid 1.502372697359533e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.4527686878142114e-06 valid 1.4787889313083724e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.2489095570428176e-06 valid 9.270812029171793e-07\n",
      "EPOCH 5:\n",
      "LOSS train 1.114706207523479e-06 valid 1.0762515785245341e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.0539584114717023e-06 valid 8.859564104568562e-07\n",
      "EPOCH 7:\n",
      "LOSS train 1.2054384738013462e-06 valid 1.5462600231330725e-06\n",
      "EPOCH 8:\n",
      "LOSS train 9.955789564413566e-07 valid 1.2126622550567845e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.694266298203914e-06 valid 8.496192549500847e-07\n",
      "EPOCH 10:\n",
      "LOSS train 9.647182666872855e-07 valid 9.143291208602022e-07\n",
      "EPOCH 11:\n",
      "LOSS train 7.688264198081444e-07 valid 7.002101369835145e-07\n",
      "EPOCH 12:\n",
      "LOSS train 5.106018791792181e-07 valid 5.330583690010826e-07\n",
      "(3, 0, False, 32, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.944407634423341e-05 valid 3.058068614336662e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.2816276527749507e-06 valid 2.41600150729937e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.82895119793955e-06 valid 1.8950723870148067e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.5856639727908502e-06 valid 1.19669311970938e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.390964214216321e-06 valid 2.4372563984798035e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.3725535650610607e-06 valid 1.6301559071507654e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.759574216563589e-06 valid 2.7013945782528026e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.301636226337062e-06 valid 1.792874400052824e-06\n",
      "EPOCH 9:\n",
      "LOSS train 8.727107713111021e-07 valid 1.7889346963784192e-06\n",
      "EPOCH 10:\n",
      "LOSS train 4.904648647337867e-07 valid 6.171106292640616e-07\n",
      "EPOCH 11:\n",
      "LOSS train 5.313657257122729e-07 valid 4.416726255840331e-07\n",
      "EPOCH 12:\n",
      "LOSS train 3.8346093188344584e-07 valid 2.3222901290864684e-07\n",
      "EPOCH 13:\n",
      "LOSS train 3.134184740929612e-07 valid 2.4905219220272556e-07\n",
      "EPOCH 14:\n",
      "LOSS train 2.358413183262069e-07 valid 4.14155721273346e-07\n",
      "EPOCH 15:\n",
      "LOSS train 2.719065687862182e-07 valid 2.1115046422437445e-07\n",
      "EPOCH 16:\n",
      "LOSS train 2.495873222113628e-07 valid 6.244012524803111e-07\n",
      "(3, 0, False, 32, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 4.775680153773044e-05 valid 2.302080702065723e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.9683681488215586e-06 valid 3.166417172906222e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.0219358933983403e-06 valid 4.421526682563126e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.2235756553793796e-06 valid 1.266356434825866e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.341319667048167e-06 valid 1.3094571613692096e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.0893105075072148e-06 valid 1.1889462712133536e-06\n",
      "EPOCH 7:\n",
      "LOSS train 8.980784646022809e-07 valid 1.0349274361942662e-06\n",
      "EPOCH 8:\n",
      "LOSS train 7.999149750612945e-07 valid 1.005183435154322e-06\n",
      "EPOCH 9:\n",
      "LOSS train 6.963243182018293e-07 valid 9.516578529655817e-07\n",
      "EPOCH 10:\n",
      "LOSS train 6.141207208708407e-07 valid 8.32254784199904e-07\n",
      "EPOCH 11:\n",
      "LOSS train 5.363388860897998e-07 valid 6.82129268625431e-07\n",
      "EPOCH 12:\n",
      "LOSS train 5.25989788856675e-07 valid 6.438916670958861e-07\n",
      "EPOCH 13:\n",
      "LOSS train 8.437290199706359e-07 valid 1.1082173614340718e-06\n",
      "EPOCH 14:\n",
      "LOSS train 7.917095275800865e-07 valid 1.154687424786971e-06\n",
      "EPOCH 15:\n",
      "LOSS train 6.455623178067772e-07 valid 7.181603791650559e-07\n",
      "EPOCH 16:\n",
      "LOSS train 3.7331902098540145e-07 valid 4.932433625981503e-07\n",
      "EPOCH 17:\n",
      "LOSS train 3.624341738004575e-07 valid 4.828721671401581e-07\n",
      "EPOCH 18:\n",
      "LOSS train 3.123298014085641e-07 valid 3.5960209743279847e-07\n",
      "EPOCH 19:\n",
      "LOSS train 2.769903742750591e-07 valid 2.9232131737444433e-07\n",
      "EPOCH 20:\n",
      "LOSS train 2.349209224363566e-07 valid 1.9807855267117702e-07\n",
      "(3, 0, False, 32, 0.0025, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 2.3593369020958684e-05 valid 2.09838867704093e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.269500192572615e-06 valid 8.719446668692399e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.7163101471602597e-06 valid 5.896336006117053e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.80928144835352e-06 valid 6.47577735435334e-06\n",
      "(3, 0, False, 32, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.683611601235647e-05 valid 4.37491326010786e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.3576944881504155e-06 valid 4.562153662845958e-06\n",
      "EPOCH 3:\n",
      "LOSS train 8.594194093102353e-07 valid 5.056542136117059e-07\n",
      "EPOCH 4:\n",
      "LOSS train 1.227250183423455e-06 valid 6.113610311331286e-07\n",
      "EPOCH 5:\n",
      "LOSS train 9.84724208113306e-07 valid 4.2032445435324917e-07\n",
      "EPOCH 6:\n",
      "LOSS train 9.961033609286518e-07 valid 8.818170726954122e-07\n",
      "EPOCH 7:\n",
      "LOSS train 8.553349988516161e-07 valid 9.308482162850851e-07\n",
      "EPOCH 8:\n",
      "LOSS train 1.0803993347107066e-06 valid 5.650094863085542e-07\n",
      "(3, 0, False, 32, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.4398436816131916e-05 valid 2.1884227408008883e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.2429989060558593e-06 valid 1.2264359838809469e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.6090002161869443e-06 valid 1.2843062222600565e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.2448441190988286e-06 valid 1.0055139227915788e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.4102201005862829e-06 valid 6.413399660232244e-07\n",
      "EPOCH 6:\n",
      "LOSS train 1.3103839924706923e-06 valid 4.0057275896288047e-07\n",
      "EPOCH 7:\n",
      "LOSS train 9.892474468917647e-07 valid 4.3560814333432063e-07\n",
      "EPOCH 8:\n",
      "LOSS train 7.748552996412538e-07 valid 2.1032175823165744e-07\n",
      "EPOCH 9:\n",
      "LOSS train 7.63640574867958e-07 valid 3.3952113653867855e-07\n",
      "EPOCH 10:\n",
      "LOSS train 6.193686375998307e-07 valid 1.2213616855660803e-06\n",
      "EPOCH 11:\n",
      "LOSS train 5.755559504439052e-07 valid 4.2607936734384566e-07\n",
      "EPOCH 12:\n",
      "LOSS train 6.301764628941388e-07 valid 4.2590238535922254e-07\n",
      "(3, 0, False, 32, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 4.449414059793415e-05 valid 4.320601419749437e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.9981739191067304e-06 valid 4.301948592910776e-06\n",
      "EPOCH 3:\n",
      "LOSS train 5.285355389503219e-06 valid 2.398891638222267e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.2195787391769977e-06 valid 2.031725216511404e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.1542374013302197e-06 valid 4.221956714900443e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.285810592902633e-06 valid 5.7715774346434046e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.8932929684109555e-06 valid 2.7408623282099143e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.3432897154045986e-06 valid 5.444121597975027e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.6294673712408817e-06 valid 4.272066234989325e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.857998781634418e-06 valid 2.5985486900026444e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.9153271621459274e-06 valid 3.234658834116999e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.7304065625083293e-06 valid 1.795773073354212e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.4506188935139353e-06 valid 1.6585187267992296e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.3140506379657184e-06 valid 1.7140288264272385e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.4278337326722992e-06 valid 1.2445937045413302e-06\n",
      "EPOCH 16:\n",
      "LOSS train 1.3318007626505642e-06 valid 1.1778549833252328e-06\n",
      "(3, 0, False, 32, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.728616359765203e-05 valid 5.294677066558506e-06\n",
      "EPOCH 2:\n",
      "LOSS train 5.195292327166154e-06 valid 3.46020829056215e-06\n",
      "EPOCH 3:\n",
      "LOSS train 5.238335291101709e-06 valid 3.046893198188627e-06\n",
      "EPOCH 4:\n",
      "LOSS train 9.834324247405296e-06 valid 4.401113073981833e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.908481308026282e-06 valid 4.225573320582043e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.083939007085172e-06 valid 1.6837469729580334e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.451944745312628e-06 valid 1.4318057992568356e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.0196050985199632e-06 valid 1.1217778137506684e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.535075838189988e-06 valid 9.790099966267007e-07\n",
      "EPOCH 10:\n",
      "LOSS train 1.025266434796596e-06 valid 1.7261974107896094e-06\n",
      "EPOCH 11:\n",
      "LOSS train 9.523521419314547e-07 valid 1.3715216482523829e-06\n",
      "EPOCH 12:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 9.48191194896161e-07 valid 4.127615511606564e-07\n",
      "EPOCH 13:\n",
      "LOSS train 9.753110576509323e-07 valid 2.945245114460704e-07\n",
      "EPOCH 14:\n",
      "LOSS train 4.3437108233498517e-07 valid 3.2630424584567663e-07\n",
      "EPOCH 15:\n",
      "LOSS train 5.279065255167725e-07 valid 2.672516643542622e-07\n",
      "EPOCH 16:\n",
      "LOSS train 4.982401421043416e-07 valid 3.933860455163085e-07\n",
      "EPOCH 17:\n",
      "LOSS train 4.923975046602423e-07 valid 1.3977424941913341e-06\n",
      "EPOCH 18:\n",
      "LOSS train 4.863409100035873e-07 valid 6.310680760179821e-07\n",
      "EPOCH 19:\n",
      "LOSS train 4.1785557180141047e-07 valid 5.613950406768708e-07\n",
      "EPOCH 20:\n",
      "LOSS train 5.764286287415084e-07 valid 1.755844436956977e-06\n",
      "(3, 0, False, 32, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0006561657048481593 valid 1.930614189404878e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.8723225743693283e-06 valid 2.92352319775091e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.144929216817235e-06 valid 2.317655116712558e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.8690081489834719e-06 valid 2.5678359634184744e-06\n",
      "(3, 0, False, 32, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0005109670325963687 valid 2.2060542050894583e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.751547583214387e-06 valid 4.424999588081846e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.627702992822197e-06 valid 3.5784826195595087e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.8466899567516585e-06 valid 1.65798928719596e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.7397931586158544e-06 valid 1.186201416203403e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.6468676490501699e-06 valid 7.073608230712125e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.680594592237271e-06 valid 2.278842839587014e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.1909868378899775e-06 valid 3.465868530838634e-06\n",
      "(3, 0, False, 32, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00022837827061530014 valid 3.7820191209902987e-06\n",
      "EPOCH 2:\n",
      "LOSS train 4.690308882103011e-06 valid 3.5715868307306664e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.804822326730106e-06 valid 2.5798544811550528e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.620945330494691e-06 valid 2.638823389133904e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.2332242016861545e-06 valid 5.4854735935805365e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.6159833856830975e-05 valid 7.254404772538692e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.164015275095861e-05 valid 8.411779708694667e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.380516520744072e-05 valid 6.755816139047965e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.368801179389378e-05 valid 6.319187377812341e-05\n",
      "EPOCH 10:\n",
      "LOSS train 3.353971536875799e-05 valid 6.249932630453259e-05\n",
      "EPOCH 11:\n",
      "LOSS train 3.351179887992565e-05 valid 6.237956404220313e-05\n",
      "EPOCH 12:\n",
      "LOSS train 3.350682789792121e-05 valid 6.235837645363063e-05\n",
      "(3, 0, False, 32, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0016158521025351862 valid 7.087564426910831e-06\n",
      "EPOCH 2:\n",
      "LOSS train 7.338090796586156e-06 valid 1.623975003894884e-05\n",
      "EPOCH 3:\n",
      "LOSS train 8.860819053214281e-06 valid 1.887125472421758e-05\n",
      "EPOCH 4:\n",
      "LOSS train 7.379275506652049e-06 valid 3.713704927577055e-06\n",
      "EPOCH 5:\n",
      "LOSS train 5.221595111767296e-06 valid 3.321807071188232e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.7163583264710934e-06 valid 2.86213366962329e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.3400031200356642e-06 valid 1.1615589983193786e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.954453275333769e-06 valid 1.2126118917876738e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.647210307886465e-06 valid 1.1237834769417532e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.6145313449709203e-06 valid 1.663247644501098e-06\n",
      "EPOCH 11:\n",
      "LOSS train 3.437857141753258e-06 valid 7.320587883441476e-06\n",
      "EPOCH 12:\n",
      "LOSS train 3.266333814109906e-06 valid 4.71371777166496e-06\n",
      "EPOCH 13:\n",
      "LOSS train 3.6571651924537973e-06 valid 4.537846507446375e-06\n",
      "EPOCH 14:\n",
      "LOSS train 3.89576874316158e-06 valid 2.9215477752586594e-06\n",
      "EPOCH 15:\n",
      "LOSS train 3.837726635853581e-06 valid 3.0040371257200604e-06\n",
      "EPOCH 16:\n",
      "LOSS train 3.711084357740319e-06 valid 3.4042791412502993e-06\n",
      "(3, 0, False, 32, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0002840255024868526 valid 2.6002737740782322e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.7011641721429056e-06 valid 4.070958766533295e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.5201816503396621e-06 valid 4.0479017116012983e-07\n",
      "EPOCH 4:\n",
      "LOSS train 1.240323312908332e-06 valid 3.042165872102487e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.5305635737813493e-05 valid 7.954623288242146e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.269602470916587e-05 valid 7.698718400206417e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.3883654937708464e-05 valid 6.474190013250336e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.3597465354982424e-05 valid 6.27548506599851e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.3522265228078244e-05 valid 6.242424569791183e-05\n",
      "EPOCH 10:\n",
      "LOSS train 3.350868985127355e-05 valid 6.236637273104861e-05\n",
      "EPOCH 11:\n",
      "LOSS train 3.3506276471989786e-05 valid 6.235603359527886e-05\n",
      "EPOCH 12:\n",
      "LOSS train 3.3505849489601206e-05 valid 6.235423643374816e-05\n",
      "EPOCH 13:\n",
      "LOSS train 3.3505771711781804e-05 valid 6.23539526714012e-05\n",
      "EPOCH 14:\n",
      "LOSS train 3.350576249472058e-05 valid 6.235393084352836e-05\n",
      "EPOCH 15:\n",
      "LOSS train 3.350576221609113e-05 valid 6.235393084352836e-05\n",
      "EPOCH 16:\n",
      "LOSS train 3.350576189888674e-05 valid 6.235393084352836e-05\n",
      "EPOCH 17:\n",
      "LOSS train 3.350576189888674e-05 valid 6.235393084352836e-05\n",
      "EPOCH 18:\n",
      "LOSS train 3.350576189888674e-05 valid 6.235393084352836e-05\n",
      "EPOCH 19:\n",
      "LOSS train 3.350576189888674e-05 valid 6.235393084352836e-05\n",
      "EPOCH 20:\n",
      "LOSS train 3.350576189888674e-05 valid 6.235393084352836e-05\n",
      "(3, 0, False, 32, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.48903120362918345 valid 5.868557855137624e-05\n",
      "EPOCH 2:\n",
      "LOSS train 3.218891367928142e-05 valid 8.187873754650354e-05\n",
      "EPOCH 3:\n",
      "LOSS train 3.528981021149477e-05 valid 0.0001110361990868114\n",
      "EPOCH 4:\n",
      "LOSS train 3.7118684992956054e-05 valid 7.477693725377321e-05\n",
      "(3, 0, False, 32, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.15175355297174634 valid 2.929571201093495e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.570628997775541e-06 valid 1.1822850865428336e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.20274044916281e-06 valid 9.607596439309418e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.69385108159431e-06 valid 1.1336934221617412e-05\n",
      "EPOCH 5:\n",
      "LOSS train 8.528094046567396e-06 valid 1.0630726137605961e-05\n",
      "EPOCH 6:\n",
      "LOSS train 7.386294976080771e-06 valid 3.920414656022331e-06\n",
      "EPOCH 7:\n",
      "LOSS train 5.006947695929964e-06 valid 2.4050664251262788e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.526425392747381e-06 valid 1.9325345874676714e-06\n",
      "(3, 0, False, 32, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.1620806563493471 valid 5.4116128012537956e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.21476841183491e-05 valid 0.0001335424167336896\n",
      "EPOCH 3:\n",
      "LOSS train 4.2099782262064015e-05 valid 8.718466415302828e-05\n",
      "EPOCH 4:\n",
      "LOSS train 2.9943905525251514e-05 valid 5.934364889981225e-05\n",
      "EPOCH 5:\n",
      "LOSS train 2.775682295357856e-05 valid 5.898842209717259e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.291365801895543e-05 valid 7.917934999568388e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.289611141728891e-05 valid 0.00011750440899049863\n",
      "EPOCH 8:\n",
      "LOSS train 3.686092354049423e-05 valid 6.669996218988672e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.57161151718242e-05 valid 9.69618558883667e-05\n",
      "EPOCH 10:\n",
      "LOSS train 4.1044860429130264e-05 valid 0.00010098976781591773\n",
      "EPOCH 11:\n",
      "LOSS train 4.1261095792502e-05 valid 0.00010103664681082591\n",
      "EPOCH 12:\n",
      "LOSS train 4.126348045376593e-05 valid 0.00010103725071530789\n",
      "(3, 0, False, 32, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.600590128104472 valid 5.521859566215426e-05\n",
      "EPOCH 2:\n",
      "LOSS train 6.921249944618772e-05 valid 5.395622429205105e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.583536722853175e-05 valid 6.500750168925151e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.0275712875253324e-05 valid 0.0001363805786240846\n",
      "EPOCH 5:\n",
      "LOSS train 4.195495253085297e-05 valid 8.746407547732815e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.0026272022280254e-05 valid 5.918549868511036e-05\n",
      "EPOCH 7:\n",
      "LOSS train 2.769079305797164e-05 valid 5.9966892877127975e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.280904492703945e-05 valid 7.633733184775338e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.278788489408537e-05 valid 0.00011695450666593388\n",
      "EPOCH 10:\n",
      "LOSS train 3.700771268656934e-05 valid 6.52595772407949e-05\n",
      "EPOCH 11:\n",
      "LOSS train 3.5512853664838845e-05 valid 9.644561941968277e-05\n",
      "EPOCH 12:\n",
      "LOSS train 4.1015176956318935e-05 valid 0.00010098281927639619\n",
      "EPOCH 13:\n",
      "LOSS train 4.1260722306796434e-05 valid 0.0001010365376714617\n",
      "EPOCH 14:\n",
      "LOSS train 4.1263471244116145e-05 valid 0.00010103719250764698\n",
      "EPOCH 15:\n",
      "LOSS train 4.126351092897447e-05 valid 0.0001010373089229688\n",
      "EPOCH 16:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "(3, 0, False, 32, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.09098517148796238 valid 2.3984432118595578e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.2756234550838151e-05 valid 1.8804932551574893e-05\n",
      "EPOCH 3:\n",
      "LOSS train 8.52127580751107e-06 valid 1.3267991562315729e-05\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.099626035778546e-05 valid 6.514678261737572e-06\n",
      "EPOCH 5:\n",
      "LOSS train 0.00012558023718018305 valid 5.120613786857575e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.273213401915031e-05 valid 7.643149001523852e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.481845578228119e-05 valid 7.467370596714318e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.506456964732859e-05 valid 7.487300172215328e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.817638662732544e-05 valid 9.993365529226139e-05\n",
      "EPOCH 10:\n",
      "LOSS train 4.120671594639822e-05 valid 0.00010102495434693992\n",
      "EPOCH 11:\n",
      "LOSS train 4.126285791176636e-05 valid 0.0001010369960567914\n",
      "EPOCH 12:\n",
      "LOSS train 4.126350181714824e-05 valid 0.00010103725071530789\n",
      "EPOCH 13:\n",
      "LOSS train 4.1263508091099834e-05 valid 0.0001010372579912655\n",
      "EPOCH 14:\n",
      "LOSS train 4.126350853845411e-05 valid 0.00010103719250764698\n",
      "EPOCH 15:\n",
      "LOSS train 4.126350925380724e-05 valid 0.00010103719978360459\n",
      "EPOCH 16:\n",
      "LOSS train 4.126351298328143e-05 valid 0.0001010373089229688\n",
      "EPOCH 17:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "EPOCH 18:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "EPOCH 19:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "EPOCH 20:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "(3, 0, False, 64, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00035692864801164625 valid 3.973210186813958e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.6577299984748102e-05 valid 1.0330109944334254e-05\n",
      "EPOCH 3:\n",
      "LOSS train 4.198893214575366e-06 valid 4.072176579938969e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.7687174279199973e-06 valid 2.2957012788538123e-06\n",
      "(3, 0, False, 64, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00013135467023463253 valid 6.702710834360914e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.5235668358288176e-06 valid 6.001554538670462e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.147870777053215e-06 valid 6.625829428230645e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.486606035116192e-06 valid 6.363512511597946e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.704191846025734e-06 valid 4.347081812738907e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.236339534103243e-06 valid 3.272986305091763e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.995980253479839e-06 valid 3.025192654604325e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.165045791139712e-06 valid 3.070382490477641e-06\n",
      "(3, 0, False, 64, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.187789401760541e-06 valid 2.402172867732588e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.5026333248263588e-06 valid 1.284263930756424e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.1970669598825792e-06 valid 3.4696727198024746e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.2355124338447207e-06 valid 3.0355117814906407e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.881134851345136e-07 valid 1.7170666524179978e-06\n",
      "EPOCH 6:\n",
      "LOSS train 9.565992366243072e-07 valid 1.2414483308020863e-06\n",
      "EPOCH 7:\n",
      "LOSS train 4.134406160093764e-07 valid 6.396055596269434e-07\n",
      "EPOCH 8:\n",
      "LOSS train 4.582345168190321e-07 valid 6.904742804181296e-07\n",
      "EPOCH 9:\n",
      "LOSS train 5.374187103718965e-07 valid 6.766246656297881e-07\n",
      "EPOCH 10:\n",
      "LOSS train 3.742311622555853e-07 valid 4.201105241463665e-07\n",
      "EPOCH 11:\n",
      "LOSS train 6.113017398257513e-07 valid 1.1598364153542207e-06\n",
      "EPOCH 12:\n",
      "LOSS train 4.670256192162323e-07 valid 3.463822508820158e-07\n",
      "(3, 0, False, 64, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 6.0209386443620856e-05 valid 1.0605905117699876e-05\n",
      "EPOCH 2:\n",
      "LOSS train 3.400158805896458e-06 valid 2.317556663911091e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.2688263529432871e-06 valid 1.468390109948814e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.2779700261934744e-06 valid 1.9324891127325827e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.2586181981016823e-06 valid 1.2149513395343092e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.331936671572775e-06 valid 1.8996578319274704e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.4476085446716181e-06 valid 1.4128463590168394e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.4735392296611804e-06 valid 2.3494637844123645e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.2255281512010042e-06 valid 1.3568297845267807e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.1072162460010256e-06 valid 1.4812048902967945e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.2093839736261858e-06 valid 9.323192102783651e-07\n",
      "EPOCH 12:\n",
      "LOSS train 9.050630700702818e-07 valid 1.2736805956592434e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.009636363057954e-06 valid 7.231928975670598e-07\n",
      "EPOCH 14:\n",
      "LOSS train 7.083249077061713e-07 valid 9.112099519370531e-07\n",
      "EPOCH 15:\n",
      "LOSS train 7.40888769625625e-07 valid 7.808249620211427e-07\n",
      "EPOCH 16:\n",
      "LOSS train 7.069983286407942e-07 valid 7.344865480263252e-07\n",
      "(3, 0, False, 64, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.07851048190443e-05 valid 2.55963095696643e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.7250452379700364e-06 valid 2.154824869649019e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.6293751320595987e-06 valid 1.814284246393072e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.2212750715011449e-06 valid 2.3570098619529745e-06\n",
      "EPOCH 5:\n",
      "LOSS train 8.832445954272957e-07 valid 1.1124453749289387e-06\n",
      "EPOCH 6:\n",
      "LOSS train 8.227018664621167e-07 valid 7.923270004539518e-07\n",
      "EPOCH 7:\n",
      "LOSS train 7.276111709867812e-07 valid 8.515233957950841e-07\n",
      "EPOCH 8:\n",
      "LOSS train 8.857873136499228e-07 valid 1.0524447588977637e-06\n",
      "EPOCH 9:\n",
      "LOSS train 7.070172596761528e-07 valid 7.493697466998128e-07\n",
      "EPOCH 10:\n",
      "LOSS train 5.028738304468488e-07 valid 5.514145868801279e-07\n",
      "EPOCH 11:\n",
      "LOSS train 7.603615835656454e-07 valid 6.258466669351037e-07\n",
      "EPOCH 12:\n",
      "LOSS train 4.2360595098361234e-07 valid 4.6127556174724305e-07\n",
      "EPOCH 13:\n",
      "LOSS train 3.6580237477617376e-07 valid 5.528118549591454e-07\n",
      "EPOCH 14:\n",
      "LOSS train 3.602533933419515e-07 valid 4.2700753510871436e-07\n",
      "EPOCH 15:\n",
      "LOSS train 2.8841821727238317e-07 valid 3.912358010893513e-07\n",
      "EPOCH 16:\n",
      "LOSS train 6.43981193704235e-07 valid 3.3502229257464933e-07\n",
      "EPOCH 17:\n",
      "LOSS train 2.723764631793854e-07 valid 2.739546687280381e-07\n",
      "EPOCH 18:\n",
      "LOSS train 7.527299362836491e-07 valid 1.9388999135117047e-06\n",
      "EPOCH 19:\n",
      "LOSS train 3.2372296528097507e-07 valid 3.804014454544813e-07\n",
      "EPOCH 20:\n",
      "LOSS train 2.1254477495091852e-07 valid 2.6307344569431734e-07\n",
      "(3, 0, False, 64, 0.0025, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00025100169459441973 valid 4.198887836537324e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.955214147010634e-06 valid 4.374083346192492e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.289855534599398e-06 valid 4.598088253260357e-06\n",
      "EPOCH 4:\n",
      "LOSS train 4.404209960076884e-06 valid 4.068799171363935e-06\n",
      "(3, 0, False, 64, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00010902480352714416 valid 4.032689957966795e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.984786119827487e-06 valid 3.8058674363128375e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.7202231318980637e-06 valid 3.5682735415321076e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.1607763603112326e-06 valid 1.6791906318758265e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.000730992555156e-06 valid 1.3406744301391882e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.4239059765299905e-06 valid 1.0730552730819909e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.8385096512667806e-05 valid 1.3957424016552977e-06\n",
      "EPOCH 8:\n",
      "LOSS train 6.383115775381282e-07 valid 1.8432374417898245e-06\n",
      "(3, 0, False, 64, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 8.714139695361082e-05 valid 4.041655301989522e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.451480083786825e-06 valid 3.159110292472178e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.48254688897788e-05 valid 3.776522589760134e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.531764983066292e-06 valid 3.0345427148859017e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.428975138122293e-06 valid 3.4178772239101818e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.840815685280875e-06 valid 2.3438815333065577e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.2539208344904762e-06 valid 1.6385469052693225e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.1251778401123827e-06 valid 1.5773154018461355e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.7294494689740288e-06 valid 1.3698312386623002e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.9133628243883112e-06 valid 1.3783653685095487e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.3013922476264477e-06 valid 8.488335083711718e-07\n",
      "EPOCH 12:\n",
      "LOSS train 1.5238563211742529e-06 valid 1.2821042219002265e-06\n",
      "(3, 0, False, 64, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.687835555466917e-05 valid 9.76519572759571e-07\n",
      "EPOCH 2:\n",
      "LOSS train 1.0446597474082224e-06 valid 1.937129809448379e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.3494611741944355e-06 valid 2.0649224552471424e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.3242042921360899e-06 valid 9.899928272716352e-07\n",
      "EPOCH 5:\n",
      "LOSS train 1.3693317183626305e-06 valid 1.4498489235847956e-06\n",
      "EPOCH 6:\n",
      "LOSS train 9.977309021302703e-07 valid 2.1835703591932543e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.117602935118538e-06 valid 1.1518627616169397e-06\n",
      "EPOCH 8:\n",
      "LOSS train 7.444524953043051e-07 valid 5.111686505188118e-07\n",
      "EPOCH 9:\n",
      "LOSS train 4.6946210509484385e-07 valid 8.857529110173346e-07\n",
      "EPOCH 10:\n",
      "LOSS train 5.902419045563088e-07 valid 1.2056336800014833e-06\n",
      "EPOCH 11:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 6.948441738421061e-07 valid 9.226783959093154e-07\n",
      "EPOCH 12:\n",
      "LOSS train 6.109009445874836e-07 valid 2.486009634594666e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.3170675335754142e-06 valid 2.0634392683405167e-07\n",
      "EPOCH 14:\n",
      "LOSS train 7.93012991372831e-07 valid 3.5506225231074495e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.0562172777366769e-06 valid 3.8604680412390735e-06\n",
      "EPOCH 16:\n",
      "LOSS train 5.501740668969674e-07 valid 5.174778834771132e-06\n",
      "(3, 0, False, 64, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0001429481740497359 valid 2.2059141429053852e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.8196627443552425e-06 valid 2.2962292405281914e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.8948863390185207e-06 valid 2.561274186518858e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.6656884337118283e-06 valid 1.74279546172329e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.388558767189186e-06 valid 5.663681349687977e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.1834563086885093e-06 valid 1.7010822830343386e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.852251730735632e-06 valid 1.0137838444279623e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.952956145011066e-06 valid 1.6247535086222342e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.2097915589047755e-06 valid 1.5309614127545501e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.4829172277432332e-06 valid 8.23031484742387e-07\n",
      "EPOCH 11:\n",
      "LOSS train 1.812357387020306e-06 valid 1.0442661277920706e-06\n",
      "EPOCH 12:\n",
      "LOSS train 9.526749863310664e-07 valid 5.698332188330824e-07\n",
      "EPOCH 13:\n",
      "LOSS train 1.2402030392412624e-06 valid 1.1944161997234914e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.2611601322385659e-06 valid 7.170469302764104e-07\n",
      "EPOCH 15:\n",
      "LOSS train 1.2803678871998046e-06 valid 6.915879566804506e-07\n",
      "EPOCH 16:\n",
      "LOSS train 1.1643122478781069e-06 valid 8.86303212155326e-07\n",
      "EPOCH 17:\n",
      "LOSS train 1.2626981380370156e-06 valid 9.192540346703026e-07\n",
      "EPOCH 18:\n",
      "LOSS train 1.4591846712426143e-06 valid 3.844958882837091e-06\n",
      "EPOCH 19:\n",
      "LOSS train 8.602631944171337e-07 valid 3.8373957522708224e-07\n",
      "EPOCH 20:\n",
      "LOSS train 3.3736056817342115e-07 valid 4.326435600887635e-07\n",
      "(3, 0, False, 64, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0006113596628343783 valid 1.6456870071124285e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.0846393096140647e-06 valid 9.77090508058609e-07\n",
      "EPOCH 3:\n",
      "LOSS train 9.451440591758968e-07 valid 1.710796709630813e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.39334259293637e-06 valid 7.046371024443943e-07\n",
      "(3, 0, False, 64, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00013477727458078502 valid 0.0002510323829483241\n",
      "EPOCH 2:\n",
      "LOSS train 7.723332556697062e-05 valid 0.00026752951089292765\n",
      "EPOCH 3:\n",
      "LOSS train 8.330899959923524e-05 valid 0.00025742719299159944\n",
      "EPOCH 4:\n",
      "LOSS train 8.413379212870668e-05 valid 0.00025242409901693463\n",
      "EPOCH 5:\n",
      "LOSS train 8.432549106212463e-05 valid 0.00025005859788507223\n",
      "EPOCH 6:\n",
      "LOSS train 8.438864878326136e-05 valid 0.0002488991303835064\n",
      "EPOCH 7:\n",
      "LOSS train 8.44140740998788e-05 valid 0.00024831571499817073\n",
      "EPOCH 8:\n",
      "LOSS train 8.442555063978233e-05 valid 0.00024801772087812424\n",
      "(3, 0, False, 64, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.001051945124244887 valid 5.336720641935244e-05\n",
      "EPOCH 2:\n",
      "LOSS train 9.881054483865951e-05 valid 0.00012448658526409417\n",
      "EPOCH 3:\n",
      "LOSS train 6.183025813052952e-05 valid 0.00018122453184332699\n",
      "EPOCH 4:\n",
      "LOSS train 5.83640619442941e-05 valid 0.00023307293304242194\n",
      "EPOCH 5:\n",
      "LOSS train 6.956416754153808e-05 valid 0.00026943397824652493\n",
      "EPOCH 6:\n",
      "LOSS train 7.940529664116457e-05 valid 0.000268525822320953\n",
      "EPOCH 7:\n",
      "LOSS train 8.296069264085485e-05 valid 0.0002601042506285012\n",
      "EPOCH 8:\n",
      "LOSS train 8.395086951928978e-05 valid 0.0002544016169849783\n",
      "EPOCH 9:\n",
      "LOSS train 8.425118058814465e-05 valid 0.00025123218074440956\n",
      "EPOCH 10:\n",
      "LOSS train 8.435693821507286e-05 valid 0.00024954750551842153\n",
      "EPOCH 11:\n",
      "LOSS train 8.439954635151744e-05 valid 0.0002486638259142637\n",
      "EPOCH 12:\n",
      "LOSS train 8.44185230432948e-05 valid 0.0002482017735019326\n",
      "(3, 0, False, 64, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0005516775540240795 valid 1.2202883226564154e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.492113818500563e-06 valid 4.42275995737873e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.578687214316502e-06 valid 4.855520728597185e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.819142074458197e-06 valid 3.339479690112057e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.0316051498016234e-06 valid 2.9818729672115296e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.1071413425438973e-06 valid 2.150197587980074e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.776457028298545e-06 valid 2.42995997723483e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.616401113558063e-05 valid 5.180842344998382e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.3855485727446424e-05 valid 5.0856393499998376e-05\n",
      "EPOCH 10:\n",
      "LOSS train 3.414846180826418e-05 valid 5.087739918963052e-05\n",
      "EPOCH 11:\n",
      "LOSS train 3.4214655013658054e-05 valid 5.0795213610399514e-05\n",
      "EPOCH 12:\n",
      "LOSS train 3.427633896819281e-05 valid 5.076946399640292e-05\n",
      "EPOCH 13:\n",
      "LOSS train 3.430298637732197e-05 valid 5.0760187150444835e-05\n",
      "EPOCH 14:\n",
      "LOSS train 3.431495864682248e-05 valid 5.07564254803583e-05\n",
      "EPOCH 15:\n",
      "LOSS train 3.4320591230183e-05 valid 5.075469016446732e-05\n",
      "EPOCH 16:\n",
      "LOSS train 3.432332405131399e-05 valid 5.075387525721453e-05\n",
      "(3, 0, False, 64, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0015413087315263 valid 1.2903734386782162e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.681924867306828e-06 valid 6.268680863286136e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.866234234908872e-06 valid 6.13129896009923e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.4132878773963495e-06 valid 6.510377716040239e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.8106736838115077e-06 valid 4.493119377002586e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.750330096501012e-06 valid 2.9201326015027007e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.6640410210737983e-06 valid 2.772741936496459e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.6772295883103876e-06 valid 4.562802132568322e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.0692416032101203e-06 valid 4.117595835850807e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.653291289439984e-06 valid 1.508661398474942e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.7194331898571419e-06 valid 8.75844989423058e-07\n",
      "EPOCH 12:\n",
      "LOSS train 1.571383772823026e-06 valid 1.0508147170185111e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.1729921325063293e-06 valid 1.1619173392318771e-06\n",
      "EPOCH 14:\n",
      "LOSS train 3.6255458967071894e-06 valid 1.0986328788931132e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.6782656664604163e-06 valid 3.324998488096753e-06\n",
      "EPOCH 16:\n",
      "LOSS train 2.1530576410396324e-06 valid 3.820461188297486e-06\n",
      "EPOCH 17:\n",
      "LOSS train 2.3942582545138132e-06 valid 5.717407930205809e-06\n",
      "EPOCH 18:\n",
      "LOSS train 2.9469574808975308e-06 valid 2.511769480406656e-06\n",
      "EPOCH 19:\n",
      "LOSS train 2.1283182208091202e-06 valid 9.768618838279508e-07\n",
      "EPOCH 20:\n",
      "LOSS train 2.5636327781701613e-06 valid 2.304150029885932e-06\n",
      "(3, 0, False, 64, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.7358927318728798 valid 2.161134580092039e-05\n",
      "EPOCH 2:\n",
      "LOSS train 6.993313567328044e-06 valid 1.733145472826436e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.64775973894865e-06 valid 1.6563504686928354e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.903192483857183e-06 valid 1.6429630704806186e-05\n",
      "(3, 0, False, 64, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.14984992103172579 valid 3.393179576960392e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.2276736848983924e-05 valid 2.7322112146066502e-05\n",
      "EPOCH 3:\n",
      "LOSS train 9.919438778741052e-06 valid 1.2421515748428646e-05\n",
      "EPOCH 4:\n",
      "LOSS train 6.189384587340182e-06 valid 9.527946531306952e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.17674958789016e-06 valid 6.429731911339331e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.6933524449755413e-06 valid 6.995967851253226e-06\n",
      "EPOCH 7:\n",
      "LOSS train 4.215971611384953e-06 valid 6.179764568514656e-06\n",
      "EPOCH 8:\n",
      "LOSS train 6.30452696143634e-06 valid 9.57418615143979e-06\n",
      "(3, 0, False, 64, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 2.440114441508429 valid 1.5031512702989858e-05\n",
      "EPOCH 2:\n",
      "LOSS train 9.793550029976962e-06 valid 1.8370184989180416e-05\n",
      "EPOCH 3:\n",
      "LOSS train 2.6762721996608443e-05 valid 3.862989615299739e-05\n",
      "EPOCH 4:\n",
      "LOSS train 6.53884111079449e-05 valid 1.870143023552373e-05\n",
      "EPOCH 5:\n",
      "LOSS train 7.601374953067538e-05 valid 2.0698240405181423e-05\n",
      "EPOCH 6:\n",
      "LOSS train 5.1216263604171706e-05 valid 3.0376037102541886e-05\n",
      "EPOCH 7:\n",
      "LOSS train 4.639352298115952e-05 valid 6.336930528050289e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.322354794266828e-05 valid 3.448329516686499e-05\n",
      "EPOCH 9:\n",
      "LOSS train 3.6227399265498725e-05 valid 4.004424772574566e-05\n",
      "EPOCH 10:\n",
      "LOSS train 0.0007314490741614674 valid 0.0001415207370882854\n",
      "EPOCH 11:\n",
      "LOSS train 5.909935939475379e-05 valid 0.0001905040699057281\n",
      "EPOCH 12:\n",
      "LOSS train 6.122930342878343e-05 valid 0.00025781302247196436\n",
      "(3, 0, False, 64, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.8972548158554171 valid 8.508860628353432e-05\n",
      "EPOCH 2:\n",
      "LOSS train 8.023765435825406e-06 valid 6.551641945407027e-06\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 4.5135480702104845e-06 valid 7.325205388042377e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.328289155922232e-05 valid 2.4686230972292833e-05\n",
      "EPOCH 5:\n",
      "LOSS train 2.155015469744068e-05 valid 2.9386797905317508e-05\n",
      "EPOCH 6:\n",
      "LOSS train 1.4187817910517843e-05 valid 5.691090336767957e-06\n",
      "EPOCH 7:\n",
      "LOSS train 7.81693071840017e-06 valid 4.646525212592678e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.965380928460429e-06 valid 3.5615782962850062e-06\n",
      "EPOCH 9:\n",
      "LOSS train 4.034509374001724e-06 valid 4.3622985685942695e-06\n",
      "EPOCH 10:\n",
      "LOSS train 0.00010863254050953756 valid 5.567272091866471e-05\n",
      "EPOCH 11:\n",
      "LOSS train 5.2710797438949436e-05 valid 0.00010736300464486703\n",
      "EPOCH 12:\n",
      "LOSS train 3.887463144655128e-05 valid 5.588846033788286e-05\n",
      "EPOCH 13:\n",
      "LOSS train 4.357016627390022e-05 valid 5.534152660402469e-05\n",
      "EPOCH 14:\n",
      "LOSS train 3.149655032916716e-05 valid 5.078900358057581e-05\n",
      "EPOCH 15:\n",
      "LOSS train 2.9688265029306956e-05 valid 6.145607767393813e-05\n",
      "EPOCH 16:\n",
      "LOSS train 2.9382294365729974e-05 valid 7.529980211984366e-05\n",
      "(3, 0, False, 64, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.2581409059576824 valid 4.2942479012708645e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.0449332630576265e-06 valid 5.8099485613638535e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.017980234177631e-05 valid 3.9496026147389784e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.0173132664664086e-05 valid 4.371571321826195e-06\n",
      "EPOCH 5:\n",
      "LOSS train 8.357224594930386e-06 valid 1.022011747409124e-05\n",
      "EPOCH 6:\n",
      "LOSS train 9.029324175898254e-06 valid 9.928536201186944e-06\n",
      "EPOCH 7:\n",
      "LOSS train 0.0021573561362091035 valid 0.00016758374113123864\n",
      "EPOCH 8:\n",
      "LOSS train 5.736773999206684e-05 valid 0.0002233945851912722\n",
      "EPOCH 9:\n",
      "LOSS train 7.158734595456575e-05 valid 0.00025929990806616843\n",
      "EPOCH 10:\n",
      "LOSS train 7.599216485261669e-05 valid 8.608227653894573e-05\n",
      "EPOCH 11:\n",
      "LOSS train 4.3638026628870256e-05 valid 5.685431096935645e-05\n",
      "EPOCH 12:\n",
      "LOSS train 3.3989438654853404e-05 valid 5.075655644759536e-05\n",
      "EPOCH 13:\n",
      "LOSS train 3.148909977997598e-05 valid 5.776232501375489e-05\n",
      "EPOCH 14:\n",
      "LOSS train 2.8603300219992262e-05 valid 7.541357626905665e-05\n",
      "EPOCH 15:\n",
      "LOSS train 3.1192013897178515e-05 valid 5.476550722960383e-05\n",
      "EPOCH 16:\n",
      "LOSS train 3.990809740092606e-05 valid 5.4016021749703214e-05\n",
      "EPOCH 17:\n",
      "LOSS train 5.046960222624689e-05 valid 5.853136462974362e-05\n",
      "EPOCH 18:\n",
      "LOSS train 5.2216947644245e-05 valid 5.896531365578994e-05\n",
      "EPOCH 19:\n",
      "LOSS train 5.2350162037152405e-05 valid 5.8997557061957195e-05\n",
      "EPOCH 20:\n",
      "LOSS train 5.235991032452231e-05 valid 5.8999910834245384e-05\n",
      "(3, 0, False, 128, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0002091108125146296 valid 6.342096457956359e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.1662806904999584e-05 valid 5.710671302949777e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.065645927448245e-06 valid 3.574909214876243e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.093768821363983e-06 valid 3.994603957835352e-06\n",
      "(3, 0, False, 128, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0001058007162821101 valid 5.035637877881527e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.317974164352913e-05 valid 1.0546670637268107e-05\n",
      "EPOCH 3:\n",
      "LOSS train 4.866215227836785e-06 valid 1.6670995819367818e-06\n",
      "EPOCH 4:\n",
      "LOSS train 4.5709597087544665e-06 valid 1.2454852367227431e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.3799230063721185e-06 valid 1.291763510380406e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.315015784457991e-06 valid 1.8117790432370384e-06\n",
      "EPOCH 7:\n",
      "LOSS train 5.187723584959272e-06 valid 8.776066238169733e-07\n",
      "EPOCH 8:\n",
      "LOSS train 5.088664574459019e-06 valid 1.4043373539607273e-06\n",
      "(3, 0, False, 128, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00036262578283746585 valid 0.00016480908379890025\n",
      "EPOCH 2:\n",
      "LOSS train 0.00012651704829573501 valid 5.592316301772371e-05\n",
      "EPOCH 3:\n",
      "LOSS train 0.0001026091062794251 valid 1.2231852451805025e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.3126968554568543e-05 valid 3.9532233131467365e-06\n",
      "EPOCH 5:\n",
      "LOSS train 5.048153158088592e-06 valid 7.593800091854064e-06\n",
      "EPOCH 6:\n",
      "LOSS train 5.036911551686058e-06 valid 1.2161250197095796e-05\n",
      "EPOCH 7:\n",
      "LOSS train 5.119783839949584e-06 valid 1.350126967736287e-05\n",
      "EPOCH 8:\n",
      "LOSS train 4.346761579771615e-06 valid 9.210287316818722e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.9418113791363725e-06 valid 3.0718272228114074e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.0437852595374896e-06 valid 2.1777450456283987e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.9647400223287955e-06 valid 3.0538458304363303e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.212594889564589e-06 valid 2.437887133055483e-06\n",
      "(3, 0, False, 128, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.140363230545296e-05 valid 4.7707439080113545e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.0068242120548135e-05 valid 2.770539140328765e-06\n",
      "EPOCH 3:\n",
      "LOSS train 5.608847142580996e-06 valid 9.742951760927099e-07\n",
      "EPOCH 4:\n",
      "LOSS train 4.686279716696754e-06 valid 1.1387205631763209e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.8462650237028e-06 valid 1.378524530082359e-06\n",
      "EPOCH 6:\n",
      "LOSS train 5.467271641610435e-06 valid 9.147746027338144e-07\n",
      "EPOCH 7:\n",
      "LOSS train 5.841712236862782e-06 valid 1.4632418015025905e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.0092216757672116e-06 valid 1.8109067241312005e-06\n",
      "EPOCH 9:\n",
      "LOSS train 4.377958072125496e-06 valid 1.237840365320153e-06\n",
      "EPOCH 10:\n",
      "LOSS train 4.524272512527077e-06 valid 1.0777559964481043e-06\n",
      "EPOCH 11:\n",
      "LOSS train 4.5985483813487185e-06 valid 1.1685079925882746e-06\n",
      "EPOCH 12:\n",
      "LOSS train 4.148325741844456e-06 valid 1.525237962596293e-06\n",
      "EPOCH 13:\n",
      "LOSS train 4.2142175845589025e-06 valid 1.2520895324996673e-06\n",
      "EPOCH 14:\n",
      "LOSS train 4.049805737587431e-06 valid 1.2492623682192061e-06\n",
      "EPOCH 15:\n",
      "LOSS train 3.7387551888360352e-06 valid 1.2999214504816337e-06\n",
      "EPOCH 16:\n",
      "LOSS train 3.8127408272719432e-06 valid 1.60192587372876e-06\n",
      "(3, 0, False, 128, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0001231804318777807 valid 0.0001207736786454916\n",
      "EPOCH 2:\n",
      "LOSS train 7.168969951913679e-05 valid 5.316538590705022e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.401768036287202e-06 valid 1.956481810339028e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.813799585202733e-06 valid 1.7938123164640274e-06\n",
      "EPOCH 5:\n",
      "LOSS train 6.599649037616161e-06 valid 1.568253651385021e-06\n",
      "EPOCH 6:\n",
      "LOSS train 6.890033031653342e-06 valid 1.4093004665483022e-06\n",
      "EPOCH 7:\n",
      "LOSS train 6.983346087129098e-06 valid 1.2984228305867873e-06\n",
      "EPOCH 8:\n",
      "LOSS train 6.681370557528515e-06 valid 1.3094420410197927e-06\n",
      "EPOCH 9:\n",
      "LOSS train 6.559197852756998e-06 valid 1.4335173545987345e-06\n",
      "EPOCH 10:\n",
      "LOSS train 5.755398315995303e-06 valid 1.8775805301629589e-06\n",
      "EPOCH 11:\n",
      "LOSS train 5.967289572004979e-06 valid 3.3887695281009655e-06\n",
      "EPOCH 12:\n",
      "LOSS train 6.34763353919142e-06 valid 2.227509867225308e-06\n",
      "EPOCH 13:\n",
      "LOSS train 6.242352096209713e-06 valid 2.1798512079840293e-06\n",
      "EPOCH 14:\n",
      "LOSS train 6.5568136834017395e-06 valid 2.0835764189541806e-06\n",
      "EPOCH 15:\n",
      "LOSS train 6.188236103404814e-06 valid 1.5836337752261898e-06\n",
      "EPOCH 16:\n",
      "LOSS train 5.9661708362877035e-06 valid 2.4299304186570225e-06\n",
      "EPOCH 17:\n",
      "LOSS train 5.525394370056581e-06 valid 2.535761041144724e-06\n",
      "EPOCH 18:\n",
      "LOSS train 5.8041123898269735e-06 valid 2.978535576403374e-06\n",
      "EPOCH 19:\n",
      "LOSS train 4.8506563039006116e-06 valid 2.4747107545408653e-06\n",
      "EPOCH 20:\n",
      "LOSS train 5.721093056038398e-06 valid 2.247170641567209e-06\n",
      "(3, 0, False, 128, 0.0025, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 6.127066416271781e-05 valid 2.0035031411680393e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.416257215363148e-06 valid 5.493464868777664e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.7469121783238685e-06 valid 1.2979215853192727e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.0372978306896925e-06 valid 1.0184388656853116e-06\n",
      "(3, 0, False, 128, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0003873781278389913 valid 1.1774976883316413e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.975459143158473e-06 valid 1.792343937268015e-05\n",
      "EPOCH 3:\n",
      "LOSS train 9.457346897792317e-06 valid 9.167161806544755e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.139796295245412e-06 valid 6.782169293728657e-06\n",
      "EPOCH 5:\n",
      "LOSS train 6.8225720090249774e-06 valid 9.11211009224644e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.0172297397804518e-05 valid 1.0842116353160236e-05\n",
      "EPOCH 7:\n",
      "LOSS train 1.0925756436225916e-05 valid 1.135372349381214e-05\n",
      "EPOCH 8:\n",
      "LOSS train 1.0254075679728535e-05 valid 1.0473569091118407e-05\n",
      "(3, 0, False, 128, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0001177748452149996 valid 1.5207903743430506e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.629554105816619e-06 valid 8.108011570584495e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.622405221816719e-06 valid 7.08797460902133e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.6591784361893404e-06 valid 5.961512670182856e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.0246563410494756e-06 valid 4.164945494267158e-06\n",
      "EPOCH 6:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 2.524407400465718e-06 valid 3.182419959557592e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.2041375744170353e-06 valid 3.0335365863720654e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.6656585298237103e-06 valid 1.7525734392620507e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.8181619173670037e-06 valid 1.2325422176218126e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.6277597426279025e-06 valid 4.0033737604971975e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.4952803255776439e-06 valid 2.2646900106337853e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.6114690684626392e-06 valid 2.3885793325462146e-06\n",
      "(3, 0, False, 128, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00021132754971834835 valid 2.935411657745135e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.108161480182839e-06 valid 1.9534379589458695e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.13524521377635e-06 valid 2.2755762074666563e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.165648372239238e-06 valid 4.343703494669171e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.702230574028174e-06 valid 2.3431011868524365e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.5236700374031658e-06 valid 2.392100668657804e-06\n",
      "EPOCH 7:\n",
      "LOSS train 5.49820569994692e-05 valid 0.00019894856086466461\n",
      "EPOCH 8:\n",
      "LOSS train 7.286053983880796e-05 valid 9.870042413240299e-05\n",
      "EPOCH 9:\n",
      "LOSS train 0.00013158743952332937 valid 8.15892854006961e-05\n",
      "EPOCH 10:\n",
      "LOSS train 0.00010463312039277004 valid 7.248690963024274e-05\n",
      "EPOCH 11:\n",
      "LOSS train 9.443604571314018e-05 valid 6.501078314613551e-05\n",
      "EPOCH 12:\n",
      "LOSS train 3.3422595679443745e-05 valid 4.438058567757253e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.2577080298505783e-05 valid 5.21004585607443e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.2012126044173533e-05 valid 1.2153566785855219e-05\n",
      "EPOCH 15:\n",
      "LOSS train 9.788455632760691e-06 valid 5.951298589934595e-06\n",
      "EPOCH 16:\n",
      "LOSS train 6.771903589132225e-06 valid 2.404644328635186e-05\n",
      "(3, 0, False, 128, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0004855543346199943 valid 1.756934216246009e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.2061570081168048e-05 valid 9.888442946248688e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.1758901936777202e-05 valid 7.217270649562124e-06\n",
      "EPOCH 4:\n",
      "LOSS train 6.347839884220303e-06 valid 8.648567927593831e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.9519905495947e-06 valid 9.341846634924877e-06\n",
      "EPOCH 6:\n",
      "LOSS train 6.157984563007249e-06 valid 1.166076890513068e-05\n",
      "EPOCH 7:\n",
      "LOSS train 8.22535051421666e-06 valid 1.2375227015581913e-05\n",
      "EPOCH 8:\n",
      "LOSS train 9.892106525055944e-06 valid 1.1284376341791358e-05\n",
      "EPOCH 9:\n",
      "LOSS train 7.3228662424351165e-06 valid 1.2155123840784654e-05\n",
      "EPOCH 10:\n",
      "LOSS train 7.157697823232283e-06 valid 1.2210784916533157e-05\n",
      "EPOCH 11:\n",
      "LOSS train 6.620791605639327e-06 valid 1.1841029845527373e-05\n",
      "EPOCH 12:\n",
      "LOSS train 5.8236754142431186e-06 valid 1.0272568033542484e-05\n",
      "EPOCH 13:\n",
      "LOSS train 4.9932930734546855e-06 valid 8.477149094687775e-06\n",
      "EPOCH 14:\n",
      "LOSS train 4.708225156477543e-06 valid 8.26728773972718e-06\n",
      "EPOCH 15:\n",
      "LOSS train 5.0985030927294285e-06 valid 6.774895609851228e-06\n",
      "EPOCH 16:\n",
      "LOSS train 6.742736522935822e-06 valid 6.0132183534733485e-06\n",
      "EPOCH 17:\n",
      "LOSS train 6.308379850024783e-06 valid 4.6020245463296305e-06\n",
      "EPOCH 18:\n",
      "LOSS train 5.616947861723438e-06 valid 2.659329538801103e-06\n",
      "EPOCH 19:\n",
      "LOSS train 1.804743610369977e-06 valid 1.939035428222269e-06\n",
      "EPOCH 20:\n",
      "LOSS train 1.5271935633879439e-06 valid 2.7100913939648308e-06\n",
      "(3, 0, False, 128, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.002417256183175588 valid 4.455756788956933e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.41780310554188e-05 valid 6.603420479223132e-05\n",
      "EPOCH 3:\n",
      "LOSS train 7.999245044426636e-05 valid 1.2356444131000899e-05\n",
      "EPOCH 4:\n",
      "LOSS train 2.3921825540150996e-05 valid 4.824336429010145e-06\n",
      "(3, 0, False, 128, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0007789676191553107 valid 0.00021994767303112894\n",
      "EPOCH 2:\n",
      "LOSS train 0.00010061813033153595 valid 7.737562555121258e-05\n",
      "EPOCH 3:\n",
      "LOSS train 7.295463275805713e-05 valid 1.174654607893899e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.2058521268672504e-05 valid 6.758812105545076e-06\n",
      "EPOCH 5:\n",
      "LOSS train 0.00010395525352517315 valid 4.85108103021048e-05\n",
      "EPOCH 6:\n",
      "LOSS train 0.00018155761107850892 valid 0.00013207619485910982\n",
      "EPOCH 7:\n",
      "LOSS train 0.0001808967428159312 valid 0.00012186119420221075\n",
      "EPOCH 8:\n",
      "LOSS train 0.00016893543168351375 valid 0.00011756135063478723\n",
      "(3, 0, False, 128, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.006583915131874239 valid 8.744413207750767e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.095541414519544e-05 valid 4.161416654824279e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.326585145755282e-05 valid 0.00024091317027341574\n",
      "EPOCH 4:\n",
      "LOSS train 2.859478768874409e-05 valid 3.387637480045669e-05\n",
      "EPOCH 5:\n",
      "LOSS train 5.188771499880966e-06 valid 9.610141205484979e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.2528138997111649e-06 valid 3.5560328797146212e-06\n",
      "EPOCH 7:\n",
      "LOSS train 7.509784621394056e-07 valid 7.983845762282726e-07\n",
      "EPOCH 8:\n",
      "LOSS train 7.252826903854261e-07 valid 5.591523404291365e-07\n",
      "EPOCH 9:\n",
      "LOSS train 5.375784275370208e-07 valid 1.0371215921622934e-06\n",
      "EPOCH 10:\n",
      "LOSS train 5.354251954365721e-07 valid 1.2258574315637816e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.6791910160718887e-06 valid 2.518067049095407e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.2511085557336402e-06 valid 1.1530706842677318e-06\n",
      "(3, 0, False, 128, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0011755690924664687 valid 5.4027041187509894e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.395498931183499e-05 valid 6.125213985797018e-05\n",
      "EPOCH 3:\n",
      "LOSS train 8.908178723533288e-05 valid 7.160737732192501e-05\n",
      "EPOCH 4:\n",
      "LOSS train 0.00010640519480604525 valid 8.407904533669353e-05\n",
      "EPOCH 5:\n",
      "LOSS train 0.00012394977897095633 valid 9.559829777572304e-05\n",
      "EPOCH 6:\n",
      "LOSS train 0.00013866259762638497 valid 0.0001040570714394562\n",
      "EPOCH 7:\n",
      "LOSS train 0.0001488785406713775 valid 0.00010925655806204304\n",
      "EPOCH 8:\n",
      "LOSS train 0.00015503262659070425 valid 0.00011213611287530512\n",
      "EPOCH 9:\n",
      "LOSS train 0.00015844015251122764 valid 0.00011365671525709331\n",
      "EPOCH 10:\n",
      "LOSS train 0.00016025227057955371 valid 0.0001144468187703751\n",
      "EPOCH 11:\n",
      "LOSS train 0.00016120142781316966 valid 0.00011485628056107089\n",
      "EPOCH 12:\n",
      "LOSS train 0.0001616971453738666 valid 0.0001150691241491586\n",
      "EPOCH 13:\n",
      "LOSS train 0.00016195676725435087 valid 0.00011518025712575763\n",
      "EPOCH 14:\n",
      "LOSS train 0.00016209305317442443 valid 0.00011523848661454394\n",
      "EPOCH 15:\n",
      "LOSS train 0.00016216503565610264 valid 0.00011526907474035397\n",
      "EPOCH 16:\n",
      "LOSS train 0.0001622031256704343 valid 0.00011528522736625746\n",
      "(3, 0, False, 128, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.009128840183333386 valid 0.00047004668158479035\n",
      "EPOCH 2:\n",
      "LOSS train 6.482530584419526e-05 valid 1.0847988960449584e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.838994983498265e-06 valid 8.58466955833137e-06\n",
      "EPOCH 4:\n",
      "LOSS train 7.278916068008987e-06 valid 1.2997787962376606e-05\n",
      "EPOCH 5:\n",
      "LOSS train 9.195289059447269e-06 valid 1.1662987162708305e-05\n",
      "EPOCH 6:\n",
      "LOSS train 9.930835913918759e-06 valid 4.781262305186829e-06\n",
      "EPOCH 7:\n",
      "LOSS train 5.993246805514603e-06 valid 8.183398676919751e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.383036547943304e-06 valid 7.898986950749531e-06\n",
      "EPOCH 9:\n",
      "LOSS train 4.9500847383150045e-06 valid 1.0549459148023743e-05\n",
      "EPOCH 10:\n",
      "LOSS train 6.544442750679079e-06 valid 6.8900867518095765e-06\n",
      "EPOCH 11:\n",
      "LOSS train 7.808665636683558e-06 valid 1.0988634130626451e-05\n",
      "EPOCH 12:\n",
      "LOSS train 7.351690320496489e-06 valid 7.5473876677278895e-06\n",
      "EPOCH 13:\n",
      "LOSS train 8.231383873180675e-06 valid 1.18469943117816e-05\n",
      "EPOCH 14:\n",
      "LOSS train 6.390695445391107e-06 valid 1.0271289283991791e-05\n",
      "EPOCH 15:\n",
      "LOSS train 5.62287691726245e-06 valid 9.87582461675629e-06\n",
      "EPOCH 16:\n",
      "LOSS train 5.467468780398356e-06 valid 8.261527909780852e-06\n",
      "EPOCH 17:\n",
      "LOSS train 5.380033894076777e-06 valid 9.673946806287859e-06\n",
      "EPOCH 18:\n",
      "LOSS train 4.412088931638715e-05 valid 0.00012288354628253728\n",
      "EPOCH 19:\n",
      "LOSS train 3.476814986217577e-05 valid 0.00011376447946531698\n",
      "EPOCH 20:\n",
      "LOSS train 5.623989791055874e-05 valid 0.00028421173919923604\n",
      "(3, 0, False, 128, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.4758513928694642 valid 5.083358701085672e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.3812073369844914e-05 valid 5.101181523059495e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.522787199241947e-05 valid 5.125583629705943e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.6747846004005664e-05 valid 5.158066051080823e-05\n",
      "(3, 0, False, 128, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.0242108808307144 valid 0.00034213525941595435\n",
      "EPOCH 2:\n",
      "LOSS train 0.00012452149735743415 valid 0.00031319583649747074\n",
      "EPOCH 3:\n",
      "LOSS train 7.129405536109039e-05 valid 0.00027580856112763286\n",
      "EPOCH 4:\n",
      "LOSS train 6.139957300005135e-05 valid 0.00027207875973545015\n",
      "EPOCH 5:\n",
      "LOSS train 6.558131813858147e-05 valid 0.0002911939227487892\n",
      "EPOCH 6:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 7.621864883488958e-05 valid 0.0003166344831697643\n",
      "EPOCH 7:\n",
      "LOSS train 8.647632609551951e-05 valid 0.00033179973252117634\n",
      "EPOCH 8:\n",
      "LOSS train 9.094343404301237e-05 valid 0.00033524175523780286\n",
      "(3, 0, False, 128, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.4786507735058574 valid 7.224559522001073e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.63231327088386e-05 valid 1.4301442206487991e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.9607560257154672e-05 valid 3.464197288849391e-05\n",
      "EPOCH 4:\n",
      "LOSS train 0.00014108932086893044 valid 1.1293612260487862e-05\n",
      "EPOCH 5:\n",
      "LOSS train 1.5388261977394065e-05 valid 9.29099041968584e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.1998212039123455e-05 valid 2.0111781850573607e-05\n",
      "EPOCH 7:\n",
      "LOSS train 0.0009808809812692247 valid 6.074446355341934e-05\n",
      "EPOCH 8:\n",
      "LOSS train 7.766626945699671e-05 valid 5.883495032321662e-05\n",
      "EPOCH 9:\n",
      "LOSS train 7.553187086634658e-05 valid 5.838773722643964e-05\n",
      "EPOCH 10:\n",
      "LOSS train 7.703182227081256e-05 valid 6.0224778280826285e-05\n",
      "EPOCH 11:\n",
      "LOSS train 8.28167503453928e-05 valid 6.479625881183892e-05\n",
      "EPOCH 12:\n",
      "LOSS train 9.271128656703623e-05 valid 7.274198287632316e-05\n",
      "(3, 0, False, 128, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 12.397006228997302 valid 0.00015405239537358284\n",
      "EPOCH 2:\n",
      "LOSS train 2.4154270726823527e-05 valid 1.5470157450181432e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.7252210090788465e-05 valid 1.2630680430447683e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.627169384523549e-05 valid 1.2751702342939097e-05\n",
      "EPOCH 5:\n",
      "LOSS train 1.4792559597092045e-05 valid 1.478746435168432e-05\n",
      "EPOCH 6:\n",
      "LOSS train 1.6367632729020522e-05 valid 1.0565278898866381e-05\n",
      "EPOCH 7:\n",
      "LOSS train 1.2616530257888993e-05 valid 9.388528269482777e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.1990559189034043e-05 valid 8.954697477747686e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.0473415415375026e-05 valid 9.762215086084325e-06\n",
      "EPOCH 10:\n",
      "LOSS train 0.002660735605641281 valid 6.637932528974488e-05\n",
      "EPOCH 11:\n",
      "LOSS train 0.00013935743931286265 valid 2.7228201361140236e-05\n",
      "EPOCH 12:\n",
      "LOSS train 8.84082226484273e-05 valid 1.5294253898900934e-05\n",
      "EPOCH 13:\n",
      "LOSS train 3.771363308033742e-05 valid 1.778264413587749e-05\n",
      "EPOCH 14:\n",
      "LOSS train 2.0794853625276802e-05 valid 2.6249887014273554e-05\n",
      "EPOCH 15:\n",
      "LOSS train 1.8236640272171324e-05 valid 3.815961827058345e-05\n",
      "EPOCH 16:\n",
      "LOSS train 2.1083338865799124e-05 valid 5.434011109173298e-05\n",
      "(3, 0, False, 128, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.13394916809018614 valid 2.2264655854087323e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.5616877447035885e-05 valid 1.7507607481093146e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.379944604434474e-05 valid 1.6398575098719448e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.372533252154823e-05 valid 1.2896339285362046e-05\n",
      "EPOCH 5:\n",
      "LOSS train 1.015892393445366e-05 valid 1.0634055797709152e-05\n",
      "EPOCH 6:\n",
      "LOSS train 6.065254170936609e-06 valid 3.2503648981219158e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.0443581432277167e-06 valid 3.0211351713660406e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.4335355589770765e-06 valid 3.6020580864715157e-06\n",
      "EPOCH 9:\n",
      "LOSS train 3.1716436591221632e-06 valid 2.5688614186947234e-06\n",
      "EPOCH 10:\n",
      "LOSS train 3.0591853268384366e-06 valid 2.733934252319159e-06\n",
      "EPOCH 11:\n",
      "LOSS train 3.4110789802438174e-06 valid 1.8380596884526312e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.8289534845724767e-06 valid 1.9030849216505885e-06\n",
      "EPOCH 13:\n",
      "LOSS train 2.8167190486964785e-06 valid 1.6422005728600197e-06\n",
      "EPOCH 14:\n",
      "LOSS train 2.8971545883213215e-06 valid 1.7999396959567093e-06\n",
      "EPOCH 15:\n",
      "LOSS train 3.697549722720862e-06 valid 1.454085690966167e-06\n",
      "EPOCH 16:\n",
      "LOSS train 3.2858898222163567e-06 valid 1.397109940626251e-06\n",
      "EPOCH 17:\n",
      "LOSS train 5.036629390379901e-06 valid 1.772481482475996e-06\n",
      "EPOCH 18:\n",
      "LOSS train 2.899713043090478e-06 valid 1.6146467487487826e-06\n",
      "EPOCH 19:\n",
      "LOSS train 2.7047842656899942e-06 valid 2.186799520131899e-06\n",
      "EPOCH 20:\n",
      "LOSS train 2.298100468056979e-06 valid 2.6980790153174894e-06\n",
      "(3, 0, False, 256, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 7.46900592144961e-05 valid 1.2608751603693236e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.123146454183968e-06 valid 3.899523562722607e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.2982866072861225e-06 valid 2.3130003228288842e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.5047896524277456e-06 valid 1.2330883691902272e-06\n",
      "(3, 0, False, 256, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 8.33295234209401e-05 valid 5.487701400852529e-06\n",
      "EPOCH 2:\n",
      "LOSS train 4.357771605436813e-06 valid 1.1462294423836283e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.763388055270758e-06 valid 1.7916865999723086e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.885610529561733e-06 valid 2.8402582756825723e-05\n",
      "EPOCH 5:\n",
      "LOSS train 1.1744453721552704e-05 valid 3.507108885969501e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.416488499955617e-06 valid 1.6656308616802562e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.2427988632792733e-06 valid 1.3861450725016766e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.943388854943363e-06 valid 1.43051272516459e-06\n",
      "(3, 0, False, 256, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00018889671483530285 valid 6.593313173652859e-06\n",
      "EPOCH 2:\n",
      "LOSS train 4.3906903633968206e-06 valid 2.1392395410657628e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.792377825490392e-06 valid 7.063300472509582e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.3168250814733754e-05 valid 0.00011596852709772065\n",
      "EPOCH 5:\n",
      "LOSS train 1.924625987363495e-05 valid 5.925971436226973e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.7808509073669086e-06 valid 1.985006065297057e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.8687680576578078e-06 valid 2.6595382678351598e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.3294481823537876e-06 valid 2.97872588816972e-06\n",
      "EPOCH 9:\n",
      "LOSS train 3.792781587415211e-06 valid 2.6711973077908624e-06\n",
      "EPOCH 10:\n",
      "LOSS train 3.314550639677766e-06 valid 1.2758101775034447e-06\n",
      "EPOCH 11:\n",
      "LOSS train 4.635524911940568e-06 valid 3.985786861449014e-06\n",
      "EPOCH 12:\n",
      "LOSS train 5.032448745260833e-06 valid 2.711784873099532e-06\n",
      "(3, 0, False, 256, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.87680719107044e-05 valid 2.7398564270697534e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.5255747152797295e-06 valid 2.5634556095610606e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.7369882866597897e-06 valid 4.271002580935601e-06\n",
      "EPOCH 4:\n",
      "LOSS train 7.31355737035653e-06 valid 1.1849969041577424e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.1122547774000304e-06 valid 6.617960934818257e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.3553760224720787e-06 valid 1.8043392628896981e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.348294719105095e-06 valid 2.124699449268519e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.6733458923705218e-06 valid 2.386522510278155e-06\n",
      "EPOCH 9:\n",
      "LOSS train 3.1650696535255157e-06 valid 5.803146905236645e-06\n",
      "EPOCH 10:\n",
      "LOSS train 5.794716865617462e-06 valid 3.716927949426463e-06\n",
      "EPOCH 11:\n",
      "LOSS train 5.068907300984729e-06 valid 4.095229087397456e-06\n",
      "EPOCH 12:\n",
      "LOSS train 7.87833690079947e-06 valid 6.478893283201614e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.5912260399329157e-05 valid 2.8937072329426883e-06\n",
      "EPOCH 14:\n",
      "LOSS train 5.657465529816986e-06 valid 3.118457243544981e-06\n",
      "EPOCH 15:\n",
      "LOSS train 8.366640844392911e-06 valid 2.906767804233823e-06\n",
      "EPOCH 16:\n",
      "LOSS train 4.649408435969622e-06 valid 4.378222456580261e-06\n",
      "(3, 0, False, 256, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0008464221459440548 valid 1.5572884876746684e-05\n",
      "EPOCH 2:\n",
      "LOSS train 9.332234584883126e-06 valid 4.124801307625603e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.7304911051148326e-06 valid 4.560128672892461e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.1741463602298757e-06 valid 3.6058386285731103e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.9416252595547746e-06 valid 3.751291615117225e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.467489615207354e-06 valid 3.1625147585145896e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.6180051963508533e-06 valid 4.885838279733434e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.0211623344550066e-05 valid 8.0617937783245e-05\n",
      "EPOCH 9:\n",
      "LOSS train 4.9453492195344515e-05 valid 2.708910187720903e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.954085567652116e-06 valid 2.7023834263673052e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.8378133259665236e-06 valid 2.7414992018748308e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.5587479074972664e-06 valid 2.7047806270275032e-06\n",
      "EPOCH 13:\n",
      "LOSS train 4.771430525352116e-06 valid 1.685098686721176e-05\n",
      "EPOCH 14:\n",
      "LOSS train 5.248888188612623e-05 valid 7.027891570032807e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.5700576650394633e-05 valid 2.5109211492235772e-06\n",
      "EPOCH 16:\n",
      "LOSS train 3.260841488870576e-06 valid 2.424734020678443e-06\n",
      "EPOCH 17:\n",
      "LOSS train 2.345572751066001e-06 valid 2.360229245823575e-06\n",
      "EPOCH 18:\n",
      "LOSS train 2.2763876521376255e-06 valid 3.079551561313565e-06\n",
      "EPOCH 19:\n",
      "LOSS train 1.5075906300196088e-05 valid 5.4407577408710495e-05\n",
      "EPOCH 20:\n",
      "LOSS train 3.9139940037336464e-05 valid 4.5397064241115e-06\n",
      "(3, 0, False, 256, 0.0025, 4, 0)\n",
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.00044745349377258396 valid 7.293009548448026e-05\n",
      "EPOCH 2:\n",
      "LOSS train 2.505998904562397e-05 valid 5.684355346602388e-05\n",
      "EPOCH 3:\n",
      "LOSS train 2.272931786212386e-05 valid 6.228301936062053e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.0319838673614914e-05 valid 0.0004620856780093163\n",
      "(3, 0, False, 256, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0011290791809430742 valid 0.0007013779832050204\n",
      "EPOCH 2:\n",
      "LOSS train 0.0002851579870156307 valid 8.464086931780912e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.874539871941274e-06 valid 3.2433465548820095e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.7480496994438573e-06 valid 3.4295615023438586e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.7244286943405314e-06 valid 1.2824259101762436e-05\n",
      "EPOCH 6:\n",
      "LOSS train 9.503115399891064e-06 valid 4.9407390179112554e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.116212919054953e-05 valid 0.00020365837553981692\n",
      "EPOCH 8:\n",
      "LOSS train 0.00040610437639309704 valid 4.4853059080196545e-05\n",
      "(3, 0, False, 256, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00017317346379519994 valid 0.0003376888344064355\n",
      "EPOCH 2:\n",
      "LOSS train 0.00010683664228343781 valid 1.908254125737585e-05\n",
      "EPOCH 3:\n",
      "LOSS train 7.11328519331036e-06 valid 1.92013499145105e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.5960842279984519e-06 valid 1.5489497400267283e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.143026514649797e-06 valid 1.1490762972243829e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.9857309777435773e-06 valid 1.0508452987778583e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.0236262885844596e-06 valid 8.986840498437232e-07\n",
      "EPOCH 8:\n",
      "LOSS train 3.686316068420756e-06 valid 5.561590660363436e-05\n",
      "EPOCH 9:\n",
      "LOSS train 4.935015226984489e-06 valid 1.5569024753858685e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.062268811148162e-06 valid 1.7937942402568297e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.540933905753226e-06 valid 1.4308740901469719e-05\n",
      "EPOCH 12:\n",
      "LOSS train 7.963637679063763e-06 valid 1.0454546099936124e-05\n",
      "(3, 0, False, 256, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00021471112205517784 valid 0.00038887799018993974\n",
      "EPOCH 2:\n",
      "LOSS train 0.00011683294770460783 valid 2.1945226762909442e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.0444999890146953e-05 valid 3.424280066610663e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.2674207799672866e-06 valid 4.636714038497303e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.131332649351812e-06 valid 5.5920754675753415e-06\n",
      "EPOCH 6:\n",
      "LOSS train 5.747057324135175e-06 valid 6.329140887828544e-06\n",
      "EPOCH 7:\n",
      "LOSS train 8.542752418058115e-06 valid 1.1137241017422639e-05\n",
      "EPOCH 8:\n",
      "LOSS train 1.8025870432843663e-05 valid 1.3035836673225276e-05\n",
      "EPOCH 9:\n",
      "LOSS train 1.2941392399380825e-05 valid 6.221540388651192e-06\n",
      "EPOCH 10:\n",
      "LOSS train 8.087548851138663e-06 valid 8.477752999169752e-06\n",
      "EPOCH 11:\n",
      "LOSS train 9.03814734669271e-06 valid 1.2571436855068896e-05\n",
      "EPOCH 12:\n",
      "LOSS train 1.1113910660987449e-05 valid 1.0215389920631424e-05\n",
      "EPOCH 13:\n",
      "LOSS train 1.5094242546568189e-05 valid 1.380946923745796e-05\n",
      "EPOCH 14:\n",
      "LOSS train 1.3781499977843373e-05 valid 1.3201269212004263e-05\n",
      "EPOCH 15:\n",
      "LOSS train 1.8248234575845058e-05 valid 9.879238859866746e-06\n",
      "EPOCH 16:\n",
      "LOSS train 2.313974606637461e-05 valid 1.7664824554231018e-05\n",
      "(3, 0, False, 256, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0002450680253865009 valid 2.48537562583806e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.1508478276292494e-05 valid 1.6875409301064792e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.2136781941704419e-06 valid 2.3608522496942896e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.848713568574592e-06 valid 3.3989708754234016e-06\n",
      "EPOCH 5:\n",
      "LOSS train 5.726697433168713e-06 valid 7.541972991020884e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.7880339217685705e-05 valid 6.480165757238865e-05\n",
      "EPOCH 7:\n",
      "LOSS train 1.2284157165741001e-05 valid 2.7747730655391933e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.052272010597659e-06 valid 3.859601292788284e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.2580703309388136e-05 valid 6.061192380002467e-06\n",
      "EPOCH 10:\n",
      "LOSS train 8.466192965070834e-06 valid 2.386205960647203e-05\n",
      "EPOCH 11:\n",
      "LOSS train 2.660311489472867e-05 valid 9.480066182732116e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.378136777061191e-05 valid 2.5563880626577884e-05\n",
      "EPOCH 13:\n",
      "LOSS train 2.8946184078541615e-05 valid 1.2248522580193821e-05\n",
      "EPOCH 14:\n",
      "LOSS train 1.6942355908786463e-05 valid 1.5789003100508125e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.0224811035770583e-05 valid 7.595694569317857e-06\n",
      "EPOCH 16:\n",
      "LOSS train 1.7968334950419626e-05 valid 1.2830459127144422e-05\n",
      "EPOCH 17:\n",
      "LOSS train 1.660305170152902e-05 valid 1.016394071484683e-05\n",
      "EPOCH 18:\n",
      "LOSS train 1.1414509171965171e-05 valid 1.3100219803163782e-05\n",
      "EPOCH 19:\n",
      "LOSS train 1.4046417861126654e-05 valid 8.667336260259617e-06\n",
      "EPOCH 20:\n",
      "LOSS train 9.658363395196643e-06 valid 1.0838667549251113e-05\n",
      "(3, 0, False, 256, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0024822028889664653 valid 0.0005956835811957717\n",
      "EPOCH 2:\n",
      "LOSS train 0.00014611747282636825 valid 3.715846469276585e-05\n",
      "EPOCH 3:\n",
      "LOSS train 2.5602307113726902e-05 valid 3.487787125777686e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.1816481579152088e-06 valid 2.667491344254813e-06\n",
      "(3, 0, False, 256, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.014845379400640013 valid 5.585574763244949e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.396406398720426e-05 valid 1.053097457770491e-05\n",
      "EPOCH 3:\n",
      "LOSS train 8.244648655622084e-06 valid 7.325251317524817e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.1586448903556143e-05 valid 1.7337879398837686e-05\n",
      "EPOCH 5:\n",
      "LOSS train 2.9518337582854433e-05 valid 2.640133970999159e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.37482486060415e-05 valid 2.006137765420135e-05\n",
      "EPOCH 7:\n",
      "LOSS train 1.3331076779268624e-05 valid 3.0436138331424445e-05\n",
      "EPOCH 8:\n",
      "LOSS train 8.826255625870681e-05 valid 4.8407004214823246e-05\n",
      "(3, 0, False, 256, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.003497534237304965 valid 0.0005351990694180131\n",
      "EPOCH 2:\n",
      "LOSS train 0.00012070904256701122 valid 5.312540815793909e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.3953568511457125e-05 valid 5.079617039882578e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.433213890969517e-05 valid 5.088518810225651e-05\n",
      "EPOCH 5:\n",
      "LOSS train 5.5057722208515124e-05 valid 5.098364272271283e-05\n",
      "EPOCH 6:\n",
      "LOSS train 5.577845620617353e-05 valid 5.106512981001288e-05\n",
      "EPOCH 7:\n",
      "LOSS train 5.647877871325201e-05 valid 5.0742786697810516e-05\n",
      "EPOCH 8:\n",
      "LOSS train 5.6928041150768714e-05 valid 4.9415517423767596e-05\n",
      "EPOCH 9:\n",
      "LOSS train 5.324205910118794e-05 valid 3.5588160244515166e-05\n",
      "EPOCH 10:\n",
      "LOSS train 1.4900675073212283e-05 valid 1.8468062989995815e-06\n",
      "EPOCH 11:\n",
      "LOSS train 7.682176492505266e-06 valid 8.521125892002601e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.564716752154765e-06 valid 1.747468559187837e-06\n",
      "(3, 0, False, 256, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.003442281211549745 valid 4.153781628701836e-05\n",
      "EPOCH 2:\n",
      "LOSS train 2.914935708674906e-05 valid 5.3179915084911045e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.312968521472675e-06 valid 4.37825292465277e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.4221205973863607e-06 valid 2.476635927450843e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.749601349782994e-06 valid 1.6119372958200984e-05\n",
      "EPOCH 6:\n",
      "LOSS train 1.648501152108609e-05 valid 4.055108547618147e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.5254543834074093e-06 valid 2.219378984591458e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.305734233399836e-06 valid 3.5602404295786982e-06\n",
      "EPOCH 9:\n",
      "LOSS train 4.254477898114115e-06 valid 3.7256809264363255e-06\n",
      "EPOCH 10:\n",
      "LOSS train 7.531586655746551e-06 valid 2.2386539058061317e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.135015037840737e-06 valid 1.184422444566735e-06\n",
      "EPOCH 12:\n",
      "LOSS train 6.4996452888314945e-06 valid 2.9065340640954673e-06\n",
      "EPOCH 13:\n",
      "LOSS train 2.738720195650299e-06 valid 5.424652499641525e-06\n",
      "EPOCH 14:\n",
      "LOSS train 5.975584059569883e-06 valid 3.308996610940085e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.550362172353077e-06 valid 1.6729392200431903e-06\n",
      "EPOCH 16:\n",
      "LOSS train 4.703780379872712e-06 valid 6.793767624913016e-06\n",
      "(3, 0, False, 256, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0025460028066702275 valid 5.068838072475046e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.3225083494708834e-05 valid 5.075756780570373e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.4212485014282373e-05 valid 5.089625483378768e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.512720481295231e-05 valid 5.1071012421743944e-05\n",
      "EPOCH 5:\n",
      "LOSS train 5.602174274976627e-05 valid 5.123719165567309e-05\n",
      "EPOCH 6:\n",
      "LOSS train 5.691820023121306e-05 valid 5.136707113706507e-05\n",
      "EPOCH 7:\n",
      "LOSS train 5.782587292854521e-05 valid 5.145899922354147e-05\n",
      "EPOCH 8:\n",
      "LOSS train 5.8748177501886494e-05 valid 5.153668826096691e-05\n",
      "EPOCH 9:\n",
      "LOSS train 5.968562416530578e-05 valid 5.163701644050889e-05\n",
      "EPOCH 10:\n",
      "LOSS train 6.063725793690736e-05 valid 5.17832268087659e-05\n",
      "EPOCH 11:\n",
      "LOSS train 6.159981639690184e-05 valid 5.198310100240633e-05\n",
      "EPOCH 12:\n",
      "LOSS train 6.256668496474684e-05 valid 5.223869811743498e-05\n",
      "EPOCH 13:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 6.352890996411539e-05 valid 5.254281859379262e-05\n",
      "EPOCH 14:\n",
      "LOSS train 6.447650809592507e-05 valid 5.287523163133301e-05\n",
      "EPOCH 15:\n",
      "LOSS train 6.539876573684798e-05 valid 5.320999480318278e-05\n",
      "EPOCH 16:\n",
      "LOSS train 6.628453876732088e-05 valid 5.352649532142095e-05\n",
      "EPOCH 17:\n",
      "LOSS train 6.71232113346963e-05 valid 5.381402661441825e-05\n",
      "EPOCH 18:\n",
      "LOSS train 6.790568367661812e-05 valid 5.4069860198069364e-05\n",
      "EPOCH 19:\n",
      "LOSS train 6.862514097420363e-05 valid 5.429514203569852e-05\n",
      "EPOCH 20:\n",
      "LOSS train 6.927742329212815e-05 valid 5.449238597066142e-05\n",
      "(3, 0, False, 256, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 2.237160091813428 valid 0.00018833635840564966\n",
      "EPOCH 2:\n",
      "LOSS train 3.883984651209842e-05 valid 4.690064542955952e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.5183936939677563e-06 valid 3.780235147132771e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.380435909715033e-06 valid 3.625791123340605e-06\n",
      "(3, 0, False, 256, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.224639415519131 valid 0.0018509146757423878\n",
      "EPOCH 2:\n",
      "LOSS train 0.0005291931234277811 valid 0.002090077381581068\n",
      "EPOCH 3:\n",
      "LOSS train 0.0005843839059196722 valid 0.0016740286955609918\n",
      "EPOCH 4:\n",
      "LOSS train 0.0005184146899060978 valid 0.0013837926089763641\n",
      "EPOCH 5:\n",
      "LOSS train 0.0004214191498224502 valid 0.0012172767892479897\n",
      "EPOCH 6:\n",
      "LOSS train 0.0003594289476881984 valid 0.0013398986775428057\n",
      "EPOCH 7:\n",
      "LOSS train 0.0004095991877364537 valid 0.0023227694910019636\n",
      "EPOCH 8:\n",
      "LOSS train 0.0014951301616074738 valid 0.001162384869530797\n",
      "(3, 0, False, 256, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.1030842263922276 valid 0.00010044073860626668\n",
      "EPOCH 2:\n",
      "LOSS train 7.759705925446437e-05 valid 3.8985981518635526e-05\n",
      "EPOCH 3:\n",
      "LOSS train 2.4741511252421033e-05 valid 3.8526628486579284e-05\n",
      "EPOCH 4:\n",
      "LOSS train 2.2353813524703533e-05 valid 4.1867046093102545e-05\n",
      "EPOCH 5:\n",
      "LOSS train 3.311072676029179e-05 valid 0.00017824112728703767\n",
      "EPOCH 6:\n",
      "LOSS train 0.00016011382508498184 valid 0.0013091907603666186\n",
      "EPOCH 7:\n",
      "LOSS train 0.001879086571746372 valid 0.0034171382430940866\n",
      "EPOCH 8:\n",
      "LOSS train 0.00034193933567795996 valid 1.3850593859388027e-05\n",
      "EPOCH 9:\n",
      "LOSS train 8.704092891581203e-06 valid 1.4897697838023305e-05\n",
      "EPOCH 10:\n",
      "LOSS train 7.371678343985553e-06 valid 1.3228380339569412e-05\n",
      "EPOCH 11:\n",
      "LOSS train 7.083983585278487e-06 valid 1.3513640624296386e-05\n",
      "EPOCH 12:\n",
      "LOSS train 7.6133886740806655e-06 valid 1.209003858093638e-05\n",
      "(3, 0, False, 256, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.6385280898880286 valid 5.485373185365461e-05\n",
      "EPOCH 2:\n",
      "LOSS train 0.00019576825451706522 valid 9.053018584381789e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.368570502892683e-05 valid 1.9646462533273734e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.0118307477140761e-05 valid 3.5889886476070387e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.7978231750619495e-06 valid 3.210070190107217e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.1610012109928766e-06 valid 5.0003295655187685e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.3282662731532819e-05 valid 1.96704022528138e-05\n",
      "EPOCH 8:\n",
      "LOSS train 2.173094403027391e-05 valid 4.330208867031615e-06\n",
      "EPOCH 9:\n",
      "LOSS train 7.609897911357646e-06 valid 2.6209288535028463e-06\n",
      "EPOCH 10:\n",
      "LOSS train 3.029857310626502e-06 valid 2.6177797280979576e-06\n",
      "EPOCH 11:\n",
      "LOSS train 4.131020323965558e-06 valid 3.3891330986079993e-06\n",
      "EPOCH 12:\n",
      "LOSS train 9.769145229751775e-06 valid 3.6820124478254e-06\n",
      "EPOCH 13:\n",
      "LOSS train 5.757016986770127e-06 valid 3.239204943383811e-06\n",
      "EPOCH 14:\n",
      "LOSS train 9.959936869797138e-06 valid 2.0589350242516957e-06\n",
      "EPOCH 15:\n",
      "LOSS train 3.2748910951586867e-06 valid 7.219481176434783e-06\n",
      "EPOCH 16:\n",
      "LOSS train 0.0012485651302011145 valid 7.429438119288534e-05\n",
      "(3, 0, False, 256, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.1644379810996999 valid 2.1944335458101705e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.6141357831009073e-05 valid 1.4083404494158458e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.714120400231392e-06 valid 9.8041136880056e-06\n",
      "EPOCH 4:\n",
      "LOSS train 9.883351243724849e-06 valid 1.9507833712850697e-05\n",
      "EPOCH 5:\n",
      "LOSS train 1.7308493786209534e-05 valid 3.3456988603575155e-05\n",
      "EPOCH 6:\n",
      "LOSS train 1.778687470576885e-05 valid 3.137577004963532e-05\n",
      "EPOCH 7:\n",
      "LOSS train 1.3143749973949848e-05 valid 1.956231972144451e-05\n",
      "EPOCH 8:\n",
      "LOSS train 9.914456267377518e-06 valid 1.1791605174948927e-05\n",
      "EPOCH 9:\n",
      "LOSS train 1.0005437123103461e-05 valid 9.064980076800566e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.078163010920154e-05 valid 8.276066182588693e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.0579226486401838e-05 valid 1.0645037036738358e-05\n",
      "EPOCH 12:\n",
      "LOSS train 9.748670148711701e-06 valid 1.5005757632025052e-05\n",
      "EPOCH 13:\n",
      "LOSS train 1.0133529067217428e-05 valid 1.5933977920212783e-05\n",
      "EPOCH 14:\n",
      "LOSS train 1.1205992762663225e-05 valid 1.6461168343084864e-05\n",
      "EPOCH 15:\n",
      "LOSS train 1.604707803262375e-05 valid 2.3137667085393332e-05\n",
      "EPOCH 16:\n",
      "LOSS train 1.0648872398088029e-05 valid 1.4168258530844469e-05\n",
      "EPOCH 17:\n",
      "LOSS train 1.0106533758118207e-05 valid 1.2698961654677987e-05\n",
      "EPOCH 18:\n",
      "LOSS train 1.3695053115413916e-05 valid 1.7677077266853303e-05\n",
      "EPOCH 19:\n",
      "LOSS train 2.083405722956205e-05 valid 2.4588323867646977e-05\n",
      "EPOCH 20:\n",
      "LOSS train 3.2434789393111514e-05 valid 3.6544774047797546e-05\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "nbs_hidden = [3]\n",
    "dors = [0]#,0.05,0.1]#,0.05]\n",
    "relu_outs=[False]\n",
    "\n",
    "batch_sizes = [32,64,128,256]\n",
    "learning_rates = [0.0025*4**i for i in range(-1,3,1)]\n",
    "nbs_e = [4,8,12,16,20] # ,8]\n",
    "negative_penalisations = [0]\n",
    "\n",
    "\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_test\"\n",
    "\n",
    "hp_sets = ((nb_h,dor,relu_out,bs,lr,nb_e,np) for nb_h in nbs_hidden for dor in dors for relu_out in relu_outs for bs in batch_sizes for lr in learning_rates for nb_e in nbs_e for np in negative_penalisations)\n",
    "\n",
    "\n",
    "for hp_set in hp_sets:\n",
    "    print(hp_set)\n",
    "    nb_hidden,dor,relu_out,bs,lr,nb_e,np = hp_set[0],hp_set[1],hp_set[2],hp_set[3],hp_set[4],hp_set[5],hp_set[6]\n",
    "    \n",
    "    #Create training and validation loaders based on batch size\n",
    "    training_loader = DataLoader(train,batch_size=bs)\n",
    "    validation_loader = DataLoader(train,batch_size=bs)\n",
    "    \n",
    "    #Initialize loss functions\n",
    "    loss_fn = NN_classes.create_loss_fn(penalize_negative=np)\n",
    "    loss_t_mse = torch.nn.MSELoss()\n",
    "    \n",
    "    #Create model based on hyperparameter set\n",
    "    m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor,relu_out=relu_out)\n",
    "    #Create model name for saving and loading\n",
    "    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro\"\n",
    "    #Create optimizer based on learning rate \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    #Train the actual model \n",
    "    t_start_train = time.perf_counter()\n",
    "    train_loss_1 = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)[0]\n",
    "    t_stop_train = time.perf_counter()\n",
    "    \n",
    "    #In the following loop, we retreive the models from saved locations and calculate losses \n",
    "    for mt in [\"min_val\",\"all_epochs\"]:\n",
    "        t_start_eval = time.perf_counter()\n",
    "        path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "        \n",
    "        #Retreive model state and set to evaluation mode\n",
    "        m.load_state_dict(torch.load(path))\n",
    "        m.eval()\n",
    "        \n",
    "        #Calculate losses\n",
    "        test_predictions = m(d_ft_in[\"test\"].float())\n",
    "        test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "        test_loss_t_mse = loss_t_mse(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "\n",
    "        train_predictions = m(d_ft_in[\"train\"].float())\n",
    "        train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "        train_loss_t_mse = loss_t_mse(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "\n",
    "        validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "        validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "        validation_loss_t_mse = loss_t_mse(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "        t_stop_eval = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        #Calculate some calculation times \n",
    "        t_train = t_stop_train - t_start_train\n",
    "        t_eval = t_stop_eval - t_start_eval\n",
    "        \n",
    "        #Finally, save all desired values in a dataframe\n",
    "        r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                        \"Dor\": dor,\n",
    "                        \"Relu_out\": relu_out,\n",
    "                        \"Batch_size\": bs,\n",
    "                        \"Lr\":lr,\n",
    "                        \"Epochs\": nb_e,\n",
    "                        \"Np\": np,\n",
    "                        \"Min_val\":mt,\n",
    "                        \"Tr_l\":train_loss.item(),\n",
    "                        \"Te_l\":test_loss.item(),\n",
    "                        \"V_l\": validation_loss.item(),\n",
    "                        \"Tr_l_t_mse\":train_loss_t_mse.item(),\n",
    "                        \"Te_l_t_mse\":test_loss_t_mse.item(),\n",
    "                        \"V_l_t_mse\": validation_loss_t_mse.item(),\n",
    "                        \"Tr_l_ret\": train_loss_1.item(),\n",
    "                        \"Train_time\": t_train,\n",
    "                        \"Eval_time\": t_eval\n",
    "                         }\n",
    "                        ,index = [i])\n",
    "        i+=1\n",
    "        results = pd.concat([results,r])\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b9ce04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Dor</th>\n",
       "      <th>Relu_out</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Np</th>\n",
       "      <th>Min_val</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "      <th>V_l</th>\n",
       "      <th>Tr_l_t_mse</th>\n",
       "      <th>Te_l_t_mse</th>\n",
       "      <th>V_l_t_mse</th>\n",
       "      <th>Tr_l_ret</th>\n",
       "      <th>Train_time</th>\n",
       "      <th>Eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.691215</td>\n",
       "      <td>0.069661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.691215</td>\n",
       "      <td>0.081346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.367656</td>\n",
       "      <td>0.079879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.367656</td>\n",
       "      <td>0.085993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>4.380685</td>\n",
       "      <td>0.073850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>4.380685</td>\n",
       "      <td>0.088889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>5.026957</td>\n",
       "      <td>0.057785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>5.026957</td>\n",
       "      <td>0.084875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>3.179991</td>\n",
       "      <td>0.084665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>3.179991</td>\n",
       "      <td>0.089942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model_type  Dor  Relu_out  Batch_size        Lr  Epochs  Np     Min_val   \n",
       "0           3    0     False          32  0.000625       4   0     min_val  \\\n",
       "1           3    0     False          32  0.000625       4   0  all_epochs   \n",
       "2           3    0     False          32  0.002500       4   0     min_val   \n",
       "3           3    0     False          32  0.002500       4   0  all_epochs   \n",
       "4           3    0     False          32  0.010000       4   0     min_val   \n",
       "5           3    0     False          32  0.010000       4   0  all_epochs   \n",
       "6           3    0     False          32  0.040000       4   0     min_val   \n",
       "7           3    0     False          32  0.040000       4   0  all_epochs   \n",
       "8           3    0     False          64  0.000625       4   0     min_val   \n",
       "9           3    0     False          64  0.000625       4   0  all_epochs   \n",
       "\n",
       "       Tr_l      Te_l       V_l  Tr_l_t_mse  Te_l_t_mse  V_l_t_mse  Tr_l_ret   \n",
       "0  0.000004  0.000003  0.000003    0.000004    0.000003   0.000003  0.000004  \\\n",
       "1  0.000004  0.000003  0.000003    0.000004    0.000003   0.000003  0.000004   \n",
       "2  0.000009  0.000009  0.000009    0.000009    0.000009   0.000009  0.000009   \n",
       "3  0.000009  0.000009  0.000009    0.000009    0.000009   0.000009  0.000009   \n",
       "4  0.000008  0.000006  0.000006    0.000008    0.000006   0.000006  0.000008   \n",
       "5  0.000008  0.000006  0.000006    0.000008    0.000006   0.000006  0.000008   \n",
       "6  0.000054  0.000054  0.000054    0.000054    0.000054   0.000054  0.000054   \n",
       "7  0.000056  0.000056  0.000056    0.000056    0.000056   0.000056  0.000054   \n",
       "8  0.000052  0.000042  0.000039    0.000052    0.000042   0.000039  0.000052   \n",
       "9  0.000052  0.000042  0.000039    0.000052    0.000042   0.000039  0.000052   \n",
       "\n",
       "   Train_time  Eval_time  \n",
       "0    4.691215   0.069661  \n",
       "1    4.691215   0.081346  \n",
       "2    4.367656   0.079879  \n",
       "3    4.367656   0.085993  \n",
       "4    4.380685   0.073850  \n",
       "5    4.380685   0.088889  \n",
       "6    5.026957   0.057785  \n",
       "7    5.026957   0.084875  \n",
       "8    3.179991   0.084665  \n",
       "9    3.179991   0.089942  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94194d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00220871776342392\n",
      "  batch 101 loss: 0.16216120034456252\n",
      "  batch 201 loss: 0.1183598280698061\n",
      "  batch 301 loss: 0.08425441972911357\n",
      "  batch 401 loss: 0.05820752337574959\n",
      "  batch 501 loss: 0.03904968816787004\n",
      "LOSS train 0.0835415490914181 valid 0.021907396614551544\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00020583266392350198\n",
      "  batch 101 loss: 0.017425798047333955\n",
      "  batch 201 loss: 0.010554323750548066\n",
      "  batch 301 loss: 0.006139044179581106\n",
      "  batch 401 loss: 0.0033966082870028913\n",
      "  batch 501 loss: 0.001813527726335451\n",
      "LOSS train 0.006940791329586151 valid 0.0007492901058867574\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.876293824054301e-06\n",
      "  batch 101 loss: 0.0005462150709354319\n",
      "  batch 201 loss: 0.00027569316422159317\n",
      "  batch 301 loss: 0.00014765006807465397\n",
      "  batch 401 loss: 8.689398763571888e-05\n",
      "  batch 501 loss: 6.443287287368094e-05\n",
      "LOSS train 0.00020099731244344293 valid 5.401807356975041e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.811530849721749e-08\n",
      "  batch 101 loss: 5.455433858060133e-05\n",
      "  batch 201 loss: 5.296258362250228e-05\n",
      "  batch 301 loss: 5.254254743704223e-05\n",
      "  batch 401 loss: 5.150836962457106e-05\n",
      "  batch 501 loss: 5.1737077992584094e-05\n",
      "LOSS train 5.1943729378290486e-05 valid 5.075108492746949e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.1202889911364763e-07\n",
      "  batch 101 loss: 5.255351850792067e-05\n",
      "  batch 201 loss: 5.264064142465941e-05\n",
      "  batch 301 loss: 5.272099095236626e-05\n",
      "  batch 401 loss: 5.192854359847843e-05\n",
      "  batch 501 loss: 5.21048267637525e-05\n",
      "LOSS train 5.177909656321204e-05 valid 5.077867172076367e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.208011210313998e-07\n",
      "  batch 101 loss: 5.2895049011567605e-05\n",
      "  batch 201 loss: 5.300076869389159e-05\n",
      "  batch 301 loss: 5.309731618581281e-05\n",
      "  batch 401 loss: 5.234709327851306e-05\n",
      "  batch 501 loss: 5.2512613619910554e-05\n",
      "LOSS train 5.2164811822939416e-05 valid 5.0812552217394114e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2764234017813578e-07\n",
      "  batch 101 loss: 5.335097021998081e-05\n",
      "  batch 201 loss: 5.3476235016205465e-05\n",
      "  batch 301 loss: 5.35969935299363e-05\n",
      "  batch 401 loss: 5.290316927130334e-05\n",
      "  batch 501 loss: 5.3053210176585705e-05\n",
      "LOSS train 5.2677212399881406e-05 valid 5.087270255899057e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.3664906620979309e-07\n",
      "  batch 101 loss: 5.396236386332021e-05\n",
      "  batch 201 loss: 5.411209191152011e-05\n",
      "  batch 301 loss: 5.426733410786255e-05\n",
      "  batch 401 loss: 5.364694596210029e-05\n",
      "  batch 501 loss: 5.377556730309152e-05\n",
      "LOSS train 5.336391610859231e-05 valid 5.0974966143257916e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.4828503228491175e-07\n",
      "  batch 101 loss: 5.478019304064219e-05\n",
      "  batch 201 loss: 5.496159463291406e-05\n",
      "  batch 301 loss: 5.5164165760288595e-05\n",
      "  batch 401 loss: 5.463604336910066e-05\n",
      "  batch 501 loss: 5.473946947859076e-05\n",
      "LOSS train 5.428044206694279e-05 valid 5.1143364544259384e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.6317864467273465e-07\n",
      "  batch 101 loss: 5.5861956598164396e-05\n",
      "  batch 201 loss: 5.608424118690891e-05\n",
      "  batch 301 loss: 5.6346601668337825e-05\n",
      "  batch 401 loss: 5.592833083937876e-05\n",
      "  batch 501 loss: 5.6004369907896036e-05\n",
      "LOSS train 5.548419173219169e-05 valid 5.1412695029284805e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.821020279952791e-07\n",
      "  batch 101 loss: 5.725649378291564e-05\n",
      "  batch 201 loss: 5.752464186116413e-05\n",
      "  batch 301 loss: 5.7853364260154195e-05\n",
      "  batch 401 loss: 5.755366678386054e-05\n",
      "  batch 501 loss: 5.7593957244534977e-05\n",
      "LOSS train 5.700890743134837e-05 valid 5.1819275540765375e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.053334148717113e-07\n",
      "  batch 101 loss: 5.896270398807246e-05\n",
      "  batch 201 loss: 5.92657490142301e-05\n",
      "  batch 301 loss: 5.96514393146208e-05\n",
      "  batch 401 loss: 5.94568910128146e-05\n",
      "  batch 501 loss: 5.943518297499395e-05\n",
      "LOSS train 5.881227343472187e-05 valid 5.236922515905462e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 3.3163727493956683e-07\n",
      "  batch 101 loss: 6.086515668357606e-05\n",
      "  batch 201 loss: 6.116743819802651e-05\n",
      "  batch 301 loss: 6.157467919365445e-05\n",
      "  batch 401 loss: 6.144086119093117e-05\n",
      "  batch 501 loss: 6.131383835963788e-05\n",
      "LOSS train 6.0719989968316306e-05 valid 5.299939584801905e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 3.577905590645969e-07\n",
      "  batch 101 loss: 6.271059531172796e-05\n",
      "  batch 201 loss: 6.29618408947863e-05\n",
      "  batch 301 loss: 6.333904713756055e-05\n",
      "  batch 401 loss: 6.320603598396702e-05\n",
      "  batch 501 loss: 6.293956968875136e-05\n",
      "LOSS train 6.24520392533208e-05 valid 5.358645648811944e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 3.7977282772772015e-07\n",
      "  batch 101 loss: 6.421880390917068e-05\n",
      "  batch 201 loss: 6.438523337237712e-05\n",
      "  batch 301 loss: 6.46973610855639e-05\n",
      "  batch 401 loss: 6.45247519514669e-05\n",
      "  batch 501 loss: 6.412140648535569e-05\n",
      "LOSS train 6.377652269343414e-05 valid 5.403011164162308e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 3.9530237700091675e-07\n",
      "  batch 101 loss: 6.525926164158591e-05\n",
      "  batch 201 loss: 6.534210750032799e-05\n",
      "  batch 301 loss: 6.558778242379049e-05\n",
      "  batch 401 loss: 6.536873424920487e-05\n",
      "  batch 501 loss: 6.486173814664654e-05\n",
      "LOSS train 6.4642422476872e-05 valid 5.43128298886586e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.6441522297536724e-08\n",
      "  batch 101 loss: 8.138549323348343e-05\n",
      "  batch 201 loss: 6.281209279165978e-05\n",
      "  batch 301 loss: 4.282480228710028e-05\n",
      "  batch 401 loss: 6.949773828637262e-05\n",
      "  batch 501 loss: 7.509535906137899e-05\n",
      "LOSS train 6.606333077479341e-05 valid 5.5898686696309596e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.692129004979506e-07\n",
      "  batch 101 loss: 7.039112063011999e-05\n",
      "  batch 201 loss: 7.189974750872352e-05\n",
      "  batch 301 loss: 8.212376491428586e-05\n",
      "  batch 401 loss: 7.326809649384814e-05\n",
      "  batch 501 loss: 6.59462705812075e-05\n",
      "LOSS train 7.208330825005838e-05 valid 5.6018172472249717e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.575110506266355e-07\n",
      "  batch 101 loss: 6.98494387415849e-05\n",
      "  batch 201 loss: 6.931801423434081e-05\n",
      "  batch 301 loss: 6.907657892497809e-05\n",
      "  batch 401 loss: 6.850456848496833e-05\n",
      "  batch 501 loss: 6.746815072801838e-05\n",
      "LOSS train 6.802597554019133e-05 valid 5.535698073799722e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.3788240873254835e-07\n",
      "  batch 101 loss: 6.780754644751141e-05\n",
      "  batch 201 loss: 6.750717529939721e-05\n",
      "  batch 301 loss: 6.74364557107765e-05\n",
      "  batch 401 loss: 6.697000940221187e-05\n",
      "  batch 501 loss: 6.613957205445331e-05\n",
      "LOSS train 6.642061517515885e-05 valid 5.478229286381975e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.200426337774843e-07\n",
      "  batch 101 loss: 6.675886350876681e-05\n",
      "  batch 201 loss: 6.660541561359423e-05\n",
      "  batch 301 loss: 6.666552501883417e-05\n",
      "  batch 401 loss: 6.630807810324768e-05\n",
      "  batch 501 loss: 6.562408495938143e-05\n",
      "LOSS train 6.569042802148158e-05 valid 5.4590269428445026e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 4.13890193158295e-07\n",
      "  batch 101 loss: 6.645001392826089e-05\n",
      "  batch 201 loss: 6.638833968281688e-05\n",
      "  batch 301 loss: 6.65242372906505e-05\n",
      "  batch 401 loss: 6.622823508223519e-05\n",
      "  batch 501 loss: 6.559753473084129e-05\n",
      "LOSS train 6.555762447861931e-05 valid 5.4594660468865186e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 4.1403211071155965e-07\n",
      "  batch 101 loss: 6.648667723311518e-05\n",
      "  batch 201 loss: 6.64423116359103e-05\n",
      "  batch 301 loss: 6.658756076831196e-05\n",
      "  batch 401 loss: 6.629605129091942e-05\n",
      "  batch 501 loss: 6.566030014255375e-05\n",
      "LOSS train 6.561470922743786e-05 valid 5.462032640934922e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.148591324337758e-07\n",
      "  batch 101 loss: 6.654052171597868e-05\n",
      "  batch 201 loss: 6.649023792306253e-05\n",
      "  batch 301 loss: 6.663055174158217e-05\n",
      "  batch 401 loss: 6.633538886489987e-05\n",
      "  batch 501 loss: 6.569358001343062e-05\n",
      "LOSS train 6.565636035835542e-05 valid 5.463324851007201e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.1527498979121446e-07\n",
      "  batch 101 loss: 6.6567163325999e-05\n",
      "  batch 201 loss: 6.651369213614089e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 6.665150314347557e-05\n",
      "  batch 401 loss: 6.635455751165865e-05\n",
      "  batch 501 loss: 6.570979028765578e-05\n",
      "LOSS train 6.567674546578346e-05 valid 5.4639582231175154e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 4.154782072873786e-07\n",
      "  batch 101 loss: 6.658015587163391e-05\n",
      "  batch 201 loss: 6.6525141164675e-05\n",
      "  batch 301 loss: 6.666174095698807e-05\n",
      "  batch 401 loss: 6.636392460677598e-05\n",
      "  batch 501 loss: 6.571772140887333e-05\n",
      "LOSS train 6.568670473254108e-05 valid 5.464264904730953e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.155778879066929e-07\n",
      "  batch 101 loss: 6.658651874204224e-05\n",
      "  batch 201 loss: 6.653074269252101e-05\n",
      "  batch 301 loss: 6.666675071755889e-05\n",
      "  batch 401 loss: 6.636851582698e-05\n",
      "  batch 501 loss: 6.572160506948421e-05\n",
      "LOSS train 6.56915805305065e-05 valid 5.464418063638732e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.156267823418602e-07\n",
      "  batch 101 loss: 6.658963744484936e-05\n",
      "  batch 201 loss: 6.653349406860797e-05\n",
      "  batch 301 loss: 6.666921227406419e-05\n",
      "  batch 401 loss: 6.637077602135833e-05\n",
      "  batch 501 loss: 6.572351514478215e-05\n",
      "LOSS train 6.569397605893311e-05 valid 5.464489368023351e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 4.156507202424109e-07\n",
      "  batch 101 loss: 6.65911649912232e-05\n",
      "  batch 201 loss: 6.653484596427007e-05\n",
      "  batch 301 loss: 6.667042311619297e-05\n",
      "  batch 401 loss: 6.637188154854811e-05\n",
      "  batch 501 loss: 6.572444579887816e-05\n",
      "LOSS train 6.569514992073337e-05 valid 5.464530840981752e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 4.1566265281289814e-07\n",
      "  batch 101 loss: 6.659192077677289e-05\n",
      "  batch 201 loss: 6.653550773080496e-05\n",
      "  batch 301 loss: 6.667102259598323e-05\n",
      "  batch 401 loss: 6.637243714976648e-05\n",
      "  batch 501 loss: 6.572492098257499e-05\n",
      "LOSS train 6.56957345614601e-05 valid 5.4645461204927415e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 4.1566854633856567e-07\n",
      "  batch 101 loss: 6.659229590241012e-05\n",
      "  batch 201 loss: 6.653583855950273e-05\n",
      "  batch 301 loss: 6.667131496215007e-05\n",
      "  batch 401 loss: 6.637269911152543e-05\n",
      "  batch 501 loss: 6.572514402250818e-05\n",
      "LOSS train 6.569601897109804e-05 valid 5.464557034429163e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 4.1567142034182325e-07\n",
      "  batch 101 loss: 6.659247850166139e-05\n",
      "  batch 201 loss: 6.65360073071497e-05\n",
      "  batch 301 loss: 6.667146704330662e-05\n",
      "  batch 401 loss: 6.637284132921195e-05\n",
      "  batch 501 loss: 6.572525763658633e-05\n",
      "LOSS train 6.569616389226929e-05 valid 5.464557762024924e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0015540061891078948\n",
      "  batch 101 loss: 0.00283241987492147\n",
      "  batch 201 loss: 5.118737945849716e-05\n",
      "  batch 301 loss: 1.7965404158530873e-05\n",
      "  batch 401 loss: 1.3972431900697302e-05\n",
      "  batch 501 loss: 2.291816829043114e-06\n",
      "LOSS train 0.0007699120739329689 valid 3.175233359797858e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.225433440296911e-07\n",
      "  batch 101 loss: 3.04310435996058e-05\n",
      "  batch 201 loss: 7.037920711923107e-06\n",
      "  batch 301 loss: 3.519603377526437e-06\n",
      "  batch 401 loss: 4.061187272554889e-06\n",
      "  batch 501 loss: 2.6634798024360862e-06\n",
      "LOSS train 8.527422773727385e-06 valid 3.091480175498873e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.43010576418601e-07\n",
      "  batch 101 loss: 2.6342253672524406e-05\n",
      "  batch 201 loss: 2.9793827991397848e-06\n",
      "  batch 301 loss: 3.523557633542396e-06\n",
      "  batch 401 loss: 2.8988423107989546e-06\n",
      "  batch 501 loss: 2.7642661541449343e-06\n",
      "LOSS train 6.973236158523891e-06 valid 2.4682203729753383e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.424075061455369e-07\n",
      "  batch 101 loss: 3.4013175490485995e-05\n",
      "  batch 201 loss: 3.0484413217379823e-06\n",
      "  batch 301 loss: 2.896149196232045e-06\n",
      "  batch 401 loss: 3.3654591345566586e-06\n",
      "  batch 501 loss: 3.0008626475819257e-06\n",
      "LOSS train 8.382953720690134e-06 valid 3.1054547434905544e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.7302950406447053e-07\n",
      "  batch 101 loss: 4.159798429896e-05\n",
      "  batch 201 loss: 3.9613705513374955e-06\n",
      "  batch 301 loss: 2.6939196541775344e-06\n",
      "  batch 401 loss: 4.272485612659693e-06\n",
      "  batch 501 loss: 3.1403288529929796e-06\n",
      "LOSS train 9.959180842807935e-06 valid 2.552056685090065e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.974116356810555e-07\n",
      "  batch 101 loss: 3.954077238773834e-05\n",
      "  batch 201 loss: 5.137620355810668e-06\n",
      "  batch 301 loss: 4.3659156565922785e-06\n",
      "  batch 401 loss: 4.955362062730728e-06\n",
      "  batch 501 loss: 3.4413604232952366e-06\n",
      "LOSS train 1.0314180204191103e-05 valid 2.01592811208684e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.610533455386758e-07\n",
      "  batch 101 loss: 3.951398959372909e-05\n",
      "  batch 201 loss: 3.410241581605078e-06\n",
      "  batch 301 loss: 2.5345754565364587e-06\n",
      "  batch 401 loss: 4.570774194405658e-06\n",
      "  batch 501 loss: 4.574808718729173e-06\n",
      "LOSS train 9.890537213434062e-06 valid 1.7059313904610462e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.335932943737135e-07\n",
      "  batch 101 loss: 4.192692444007662e-05\n",
      "  batch 201 loss: 3.4189187022093394e-06\n",
      "  batch 301 loss: 2.5338981444633648e-06\n",
      "  batch 401 loss: 4.06424412318529e-06\n",
      "  batch 501 loss: 4.658038793223795e-06\n",
      "LOSS train 1.0271828813840013e-05 valid 1.616805457160808e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 8.770985004957765e-07\n",
      "  batch 101 loss: 4.633077261502194e-05\n",
      "  batch 201 loss: 4.1709693820735086e-06\n",
      "  batch 301 loss: 2.483355900153583e-06\n",
      "  batch 401 loss: 3.5869683756573068e-06\n",
      "  batch 501 loss: 4.848132997778976e-06\n",
      "LOSS train 1.107146006383652e-05 valid 1.5851459465920925e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 8.250029350165277e-07\n",
      "  batch 101 loss: 4.0720870631218984e-05\n",
      "  batch 201 loss: 3.897269614867582e-06\n",
      "  batch 301 loss: 2.628290601762728e-06\n",
      "  batch 401 loss: 3.2380623706274037e-06\n",
      "  batch 501 loss: 4.881997651295933e-06\n",
      "LOSS train 9.995768268979101e-06 valid 1.554523259983398e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.92007049312815e-07\n",
      "  batch 101 loss: 3.96304453323637e-05\n",
      "  batch 201 loss: 3.3094094591490377e-06\n",
      "  batch 301 loss: 2.9570759699026894e-06\n",
      "  batch 401 loss: 3.0415118749260727e-06\n",
      "  batch 501 loss: 4.345293456040622e-06\n",
      "LOSS train 9.602870342975857e-06 valid 1.4588299563911278e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.967330020619556e-07\n",
      "  batch 101 loss: 0.00024029937566524495\n",
      "  batch 201 loss: 2.548346532080359e-05\n",
      "  batch 301 loss: 5.844568814268314e-06\n",
      "  batch 401 loss: 5.322872146322766e-06\n",
      "  batch 501 loss: 2.698988603668795e-06\n",
      "LOSS train 4.847895616056267e-05 valid 1.5530338714597747e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 6.556398875545711e-07\n",
      "  batch 101 loss: 2.4063135365111064e-05\n",
      "  batch 201 loss: 2.234275157633192e-06\n",
      "  batch 301 loss: 3.1451452176156636e-06\n",
      "  batch 401 loss: 3.621661122181763e-06\n",
      "  batch 501 loss: 2.3607611159093267e-06\n",
      "LOSS train 6.424771062447967e-06 valid 1.5747436918900348e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 6.230724102351814e-07\n",
      "  batch 101 loss: 3.4679528991432565e-05\n",
      "  batch 201 loss: 2.1588319231113927e-06\n",
      "  batch 301 loss: 2.7080079139807366e-06\n",
      "  batch 401 loss: 3.6742341752926676e-06\n",
      "  batch 501 loss: 2.5808646427094574e-06\n",
      "LOSS train 8.228896439169757e-06 valid 1.3478171240421943e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 7.121364615159109e-07\n",
      "  batch 101 loss: 2.979538918111757e-05\n",
      "  batch 201 loss: 2.807785666334439e-06\n",
      "  batch 301 loss: 2.76715791613924e-06\n",
      "  batch 401 loss: 3.6393635386389177e-06\n",
      "  batch 501 loss: 3.2332020973058205e-06\n",
      "LOSS train 7.634340732920464e-06 valid 1.2681684893323109e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 7.165557326516136e-07\n",
      "  batch 101 loss: 2.3415028405224802e-05\n",
      "  batch 201 loss: 2.6415410286517726e-06\n",
      "  batch 301 loss: 2.762534386988591e-06\n",
      "  batch 401 loss: 3.351996131044643e-06\n",
      "  batch 501 loss: 2.5742228282865653e-06\n",
      "LOSS train 6.35447255620975e-06 valid 1.4642630048911087e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002648560330271721\n",
      "  batch 101 loss: 0.0004034156311308834\n",
      "  batch 201 loss: 3.196832052822174e-05\n",
      "  batch 301 loss: 2.1348583704821067e-05\n",
      "  batch 401 loss: 8.583994921309568e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 501 loss: 2.6321989221855803e-06\n",
      "LOSS train 0.00012633684148409592 valid 2.454861942169373e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.595113856950775e-08\n",
      "  batch 101 loss: 4.002243027372288e-06\n",
      "  batch 201 loss: 1.423041800308056e-06\n",
      "  batch 301 loss: 1.1373110351087235e-06\n",
      "  batch 401 loss: 1.931201853011544e-06\n",
      "  batch 501 loss: 1.743991708451631e-06\n",
      "LOSS train 2.1840234116734675e-06 valid 2.8916783776367083e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.973846105509438e-08\n",
      "  batch 101 loss: 4.116663947399957e-06\n",
      "  batch 201 loss: 1.3503300545636422e-06\n",
      "  batch 301 loss: 3.3149252671194063e-06\n",
      "  batch 401 loss: 1.360691420160265e-06\n",
      "  batch 501 loss: 1.2521881053828566e-06\n",
      "LOSS train 2.299007544967255e-06 valid 1.7874917830340564e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.714941602898761e-08\n",
      "  batch 101 loss: 4.848460049799996e-06\n",
      "  batch 201 loss: 3.0316521821305287e-06\n",
      "  batch 301 loss: 1.7403401980686796e-06\n",
      "  batch 401 loss: 2.0549777966039075e-06\n",
      "  batch 501 loss: 1.3757053494600768e-06\n",
      "LOSS train 2.464829638360855e-06 valid 2.556132812969736e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.701586476585362e-08\n",
      "  batch 101 loss: 6.426224763060873e-06\n",
      "  batch 201 loss: 1.995711005662315e-06\n",
      "  batch 301 loss: 2.041680877340468e-06\n",
      "  batch 401 loss: 2.816984614071316e-06\n",
      "  batch 501 loss: 2.120934514948658e-06\n",
      "LOSS train 2.77075364234522e-06 valid 1.3853986047251965e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.811307114025112e-08\n",
      "  batch 101 loss: 5.283123647998877e-06\n",
      "  batch 201 loss: 2.4253216083991447e-06\n",
      "  batch 301 loss: 5.145207134091834e-06\n",
      "  batch 401 loss: 3.0428077576516445e-06\n",
      "  batch 501 loss: 4.070014912826991e-06\n",
      "LOSS train 3.703179328846719e-06 valid 1.8092536038238904e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.420879228448029e-08\n",
      "  batch 101 loss: 3.842693952549325e-06\n",
      "  batch 201 loss: 2.049256834766311e-06\n",
      "  batch 301 loss: 2.118517578111323e-06\n",
      "  batch 401 loss: 2.790546661941562e-06\n",
      "  batch 501 loss: 1.8218735110053785e-06\n",
      "LOSS train 2.404373902854966e-06 valid 2.566574721640791e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2037761734973173e-07\n",
      "  batch 101 loss: 4.10945798861917e-06\n",
      "  batch 201 loss: 1.2032581547316567e-06\n",
      "  batch 301 loss: 1.933073099280591e-06\n",
      "  batch 401 loss: 1.4850603929517092e-06\n",
      "  batch 501 loss: 1.4401624083859588e-06\n",
      "LOSS train 2.110616840937569e-06 valid 1.290189970859501e-06\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 5.558424618357094e-08\n",
      "  batch 101 loss: 3.2543911612492595e-06\n",
      "  batch 201 loss: 2.4786521169062325e-06\n",
      "  batch 301 loss: 2.9778297974303314e-06\n",
      "  batch 401 loss: 1.8423113324672612e-06\n",
      "  batch 501 loss: 1.589338677234764e-06\n",
      "LOSS train 2.2995111994149113e-06 valid 1.8012482314588851e-06\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.11372469291382e-08\n",
      "  batch 101 loss: 1.7437201582026774e-06\n",
      "  batch 201 loss: 1.5578248465430987e-06\n",
      "  batch 301 loss: 1.4893368752666446e-06\n",
      "  batch 401 loss: 1.7361436707119536e-06\n",
      "  batch 501 loss: 1.51453698123305e-06\n",
      "LOSS train 1.5707837188161608e-06 valid 1.4669030861114152e-06\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.0699639005906648e-08\n",
      "  batch 101 loss: 2.39438862472241e-06\n",
      "  batch 201 loss: 1.6450056158134884e-06\n",
      "  batch 301 loss: 1.999502696037325e-06\n",
      "  batch 401 loss: 1.8853353770964532e-06\n",
      "  batch 501 loss: 1.707150080392239e-06\n",
      "LOSS train 1.8952090865462105e-06 valid 1.0174694580200594e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.677836280578049e-08\n",
      "  batch 101 loss: 1.2326744813151436e-06\n",
      "  batch 201 loss: 1.003344137870954e-06\n",
      "  batch 301 loss: 9.325857968178752e-07\n",
      "  batch 401 loss: 1.1626868796099644e-06\n",
      "  batch 501 loss: 7.910215017403743e-07\n",
      "LOSS train 9.83155031314342e-07 valid 9.796990525501315e-07\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.3900305475544883e-08\n",
      "  batch 101 loss: 1.1943467038122435e-06\n",
      "  batch 201 loss: 1.0507607605347858e-06\n",
      "  batch 301 loss: 8.599105377271599e-07\n",
      "  batch 401 loss: 1.2782777038466975e-06\n",
      "  batch 501 loss: 9.387768263025009e-07\n",
      "LOSS train 1.0137928737629012e-06 valid 8.52457787914318e-07\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 1.1539858633113908e-08\n",
      "  batch 101 loss: 1.5025863248041559e-06\n",
      "  batch 201 loss: 8.836975439407979e-07\n",
      "  batch 301 loss: 1.1944459777168958e-06\n",
      "  batch 401 loss: 1.089134250022994e-06\n",
      "  batch 501 loss: 1.2438869235609217e-06\n",
      "LOSS train 1.1119906012231538e-06 valid 4.914833766633819e-07\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 1.0563873047431116e-08\n",
      "  batch 101 loss: 5.977340196317016e-07\n",
      "  batch 201 loss: 8.078080945495003e-07\n",
      "  batch 301 loss: 1.3244477447216242e-06\n",
      "  batch 401 loss: 8.794355041175095e-07\n",
      "  batch 501 loss: 1.1511669630692723e-06\n",
      "LOSS train 9.379361579227955e-07 valid 8.209287898353068e-07\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 9.229029842572345e-09\n",
      "  batch 101 loss: 8.12150967846037e-07\n",
      "  batch 201 loss: 1.0881596366374424e-06\n",
      "  batch 301 loss: 1.3055770434533542e-06\n",
      "  batch 401 loss: 9.470729253280296e-07\n",
      "  batch 501 loss: 1.1344880195451878e-06\n",
      "LOSS train 1.01401482689615e-06 valid 5.076984166407783e-07\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0019672051072120666\n",
      "  batch 101 loss: 0.0035839258258795327\n",
      "  batch 201 loss: 4.49724816746766e-06\n",
      "  batch 301 loss: 4.87203961156979e-06\n",
      "  batch 401 loss: 1.0601620298018588e-05\n",
      "  batch 501 loss: 4.524480795566887e-06\n",
      "LOSS train 0.0009599113112842429 valid 1.2051367775711697e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.6523575292667374e-07\n",
      "  batch 101 loss: 1.0219012714003383e-05\n",
      "  batch 201 loss: 2.3599963648734957e-06\n",
      "  batch 301 loss: 2.1135621391010772e-06\n",
      "  batch 401 loss: 5.671072405277755e-06\n",
      "  batch 501 loss: 4.087123268590176e-06\n",
      "LOSS train 4.509261220805292e-06 valid 1.3841709005646408e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.2272855353076013e-07\n",
      "  batch 101 loss: 8.44160504499314e-06\n",
      "  batch 201 loss: 2.400894831282585e-06\n",
      "  batch 301 loss: 3.0221251898865378e-06\n",
      "  batch 401 loss: 6.207920922349785e-06\n",
      "  batch 501 loss: 4.142309607573224e-06\n",
      "LOSS train 4.502117540969818e-06 valid 1.1848530448332895e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.775960405822844e-07\n",
      "  batch 101 loss: 1.5082058694133593e-05\n",
      "  batch 201 loss: 2.554750530094907e-06\n",
      "  batch 301 loss: 3.3129692220512652e-06\n",
      "  batch 401 loss: 8.557986510595584e-06\n",
      "  batch 501 loss: 4.942459669052823e-06\n",
      "LOSS train 6.6240904865538175e-06 valid 1.0629559255903587e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.449486292898655e-07\n",
      "  batch 101 loss: 2.1227980520279745e-05\n",
      "  batch 201 loss: 6.198245953044079e-06\n",
      "  batch 301 loss: 6.162028381737628e-06\n",
      "  batch 401 loss: 8.590358454796387e-06\n",
      "  batch 501 loss: 1.6203625808941523e-05\n",
      "LOSS train 1.2489982361819466e-05 valid 1.844410871854052e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.278386334888637e-06\n",
      "  batch 101 loss: 2.458161208551246e-05\n",
      "  batch 201 loss: 1.4912609088639784e-05\n",
      "  batch 301 loss: 1.1111799262124578e-05\n",
      "  batch 401 loss: 1.3072712476969173e-05\n",
      "  batch 501 loss: 1.051060627219158e-05\n",
      "LOSS train 1.4282690220487668e-05 valid 1.7519447283120826e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0021546768257394e-06\n",
      "  batch 101 loss: 2.7886302885349323e-05\n",
      "  batch 201 loss: 1.2002540766502533e-05\n",
      "  batch 301 loss: 1.5954466426819636e-05\n",
      "  batch 401 loss: 1.2231704328655724e-05\n",
      "  batch 501 loss: 1.444090117217911e-05\n",
      "LOSS train 1.5884213559331293e-05 valid 1.8265523976879194e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.040259812725708e-06\n",
      "  batch 101 loss: 2.0687577024602887e-05\n",
      "  batch 201 loss: 1.1293736223478845e-05\n",
      "  batch 301 loss: 1.6387995121931453e-05\n",
      "  batch 401 loss: 1.6151492104938825e-05\n",
      "  batch 501 loss: 1.3283370959698004e-05\n",
      "LOSS train 1.4460463255224714e-05 valid 2.887823575292714e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 7.90407966633211e-09\n",
      "  batch 101 loss: 9.79354912033159e-06\n",
      "  batch 201 loss: 1.5701054176133768e-05\n",
      "  batch 301 loss: 5.949715072688377e-06\n",
      "  batch 401 loss: 1.5964318477017516e-05\n",
      "  batch 501 loss: 1.0674904876850632e-05\n",
      "LOSS train 1.0768203293534878e-05 valid 2.1644935259246267e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.0495097058083048e-08\n",
      "  batch 101 loss: 1.0035756390891493e-05\n",
      "  batch 201 loss: 1.1899799899879326e-05\n",
      "  batch 301 loss: 5.999845974429263e-06\n",
      "  batch 401 loss: 1.8254704784226305e-05\n",
      "  batch 501 loss: 6.982689874917014e-06\n",
      "LOSS train 9.807321977870395e-06 valid 8.16478222986916e-06\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 6.149878572614398e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 1.10278523427354e-05\n",
      "  batch 201 loss: 5.794633508173774e-06\n",
      "  batch 301 loss: 5.630273479937387e-06\n",
      "  batch 401 loss: 1.1395645855714066e-05\n",
      "  batch 501 loss: 5.643354879509843e-06\n",
      "LOSS train 7.427067833182455e-06 valid 6.883709829708096e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.190307663520798e-08\n",
      "  batch 101 loss: 8.977627167539027e-06\n",
      "  batch 201 loss: 5.449104569095198e-06\n",
      "  batch 301 loss: 4.885684838598081e-06\n",
      "  batch 401 loss: 1.0483772358043098e-05\n",
      "  batch 501 loss: 5.800544033149891e-06\n",
      "LOSS train 6.744180299765468e-06 valid 7.639236173417885e-06\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.3157741705072113e-08\n",
      "  batch 101 loss: 6.7807615019432884e-06\n",
      "  batch 201 loss: 4.09711113462663e-06\n",
      "  batch 301 loss: 4.5651659024770194e-06\n",
      "  batch 401 loss: 7.857106729147744e-06\n",
      "  batch 501 loss: 5.5235532408914875e-06\n",
      "LOSS train 5.51727438254768e-06 valid 2.9986292702233186e-06\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 4.692155016527977e-08\n",
      "  batch 101 loss: 5.813234492961783e-06\n",
      "  batch 201 loss: 3.691241552132851e-06\n",
      "  batch 301 loss: 3.807425546256127e-06\n",
      "  batch 401 loss: 4.831734209744809e-06\n",
      "  batch 501 loss: 3.715324723430058e-06\n",
      "LOSS train 4.2109574368910975e-06 valid 2.554924321884755e-06\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 3.162634129694197e-08\n",
      "  batch 101 loss: 4.724649632521505e-06\n",
      "  batch 201 loss: 3.4257293672013135e-06\n",
      "  batch 301 loss: 3.2871884096152824e-06\n",
      "  batch 401 loss: 4.447401875609102e-06\n",
      "  batch 501 loss: 3.1763684530972114e-06\n",
      "LOSS train 3.68515456410532e-06 valid 3.0806015729467617e-06\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 5.990149247736554e-09\n",
      "  batch 101 loss: 3.272142672017253e-06\n",
      "  batch 201 loss: 3.316970971702915e-06\n",
      "  batch 301 loss: 3.041857664811687e-06\n",
      "  batch 401 loss: 4.957879612277339e-06\n",
      "  batch 501 loss: 3.9214846992763345e-06\n",
      "LOSS train 3.531545419222323e-06 valid 6.517738711409038e-06\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00045495521277189255\n",
      "  batch 101 loss: 0.01858087623069082\n",
      "  batch 201 loss: 3.8617798294353635e-05\n",
      "  batch 301 loss: 1.911413151674424e-05\n",
      "  batch 401 loss: 1.3657978162200379e-05\n",
      "  batch 501 loss: 4.686122356076794e-06\n",
      "LOSS train 0.0032897713446854828 valid 6.726071660523303e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2246665392012802e-07\n",
      "  batch 101 loss: 5.793344370630393e-06\n",
      "  batch 201 loss: 1.6997515703565114e-06\n",
      "  batch 301 loss: 1.8183122358550463e-06\n",
      "  batch 401 loss: 7.217977666158504e-06\n",
      "  batch 501 loss: 2.2470126351947782e-06\n",
      "LOSS train 3.4597943291714534e-06 valid 4.423277005116688e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.54155223350972e-08\n",
      "  batch 101 loss: 5.746389335854474e-06\n",
      "  batch 201 loss: 1.4145326481695974e-06\n",
      "  batch 301 loss: 2.160083039939309e-06\n",
      "  batch 401 loss: 4.640494557577313e-06\n",
      "  batch 501 loss: 2.2117221777762098e-06\n",
      "LOSS train 2.9886719691375346e-06 valid 4.881403128820239e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.0114064682275058e-07\n",
      "  batch 101 loss: 5.753035622717562e-06\n",
      "  batch 201 loss: 1.211677168839742e-06\n",
      "  batch 301 loss: 4.937321009634843e-06\n",
      "  batch 401 loss: 4.006765727098127e-06\n",
      "  batch 501 loss: 2.31554612824425e-06\n",
      "LOSS train 3.4540012216470713e-06 valid 9.536172910884488e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.614193701650947e-07\n",
      "  batch 101 loss: 1.0555380003722802e-05\n",
      "  batch 201 loss: 1.083634174534609e-06\n",
      "  batch 301 loss: 2.997871321213097e-06\n",
      "  batch 401 loss: 2.7614650767304737e-06\n",
      "  batch 501 loss: 2.280467958541976e-06\n",
      "LOSS train 4.017385285668135e-06 valid 1.020179388433462e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.854098031297326e-07\n",
      "  batch 101 loss: 5.728188168916404e-06\n",
      "  batch 201 loss: 1.2359230252911857e-06\n",
      "  batch 301 loss: 3.781039481864923e-06\n",
      "  batch 401 loss: 2.30616162710362e-06\n",
      "  batch 501 loss: 1.8734524923047501e-06\n",
      "LOSS train 3.18189031492422e-06 valid 8.40926259115804e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.299930176581256e-07\n",
      "  batch 101 loss: 5.317493820555796e-06\n",
      "  batch 201 loss: 9.062428893003016e-07\n",
      "  batch 301 loss: 3.9329934126897115e-06\n",
      "  batch 401 loss: 2.498455149577694e-06\n",
      "  batch 501 loss: 1.8239284250398668e-06\n",
      "LOSS train 3.000889631787333e-06 valid 2.4820574253681116e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3120972653268836e-07\n",
      "  batch 101 loss: 3.6428409357824874e-06\n",
      "  batch 201 loss: 1.0603613674220469e-06\n",
      "  batch 301 loss: 3.264165623875215e-06\n",
      "  batch 401 loss: 1.9065064756773608e-06\n",
      "  batch 501 loss: 1.7836272687077326e-06\n",
      "LOSS train 2.3330615964304738e-06 valid 1.9883930235664593e-06\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 9.868430424830876e-08\n",
      "  batch 101 loss: 2.9517143775592556e-06\n",
      "  batch 201 loss: 1.3672055109026359e-06\n",
      "  batch 301 loss: 3.394381101173849e-06\n",
      "  batch 401 loss: 1.2157076022845104e-06\n",
      "  batch 501 loss: 1.7034223850487252e-06\n",
      "LOSS train 2.139859425103533e-06 valid 1.5322718809329672e-06\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.576414661030867e-08\n",
      "  batch 101 loss: 1.7459073016112825e-06\n",
      "  batch 201 loss: 1.5889469423768788e-06\n",
      "  batch 301 loss: 1.9028117036157254e-06\n",
      "  batch 401 loss: 1.1656577730434491e-06\n",
      "  batch 501 loss: 2.6887291775778976e-06\n",
      "LOSS train 1.7362245110076352e-06 valid 8.195225404961093e-07\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.8593258321052416e-08\n",
      "  batch 101 loss: 1.6397973762138917e-06\n",
      "  batch 201 loss: 1.5014980517236153e-06\n",
      "  batch 301 loss: 1.4261256069403316e-06\n",
      "  batch 401 loss: 2.0439899742541456e-06\n",
      "  batch 501 loss: 1.8449192093328293e-06\n",
      "LOSS train 1.600452437046094e-06 valid 1.0391121350039612e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.190767301930464e-08\n",
      "  batch 101 loss: 2.1734044952381735e-06\n",
      "  batch 201 loss: 1.6716627030177732e-06\n",
      "  batch 301 loss: 1.8091989457502678e-06\n",
      "  batch 401 loss: 2.1251368002594973e-06\n",
      "  batch 501 loss: 1.6977282945163098e-06\n",
      "LOSS train 1.916476198804876e-06 valid 1.4283435803008615e-06\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 4.248201548762154e-08\n",
      "  batch 101 loss: 3.3264394625120986e-06\n",
      "  batch 201 loss: 6.658887889159359e-06\n",
      "  batch 301 loss: 1.935207201242406e-06\n",
      "  batch 401 loss: 9.99516998092531e-07\n",
      "  batch 501 loss: 1.5818653831445317e-06\n",
      "LOSS train 2.6198394821191553e-06 valid 1.2541199794213753e-06\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 2.702122174014221e-08\n",
      "  batch 101 loss: 2.219331693993354e-06\n",
      "  batch 201 loss: 1.1397844303218108e-06\n",
      "  batch 301 loss: 1.7191899243584886e-06\n",
      "  batch 401 loss: 2.3958000440416073e-06\n",
      "  batch 501 loss: 2.6686534293673958e-06\n",
      "LOSS train 2.05021206097057e-06 valid 2.4943831249402137e-06\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 7.115986591088586e-08\n",
      "  batch 101 loss: 2.266326793431972e-06\n",
      "  batch 201 loss: 1.4436086521385505e-06\n",
      "  batch 301 loss: 1.952089592833772e-06\n",
      "  batch 401 loss: 2.0293404902105295e-06\n",
      "  batch 501 loss: 2.5651825959016607e-06\n",
      "LOSS train 2.034640711746961e-06 valid 1.4331001239042962e-06\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 4.228866146149812e-08\n",
      "  batch 101 loss: 2.7246106041189934e-06\n",
      "  batch 201 loss: 2.01024268733363e-06\n",
      "  batch 301 loss: 1.9040970106942723e-06\n",
      "  batch 401 loss: 2.594898303129867e-06\n",
      "  batch 501 loss: 4.1827563404694956e-06\n",
      "LOSS train 2.523738806768222e-06 valid 2.6488742150831968e-06\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0017971032857894897\n",
      "  batch 101 loss: 0.00871034726871585\n",
      "  batch 201 loss: 2.3364000830952137e-05\n",
      "  batch 301 loss: 4.23943497764867e-06\n",
      "  batch 401 loss: 7.741983421851729e-06\n",
      "  batch 501 loss: 5.745829211036834e-06\n",
      "LOSS train 0.00181598769229434 valid 2.561793735367246e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8762992112897336e-07\n",
      "  batch 101 loss: 1.8660723699213123e-05\n",
      "  batch 201 loss: 3.941834181659942e-06\n",
      "  batch 301 loss: 3.4303751715469843e-06\n",
      "  batch 401 loss: 4.873154121582957e-06\n",
      "  batch 501 loss: 4.243281209141969e-06\n",
      "LOSS train 6.4267628849636495e-06 valid 1.8228942280984484e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File trained_models/RTS24_AC_12w_split_by_exec_pn/min_val/model_OE_2h_20e_0.000625lr_0dor_0np_False_ro.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOE_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_hidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124me_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mnp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelu_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ro\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m---> 21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multiple_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolder_to_save\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m saved_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mt \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_val\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\OneDrive - KU Leuven\\JointResearch\\05. Models\\03.ANN_CostEstimator_For_LB\\training_methods.py:106\u001b[0m, in \u001b[0;36mtrain_multiple_epochs\u001b[1;34m(nb_epochs, model, training_loader, validation_loader, loss_fn, optimizer, model_name, folder)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(folder \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    105\u001b[0m             min_val_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/min_val/model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder,model_name)\n\u001b[1;32m--> 106\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     epoch_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    113\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/all_epochs/model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder,model_name)\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File trained_models/RTS24_AC_12w_split_by_exec_pn/min_val/model_OE_2h_20e_0.000625lr_0dor_0np_False_ro.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0025/4*4**i for i in range(3)]\n",
    "negative_penalisations = [0,0.00001,0.0001,0.001,0.01]\n",
    "\n",
    "nbs_e = [16,20]#,8,12,16]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [2,3]\n",
    "dors = [0]#,0.05,0.1]#,0.05]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_pn\"\n",
    "for np in negative_penalisations: \n",
    "    loss_fn = NN_classes.create_loss_fn(penalize_negative=np)\n",
    "    #loss_fn = torch.nn.MSELoss()\n",
    "    for relu_out in [False,True]:\n",
    "        for nb_e in nbs_e:\n",
    "            for lr in learning_rates:\n",
    "                for nb_hidden in nbs_hidden: \n",
    "                    for dor in dors:\n",
    "                        m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor,relu_out=relu_out)\n",
    "                        m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro\"\n",
    "                        optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                        train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                        saved_models = dict()\n",
    "\n",
    "                        for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                            path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                            model = m\n",
    "                            m.load_state_dict(torch.load(path))\n",
    "                            m.eval()\n",
    "\n",
    "                            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                            test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "                            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                            train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "                            validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                            validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "\n",
    "                            if mt == \"min_val\": \n",
    "                                min_val = True\n",
    "                            else: \n",
    "                                min_val = False\n",
    "\n",
    "                            r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                              \"Min_val\":min_val,\n",
    "                                              \"Epochs\": nb_e,\n",
    "                                              \"Lr\":lr,\n",
    "                                              \"Dor\": dor,\n",
    "                                              \"Tr_l\":train_loss.item(),\n",
    "                                              \"Te_l\":test_loss.item(),\n",
    "                                              \"V_l\": validation_loss.item(),\n",
    "                                             \"Np\": np,\n",
    "                                             \"Relu_out\": relu_out}\n",
    "                                             ,index = [i]\n",
    "                            )\n",
    "                            i+=1\n",
    "                            results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee5a668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Dor</th>\n",
       "      <th>Relu_out</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Np</th>\n",
       "      <th>Min_val</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "      <th>V_l</th>\n",
       "      <th>Tr_l_t_mse</th>\n",
       "      <th>Te_l_t_mse</th>\n",
       "      <th>V_l_t_mse</th>\n",
       "      <th>Tr_l_ret</th>\n",
       "      <th>Train_time</th>\n",
       "      <th>Eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.472839</td>\n",
       "      <td>0.077540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.472839</td>\n",
       "      <td>0.094477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.685800</td>\n",
       "      <td>0.083754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.685800</td>\n",
       "      <td>0.093732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.437337</td>\n",
       "      <td>0.095970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.437337</td>\n",
       "      <td>0.131914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.491558</td>\n",
       "      <td>0.059747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.491558</td>\n",
       "      <td>0.091356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.274385</td>\n",
       "      <td>0.081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.274385</td>\n",
       "      <td>0.089255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.111628</td>\n",
       "      <td>0.091645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.111628</td>\n",
       "      <td>0.084888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.809986</td>\n",
       "      <td>0.074580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.809986</td>\n",
       "      <td>0.084149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.200448</td>\n",
       "      <td>0.119675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.200448</td>\n",
       "      <td>0.132856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.525011</td>\n",
       "      <td>0.107901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.525011</td>\n",
       "      <td>0.136302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>2.818310</td>\n",
       "      <td>0.077888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>2.818310</td>\n",
       "      <td>0.099340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.562706</td>\n",
       "      <td>0.091024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.562706</td>\n",
       "      <td>0.138826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model_type  Dor  Relu_out  Batch_size        Lr  Epochs  Np     Min_val   \n",
       "0            3    0     False          32  0.000625       4   0     min_val  \\\n",
       "1            3    0     False          32  0.000625       4   0  all_epochs   \n",
       "2            3    0     False          32  0.002500       4   0     min_val   \n",
       "3            3    0     False          32  0.002500       4   0  all_epochs   \n",
       "4            3    0     False          32  0.010000       4   0     min_val   \n",
       "5            3    0     False          32  0.010000       4   0  all_epochs   \n",
       "6            3    0     False          32  0.040000       4   0     min_val   \n",
       "7            3    0     False          32  0.040000       4   0  all_epochs   \n",
       "8            3    0     False          64  0.000625       4   0     min_val   \n",
       "9            3    0     False          64  0.000625       4   0  all_epochs   \n",
       "10           3    0     False          64  0.002500       4   0     min_val   \n",
       "11           3    0     False          64  0.002500       4   0  all_epochs   \n",
       "12           3    0     False          64  0.010000       4   0     min_val   \n",
       "13           3    0     False          64  0.010000       4   0  all_epochs   \n",
       "14           3    0     False          64  0.040000       4   0     min_val   \n",
       "15           3    0     False          64  0.040000       4   0  all_epochs   \n",
       "16           3    0     False         128  0.000625       4   0     min_val   \n",
       "17           3    0     False         128  0.000625       4   0  all_epochs   \n",
       "18           3    0     False         128  0.002500       4   0     min_val   \n",
       "19           3    0     False         128  0.002500       4   0  all_epochs   \n",
       "20           3    0     False         128  0.010000       4   0     min_val   \n",
       "21           3    0     False         128  0.010000       4   0  all_epochs   \n",
       "\n",
       "        Tr_l      Te_l       V_l  Tr_l_t_mse  Te_l_t_mse  V_l_t_mse  Tr_l_ret   \n",
       "0   0.000002  0.000002  0.000002    0.000002    0.000002   0.000002  0.000002  \\\n",
       "1   0.000002  0.000002  0.000002    0.000002    0.000002   0.000002  0.000002   \n",
       "2   0.000003  0.000002  0.000002    0.000003    0.000002   0.000002  0.000003   \n",
       "3   0.000003  0.000002  0.000002    0.000003    0.000002   0.000002  0.000003   \n",
       "4   0.000007  0.000005  0.000004    0.000007    0.000005   0.000004  0.000007   \n",
       "5   0.000007  0.000005  0.000004    0.000007    0.000005   0.000004  0.000007   \n",
       "6   0.000010  0.000005  0.000002    0.000010    0.000005   0.000002  0.000010   \n",
       "7   0.000015  0.000006  0.000003    0.000015    0.000006   0.000003  0.000010   \n",
       "8   0.000041  0.000041  0.000040    0.000041    0.000041   0.000040  0.000040   \n",
       "9   0.000121  0.000116  0.000127    0.000121    0.000116   0.000127  0.000040   \n",
       "10  0.000009  0.000006  0.000006    0.000009    0.000006   0.000006  0.000008   \n",
       "11  0.000009  0.000006  0.000006    0.000009    0.000006   0.000006  0.000008   \n",
       "12  0.000035  0.000032  0.000031    0.000035    0.000032   0.000031  0.000035   \n",
       "13  0.000044  0.000039  0.000037    0.000044    0.000039   0.000037  0.000035   \n",
       "14  0.000014  0.000014  0.000030    0.000014    0.000014   0.000030  0.000014   \n",
       "15  0.000014  0.000014  0.000030    0.000014    0.000014   0.000030  0.000014   \n",
       "16  0.000052  0.000052  0.000054    0.000052    0.000052   0.000054  0.000051   \n",
       "17  0.000151  0.000153  0.000153    0.000151    0.000153   0.000153  0.000051   \n",
       "18  0.000299  0.000315  0.000315    0.000299    0.000315   0.000315  0.000294   \n",
       "19  0.000460  0.000513  0.000508    0.000460    0.000513   0.000508  0.000294   \n",
       "20  0.000057  0.000057  0.000059    0.000057    0.000057   0.000059  0.000056   \n",
       "21  0.000114  0.000117  0.000121    0.000114    0.000117   0.000121  0.000056   \n",
       "\n",
       "    Train_time  Eval_time  \n",
       "0     4.472839   0.077540  \n",
       "1     4.472839   0.094477  \n",
       "2     4.685800   0.083754  \n",
       "3     4.685800   0.093732  \n",
       "4     7.437337   0.095970  \n",
       "5     7.437337   0.131914  \n",
       "6     5.491558   0.059747  \n",
       "7     5.491558   0.091356  \n",
       "8     3.274385   0.081493  \n",
       "9     3.274385   0.089255  \n",
       "10    3.111628   0.091645  \n",
       "11    3.111628   0.084888  \n",
       "12    3.809986   0.074580  \n",
       "13    3.809986   0.084149  \n",
       "14    4.200448   0.119675  \n",
       "15    4.200448   0.132856  \n",
       "16    2.525011   0.107901  \n",
       "17    2.525011   0.136302  \n",
       "18    2.818310   0.077888  \n",
       "19    2.818310   0.099340  \n",
       "20    2.562706   0.091024  \n",
       "21    2.562706   0.138826  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe247f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/All_Exec_split_by_exec_penalize_neg_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0f94c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m (\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mMin_val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (results\u001b[38;5;241m.\u001b[39mRelu_out \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m results[f]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "f = (results.Min_val == True) & (results.Relu_out == False)\n",
    "results[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (results.Epochs == 12)  & (results.Model_type != 0) \n",
    "sns.boxplot(y = \"Te_l\",x=\"Dor\",data = results[f],hue = \"Min_val\")\n",
    "plt.savefig(\"Figures/Split_by_exec/Min_val_effect_Testloss_fDor.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a759efd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f =  (results.Model_type == 0) & (results.Min_val == True)\n",
    "sns.boxplot(y = \"V_l\",x=\"Epochs\",data = results[f],hue = \"Lr\")\n",
    "plt.savefig(\"Figures/Split_by_exec/Lr_effect_Testloss_fEpochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859941d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00034164607524871826\n",
      "  batch 101 loss: 0.0002987160199381833\n",
      "LOSS train 0.00046863495772977727 valid 6.615156962652691e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1388824532332364e-07\n",
      "  batch 101 loss: 4.279915004374857e-06\n",
      "LOSS train 3.882639331988874e-06 valid 4.274984803487314e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.7142380026343745e-08\n",
      "  batch 101 loss: 4.714095463782542e-06\n",
      "LOSS train 4.845918405521013e-06 valid 8.368496310140472e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.067415334749967e-07\n",
      "  batch 101 loss: 9.704118892841507e-06\n",
      "LOSS train 9.098841420518179e-06 valid 1.0499067684577312e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2910639017936775e-07\n",
      "  batch 101 loss: 8.047440166905062e-06\n",
      "LOSS train 7.632603447944408e-06 valid 6.453345577028813e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4348146578413435e-07\n",
      "  batch 101 loss: 6.558642437823892e-06\n",
      "LOSS train 6.467442821458197e-06 valid 4.256539796188008e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.248340236605146e-08\n",
      "  batch 101 loss: 6.1425094669687045e-06\n",
      "LOSS train 6.336900537815176e-06 valid 4.042789441882633e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.169131549744634e-08\n",
      "  batch 101 loss: 5.983339131745424e-06\n",
      "LOSS train 6.172924947311109e-06 valid 4.524460564425681e-06\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.05 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005922958254814148\n",
      "  batch 101 loss: 0.0017523869141587056\n",
      "LOSS train 0.0020242822022996437 valid 0.00013990193838253617\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1807685950770974e-05\n",
      "  batch 101 loss: 0.0007025678611535113\n",
      "LOSS train 0.0006539419920729256 valid 3.089294477831572e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.049980780109763e-06\n",
      "  batch 101 loss: 0.0002934100830316311\n",
      "LOSS train 0.00027420853407387314 valid 8.263343625003472e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.201225142925978e-06\n",
      "  batch 101 loss: 0.0001229988248087466\n",
      "LOSS train 0.00011415099240771502 valid 2.0785542801604606e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.711556175607257e-07\n",
      "  batch 101 loss: 6.961023875192041e-05\n",
      "LOSS train 6.641250860889656e-05 valid 1.2547947335406207e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.643611802952365e-07\n",
      "  batch 101 loss: 4.652859639463713e-05\n",
      "LOSS train 4.482836706912652e-05 valid 1.7496835425845347e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.848715954925865e-07\n",
      "  batch 101 loss: 3.48935174770304e-05\n",
      "LOSS train 3.2981187260257635e-05 valid 9.343787496618461e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.013871875940822e-07\n",
      "  batch 101 loss: 2.6938014907500475e-05\n",
      "LOSS train 2.6516977179836185e-05 valid 1.499108475400135e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003759964555501938\n",
      "  batch 101 loss: 0.0002994551378594679\n",
      "LOSS train 0.0004980125270533485 valid 3.066094723180868e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.233185114164371e-07\n",
      "  batch 101 loss: 2.3100672784437393e-05\n",
      "LOSS train 1.828971353175092e-05 valid 5.12090355186956e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.323178927734262e-08\n",
      "  batch 101 loss: 1.5219171934859333e-05\n",
      "LOSS train 1.2941402010222649e-05 valid 4.169864951109048e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.2489332271507e-08\n",
      "  batch 101 loss: 1.201928494879212e-05\n",
      "LOSS train 1.0501387999067735e-05 valid 5.3815638239029795e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3004389074922073e-07\n",
      "  batch 101 loss: 1.1259929774496413e-05\n",
      "LOSS train 1.0635669727176109e-05 valid 5.129483724886086e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2970699572179001e-07\n",
      "  batch 101 loss: 9.978570278548205e-06\n",
      "LOSS train 1.0972103528247216e-05 valid 2.3275651983567514e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.492273991578258e-08\n",
      "  batch 101 loss: 6.416301563518801e-06\n",
      "LOSS train 7.143410925165556e-06 valid 1.5495916159125045e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.3757895582530184e-08\n",
      "  batch 101 loss: 5.631845216953479e-06\n",
      "LOSS train 6.252941732593145e-06 valid 1.602147676749155e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.05 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.1198659922229126e-06\n",
      "  batch 101 loss: 0.0006333536207603174\n",
      "LOSS train 0.0004843319122805719 valid 7.497398473788053e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.627797559602186e-07\n",
      "  batch 101 loss: 8.850092912325636e-05\n",
      "LOSS train 9.681447429673187e-05 valid 5.2603452786570415e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0451241400442086e-07\n",
      "  batch 101 loss: 0.00013104593766911422\n",
      "LOSS train 0.00013360057275920315 valid 6.870205834275112e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.950898336479441e-07\n",
      "  batch 101 loss: 0.00013878705189654283\n",
      "LOSS train 0.00013746411305362886 valid 7.302850281121209e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.970096678240225e-07\n",
      "  batch 101 loss: 0.00013051096957042318\n",
      "LOSS train 0.00013020883218409603 valid 7.943673699628562e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.125953349401243e-06\n",
      "  batch 101 loss: 0.00012879935582532198\n",
      "LOSS train 0.00012868645641875268 valid 7.725907926214859e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.451952064409852e-07\n",
      "  batch 101 loss: 0.00012520438244791876\n",
      "LOSS train 0.0001248416180423039 valid 7.843563798815012e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.82089768513106e-07\n",
      "  batch 101 loss: 0.0001242020824338397\n",
      "LOSS train 0.0001238926775927112 valid 7.970364822540432e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.001558859497308731\n",
      "  batch 101 loss: 0.005186513712633314\n",
      "LOSS train 0.004936050064179985 valid 8.999645797302946e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.814881973899901e-07\n",
      "  batch 101 loss: 5.0159332658950007e-05\n",
      "LOSS train 4.703553167347594e-05 valid 4.639695180230774e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.430719712516293e-06\n",
      "  batch 101 loss: 2.501594723753442e-05\n",
      "LOSS train 2.19400943598327e-05 valid 2.1612640921375714e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.538089367793873e-07\n",
      "  batch 101 loss: 1.0706216955327363e-05\n",
      "LOSS train 8.868750437754998e-06 valid 1.7195869077113457e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.948681402718648e-07\n",
      "  batch 101 loss: 1.2491017996438813e-05\n",
      "LOSS train 1.0224801394366399e-05 valid 2.0096551452297717e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.271369395311922e-07\n",
      "  batch 101 loss: 9.39893874971176e-06\n",
      "LOSS train 7.933744244875899e-06 valid 1.791460636013653e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.35157193755731e-07\n",
      "  batch 101 loss: 1.2398367850323666e-05\n",
      "LOSS train 1.0327998213813067e-05 valid 2.206591670983471e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.978040230227635e-07\n",
      "  batch 101 loss: 9.679812596345982e-06\n",
      "LOSS train 8.259995970306956e-06 valid 2.086797212541569e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.05 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.350731123238802e-05\n",
      "  batch 101 loss: 7.162675751317238e-05\n",
      "LOSS train 8.467759913230151e-05 valid 0.0002419580123387277\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.040080476552248e-07\n",
      "  batch 101 loss: 8.283613414732826e-05\n",
      "LOSS train 8.413587737068886e-05 valid 0.0002600552688818425\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.028914557537064e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 8.349045927786846e-05\n",
      "LOSS train 8.473249215222182e-05 valid 0.00026572568458504975\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0685541201382875e-06\n",
      "  batch 101 loss: 8.358249860577871e-05\n",
      "LOSS train 8.481747875356945e-05 valid 0.00026821994106285274\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.0860618931474164e-06\n",
      "  batch 101 loss: 8.359412293373224e-05\n",
      "LOSS train 8.482635637699415e-05 valid 0.00026957711088471115\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0956062033073977e-06\n",
      "  batch 101 loss: 8.419897172757374e-05\n",
      "LOSS train 8.527024099659193e-05 valid 0.00027045802562497556\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1018083023373038e-06\n",
      "  batch 101 loss: 8.358560233659773e-05\n",
      "LOSS train 8.481361840040336e-05 valid 0.00027101245359517634\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1057133815484121e-06\n",
      "  batch 101 loss: 8.357661963202645e-05\n",
      "LOSS train 8.480413396891824e-05 valid 0.00027139612939208746\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.2990431170910595e-05\n",
      "  batch 101 loss: 0.007805943408557141\n",
      "LOSS train 0.005747665538861493 valid 4.2484050936764106e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8268736059544606e-07\n",
      "  batch 101 loss: 8.301740681417868e-05\n",
      "LOSS train 8.033088703372295e-05 valid 0.00010289065539836884\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.17837678064825e-07\n",
      "  batch 101 loss: 0.00022230391290520402\n",
      "LOSS train 0.00017162923652939136 valid 7.296604599105194e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.987531716935336e-08\n",
      "  batch 101 loss: 3.4348710817084796e-05\n",
      "LOSS train 3.292066638919195e-05 valid 6.374463555403054e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.022118552413303e-08\n",
      "  batch 101 loss: 2.536394498520167e-05\n",
      "LOSS train 2.3394936250832518e-05 valid 6.77164935041219e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4851939340587704e-07\n",
      "  batch 101 loss: 1.840903212610101e-05\n",
      "LOSS train 1.679027712745572e-05 valid 2.2540629288414493e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.177058948087506e-08\n",
      "  batch 101 loss: 1.056158576574262e-05\n",
      "LOSS train 9.818336009532083e-06 valid 9.886070984066464e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.5841300056490584e-09\n",
      "  batch 101 loss: 5.26026486312503e-06\n",
      "LOSS train 4.806640704896215e-06 valid 6.970905815251172e-06\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.05 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005871903896331787\n",
      "  batch 101 loss: 0.0037287083886621986\n",
      "LOSS train 0.003240747868323896 valid 5.29523313161917e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.856662951875478e-06\n",
      "  batch 101 loss: 0.00016238674721535062\n",
      "LOSS train 0.00014129626918866649 valid 4.4139633246231824e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.177985389716923e-07\n",
      "  batch 101 loss: 7.568057643766225e-05\n",
      "LOSS train 7.288980431652433e-05 valid 5.804567990708165e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.2960932660207616e-08\n",
      "  batch 101 loss: 0.00010113231340483253\n",
      "LOSS train 9.823808144438194e-05 valid 7.378833106486127e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3905653304391308e-08\n",
      "  batch 101 loss: 8.790069935685096e-05\n",
      "LOSS train 8.388547479025343e-05 valid 0.000104082930192817\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0352472600061446e-07\n",
      "  batch 101 loss: 7.341942575294524e-05\n",
      "LOSS train 7.061245153895172e-05 valid 0.0001280449505429715\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.072174356726464e-07\n",
      "  batch 101 loss: 6.473507615510243e-05\n",
      "LOSS train 6.273131256284946e-05 valid 0.00014535507943946868\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9930180971859955e-07\n",
      "  batch 101 loss: 5.992781000713876e-05\n",
      "LOSS train 5.8497523609830284e-05 valid 0.00016045761003624648\n"
     ]
    }
   ],
   "source": [
    "##Old loop\n",
    "\n",
    "learning_rates = [0.0025*4**i for i in range(2)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [8]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [2,3]\n",
    "dors = [0,0.05]#,0.1,0.2,0.4]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_nl\"\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            for dor in dors:\n",
    "                m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor)\n",
    "                m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor\"\n",
    "                optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                saved_models = dict()\n",
    "\n",
    "                for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                    path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                    model = m\n",
    "                    m.load_state_dict(torch.load(path))\n",
    "                    m.eval()\n",
    "\n",
    "                    test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                    test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "                    train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                    train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "                    validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                    validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "\n",
    "                    if mt == \"min_val\": \n",
    "                        min_val = True\n",
    "                    else: \n",
    "                        min_val = False\n",
    "\n",
    "                    r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                      \"Min_val\":min_val,\n",
    "                                      \"Epochs\": nb_e,\n",
    "                                      \"Lr\":lr,\n",
    "                                      \"Dor\": dor,\n",
    "                                      \"Tr_l\":train_loss.item(),\n",
    "                                      \"Te_l\":test_loss.item(),\n",
    "                                      \"V_l\": validation_loss.item()}\n",
    "                                     ,index = [i]\n",
    "                    )\n",
    "                    i+=1\n",
    "                    results = pd.concat([results,r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
