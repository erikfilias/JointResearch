{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb899fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoading\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import NN_classes\n",
    "import training_methods\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "061f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"../Data/RTS24_AC_12w\"\n",
    "all_executions = DataLoading.list_executions(folder=\"../Data/RTS24_AC_12w\",per = period,sc=sc)\n",
    "len(all_executions)\n",
    "executions = all_executions[0:40]\n",
    "te_s = 0.3\n",
    "val_s = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aa0d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_102_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_103_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_105_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_104_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_106_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_124_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_104_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_105_N_110_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_106_N_108_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_106_N_110_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_107_N_108_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_108_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_108_N_110_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_109_N_111_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_109_N_112_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_110_N_111_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_110_N_112_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_111_N_113_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_111_N_114_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_112_N_113_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_112_N_123_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_113_N_123_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_114_N_116_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_116_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_121_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_121_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_115_N_124_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_116_N_117_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_116_N_119_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_117_N_118_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_117_N_122_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_118_N_121_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_118_N_121_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_119_N_120_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_119_N_120_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_120_N_123_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_120_N_123_cac2_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_121_N_122_cac1_2030.csv\n",
      "1227\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6671c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te_by_exec(dfs_in,dfs_out,executions,te_s,val_s,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93163ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out,maxs = DataLoading.concat_and_normalize_split_by_exec(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f72c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "# training_loader = DataLoader(train,batch_size=64)\n",
    "# validation_loader = DataLoader(train,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa7f15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0, False, 32, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.343022574155718e-05 valid 5.100864200358046e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.579512969655115e-06 valid 4.471147349249804e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.463987318292545e-06 valid 3.24423422171094e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.2280907190170575e-06 valid 2.9188311145844636e-06\n",
      "(3, 0, False, 32, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.0753570779795436e-05 valid 5.955575943517033e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.9469225755418637e-06 valid 2.959886842290871e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.3431372537050984e-06 valid 4.230757440382149e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.1101525979517631e-06 valid 2.877977749449201e-06\n",
      "EPOCH 5:\n",
      "LOSS train 7.827647456935243e-07 valid 8.669697422192257e-07\n",
      "EPOCH 6:\n",
      "LOSS train 6.612516073748443e-07 valid 7.014656375758932e-07\n",
      "EPOCH 7:\n",
      "LOSS train 5.977120938280311e-07 valid 7.22109803064086e-07\n",
      "EPOCH 8:\n",
      "LOSS train 5.754610883495555e-07 valid 5.494541710504564e-07\n",
      "(3, 0, False, 32, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00016480054738617894 valid 3.235133362977649e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.954017235269974e-06 valid 5.2800851335632615e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.317366442942437e-06 valid 4.185391844657715e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.3169023581725894e-06 valid 6.62765796732856e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.0584859665026956e-06 valid 6.693226623610826e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.321490493765547e-06 valid 2.9489235657820245e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.8024786847282578e-06 valid 2.2112744773039594e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.648358461588953e-06 valid 2.2708907181367977e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.4869495380991317e-06 valid 2.445701511533116e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.2875799910165715e-06 valid 2.515210098863463e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.2341871993138744e-06 valid 2.0729225980176125e-06\n",
      "EPOCH 12:\n",
      "LOSS train 9.134460627913867e-07 valid 1.2945894241056521e-06\n",
      "(3, 0, False, 32, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0002110144552004288 valid 5.2862210395687725e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.0401759680271103e-06 valid 5.618386694550281e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.926992172330958e-06 valid 6.709030003548833e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.3575791915930744e-06 valid 6.341745574900415e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.874467684644154e-06 valid 5.168664756638464e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.371071289420246e-06 valid 4.677093329519266e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.898893755463338e-06 valid 3.921181360055925e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.671009190854919e-06 valid 3.7253796563163633e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.2265582283550085e-06 valid 3.388563072803663e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.8787458158859795e-06 valid 3.0519022402586415e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.6788198409295974e-06 valid 2.5573156108293915e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.612308226390729e-06 valid 2.2382207589544123e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.5146508418975548e-06 valid 2.6005832296505105e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.3826288613585817e-06 valid 2.516680979169905e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.3193530284529717e-06 valid 2.378186991336406e-06\n",
      "EPOCH 16:\n",
      "LOSS train 1.1131942007862275e-06 valid 2.3764046090946067e-06\n",
      "(3, 0, False, 32, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 7.557257458551159e-05 valid 3.219676500521018e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.602868263214349e-06 valid 4.330276624386897e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.2438175627159787e-06 valid 5.760663498222129e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.8139105526227234e-06 valid 3.1176505217445083e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.7372924293942985e-06 valid 2.7264361506240675e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.7866802302161908e-06 valid 2.7969394977844786e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.142902731244939e-06 valid 2.280662783959997e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.175934189188415e-06 valid 1.7517282913104282e-06\n",
      "EPOCH 9:\n",
      "LOSS train 9.454825102011112e-07 valid 1.1456162383183255e-06\n",
      "EPOCH 10:\n",
      "LOSS train 6.903847662243882e-07 valid 7.824295380487456e-07\n",
      "EPOCH 11:\n",
      "LOSS train 5.825668524251772e-07 valid 5.678357979377324e-07\n",
      "EPOCH 12:\n",
      "LOSS train 5.253838156786363e-07 valid 5.891303089811117e-07\n",
      "EPOCH 13:\n",
      "LOSS train 4.732701463986207e-07 valid 4.7639073841310164e-07\n",
      "EPOCH 14:\n",
      "LOSS train 4.0322088927892464e-07 valid 4.7253561774596164e-07\n",
      "EPOCH 15:\n",
      "LOSS train 3.9149438343887385e-07 valid 4.4110331032243266e-07\n",
      "EPOCH 16:\n",
      "LOSS train 3.5382452669706066e-07 valid 3.8207994634831266e-07\n",
      "EPOCH 17:\n",
      "LOSS train 3.418523773824564e-07 valid 3.3626869822001026e-07\n",
      "EPOCH 18:\n",
      "LOSS train 3.0237894408294625e-07 valid 3.942740249840426e-07\n",
      "EPOCH 19:\n",
      "LOSS train 2.872712451342603e-07 valid 6.116748636486591e-07\n",
      "EPOCH 20:\n",
      "LOSS train 2.408508868437068e-07 valid 6.37393952729326e-07\n",
      "(3, 0, False, 32, 0.0025, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 2.2440974662121294e-05 valid 1.774860720615834e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.153241760004331e-06 valid 1.6789753090051818e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.3595022745283062e-06 valid 3.526394721120596e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.3266199252564832e-06 valid 7.37391019356437e-07\n",
      "(3, 0, False, 32, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00011517524939781224 valid 6.714679329888895e-06\n",
      "EPOCH 2:\n",
      "LOSS train 5.153284738087365e-06 valid 4.097608325537294e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.948260288734175e-06 valid 3.518182211337262e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.139134408730456e-06 valid 2.6324651116738096e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.733825287211901e-06 valid 2.32631282415241e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.169487006203128e-06 valid 2.831801566571812e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.155793601952668e-06 valid 1.0885395340665127e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.841550382414861e-06 valid 1.0892204045376275e-06\n",
      "(3, 0, False, 32, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 8.526535967544526e-05 valid 1.20969743875321e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.4577339755364165e-06 valid 4.733017249236582e-06\n",
      "EPOCH 3:\n",
      "LOSS train 5.516504999879155e-06 valid 4.008093583252048e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.909256645374001e-06 valid 3.352391559019452e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.558124730353133e-06 valid 7.3682535912666935e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.700575052738017e-06 valid 3.214515800209483e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.5063020727103664e-06 valid 2.1846021809324156e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.891417719639464e-06 valid 1.104484226743807e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.318801760737957e-06 valid 1.4111172959019314e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.9027172627504222e-06 valid 1.0690423550840933e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.5402932401239613e-06 valid 5.894071364309639e-07\n",
      "EPOCH 12:\n",
      "LOSS train 1.127240668159142e-06 valid 4.443255363639764e-07\n",
      "(3, 0, False, 32, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 6.363277851374222e-05 valid 5.651286755892215e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.893697852450297e-06 valid 5.766115464211907e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.2809023755244445e-06 valid 2.9333123165997677e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.4487787513755478e-06 valid 1.5439453591170604e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.9557440912836943e-06 valid 2.7412306735641323e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.2579106267442521e-06 valid 2.773371988951112e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.3150779250755325e-06 valid 2.6223924578516744e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.3960948595814787e-06 valid 1.0684855169529328e-06\n",
      "EPOCH 9:\n",
      "LOSS train 8.844343561924344e-07 valid 6.17948842318583e-07\n",
      "EPOCH 10:\n",
      "LOSS train 9.602111519966321e-07 valid 1.0994251624651952e-06\n",
      "EPOCH 11:\n",
      "LOSS train 8.122415348001532e-07 valid 4.845861099056492e-07\n",
      "EPOCH 12:\n",
      "LOSS train 7.728989657866503e-07 valid 5.274399086374615e-07\n",
      "EPOCH 13:\n",
      "LOSS train 6.765500923474125e-07 valid 5.799270752504526e-07\n",
      "EPOCH 14:\n",
      "LOSS train 6.755607139429626e-07 valid 3.2140263783730916e-07\n",
      "EPOCH 15:\n",
      "LOSS train 6.044403921132837e-07 valid 3.5746444382311893e-07\n",
      "EPOCH 16:\n",
      "LOSS train 9.9529972917396e-07 valid 3.5726498026633635e-07\n",
      "(3, 0, False, 32, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 6.559366724006258e-05 valid 2.8549950457090745e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.9677164529409716e-06 valid 5.126426003698725e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.7538077721494033e-06 valid 1.9768640413531102e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.2911014452577472e-06 valid 3.5327941532159457e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.2708312329922624e-06 valid 6.0030247368558776e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.0105588131408343e-06 valid 2.165952537325211e-05\n",
      "EPOCH 7:\n",
      "LOSS train 2.2195978551817862e-06 valid 1.2525906640803441e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.3592309715283343e-06 valid 6.492674629043904e-07\n",
      "EPOCH 9:\n",
      "LOSS train 9.224288848044575e-07 valid 1.7389495496900054e-06\n",
      "EPOCH 10:\n",
      "LOSS train 7.795671892461306e-07 valid 4.5487030320146005e-07\n",
      "EPOCH 11:\n",
      "LOSS train 1.0585173985208079e-06 valid 5.741580366702692e-07\n",
      "EPOCH 12:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.0112993016208509e-06 valid 1.9391286514292005e-06\n",
      "EPOCH 13:\n",
      "LOSS train 5.858647979490847e-07 valid 4.19322361722152e-07\n",
      "EPOCH 14:\n",
      "LOSS train 6.727180024718252e-07 valid 1.0591764976197737e-06\n",
      "EPOCH 15:\n",
      "LOSS train 7.095848711890153e-07 valid 1.1315921710775e-06\n",
      "EPOCH 16:\n",
      "LOSS train 5.953942307356433e-07 valid 3.5500866601978487e-07\n",
      "EPOCH 17:\n",
      "LOSS train 5.182355708773113e-07 valid 3.794723681949108e-07\n",
      "EPOCH 18:\n",
      "LOSS train 5.117221497738544e-07 valid 5.224309802542848e-07\n",
      "EPOCH 19:\n",
      "LOSS train 5.267687077319065e-07 valid 9.0612400072132e-07\n",
      "EPOCH 20:\n",
      "LOSS train 5.223120549884395e-07 valid 4.4989920411353523e-07\n",
      "(3, 0, False, 32, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0016284713670057984 valid 2.1710293367505074e-06\n",
      "EPOCH 2:\n",
      "LOSS train 6.6466621637497075e-06 valid 1.7831711147664464e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.0893577418209094e-06 valid 1.125759695241868e-06\n",
      "EPOCH 4:\n",
      "LOSS train 8.854298546595188e-07 valid 7.124301077965356e-07\n",
      "(3, 0, False, 32, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0009249533318813504 valid 0.00010691444913391024\n",
      "EPOCH 2:\n",
      "LOSS train 3.1125648931249696e-05 valid 5.883371704840101e-05\n",
      "EPOCH 3:\n",
      "LOSS train 2.7506046597945758e-05 valid 6.4111765823327e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.2122899696979815e-05 valid 5.0789407396223396e-05\n",
      "EPOCH 5:\n",
      "LOSS train 3.2922647248352646e-05 valid 5.4669639212079346e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.305973589350193e-05 valid 6.03931475779973e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.341760596076839e-05 valid 6.198500341270119e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.349015547955294e-05 valid 6.228779238881543e-05\n",
      "(3, 0, False, 32, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0009023214630333988 valid 1.7660559024079703e-06\n",
      "EPOCH 2:\n",
      "LOSS train 7.678357387141658e-06 valid 4.507617632043548e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.0271487457464166e-06 valid 2.932619963758043e-06\n",
      "EPOCH 4:\n",
      "LOSS train 2.1133487932171225e-06 valid 8.78850642038742e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.996611628856322e-06 valid 5.17669104738161e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.6613336164659514e-06 valid 3.4023166790575488e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.6336755003516016e-06 valid 4.540454028756358e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.5491788854748425e-06 valid 3.9191677387862e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.512477212757703e-06 valid 3.7708903164457297e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.7465733821943148e-06 valid 1.3909790368415997e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.9322220477801454e-06 valid 1.1503907444421202e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.867350702373013e-06 valid 1.0283386018272722e-06\n",
      "(3, 0, False, 32, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00014362345000640522 valid 1.294991761824349e-06\n",
      "EPOCH 2:\n",
      "LOSS train 6.785345540790175e-07 valid 3.3602805160626303e-07\n",
      "EPOCH 3:\n",
      "LOSS train 1.1253053016795658e-06 valid 1.551370360175497e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.8194308246222155e-06 valid 6.321004093479132e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.423160725593266e-06 valid 2.9321563488338143e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.3843018715997594e-06 valid 1.7397602505297982e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.1034552237361806e-06 valid 1.9185940800525714e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.7129672955831527e-06 valid 1.1684587661875412e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.8724205029353686e-06 valid 1.2796016335414606e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.5654088097596345e-06 valid 5.778017566626659e-07\n",
      "EPOCH 11:\n",
      "LOSS train 1.4749362023711704e-06 valid 1.6673237723807688e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.445813279498465e-06 valid 1.6655785657349043e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.9708728997769644e-06 valid 1.868840740826272e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.385848431642609e-06 valid 2.511363163648639e-06\n",
      "EPOCH 15:\n",
      "LOSS train 2.2819365008775604e-06 valid 5.17714113357215e-07\n",
      "EPOCH 16:\n",
      "LOSS train 1.2581802216650488e-06 valid 3.2341959013137966e-06\n",
      "(3, 0, False, 32, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.000841552477164112 valid 3.176000518578803e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.442212221526e-06 valid 4.124919541936833e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.535366492852841e-06 valid 6.095575372455642e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.5324736437835722e-06 valid 1.5399839412566507e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.896786126767431e-06 valid 2.1224200281722005e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.4338824519492784e-06 valid 1.3186049727664795e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.750734351970561e-06 valid 1.0839036121979007e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.4539305188777066e-06 valid 1.9820658962999005e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.957571315774271e-06 valid 2.2756908037990797e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.246313942912614e-06 valid 2.7686166959028924e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.661766404446541e-06 valid 2.8343317808321444e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.5144476356140685e-06 valid 4.108334451302653e-06\n",
      "EPOCH 13:\n",
      "LOSS train 2.5230410913159106e-06 valid 4.609889401763212e-06\n",
      "EPOCH 14:\n",
      "LOSS train 2.5365115695673674e-06 valid 4.7574831114616245e-06\n",
      "EPOCH 15:\n",
      "LOSS train 2.529404559271947e-06 valid 4.90142065245891e-06\n",
      "EPOCH 16:\n",
      "LOSS train 2.5107019825766445e-06 valid 5.011914254282601e-06\n",
      "EPOCH 17:\n",
      "LOSS train 2.4999359642704885e-06 valid 5.12642327521462e-06\n",
      "EPOCH 18:\n",
      "LOSS train 2.4837648735603248e-06 valid 5.433197657112032e-06\n",
      "EPOCH 19:\n",
      "LOSS train 2.4736829416099188e-06 valid 5.162985871720593e-06\n",
      "EPOCH 20:\n",
      "LOSS train 2.474757642191102e-06 valid 5.562451406149194e-06\n",
      "(3, 0, False, 32, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.2813475261054954 valid 5.1653092668857425e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.925765075394522e-05 valid 9.825929009821266e-05\n",
      "EPOCH 3:\n",
      "LOSS train 4.6518432367687054e-05 valid 0.0001279534917557612\n",
      "EPOCH 4:\n",
      "LOSS train 3.455341252937739e-05 valid 6.256883352762088e-05\n",
      "(3, 0, False, 32, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.3883882844780202 valid 5.662398325512186e-05\n",
      "EPOCH 2:\n",
      "LOSS train 3.600831568823079e-05 valid 0.000103736761957407\n",
      "EPOCH 3:\n",
      "LOSS train 4.477718938295387e-05 valid 0.00011542684660525993\n",
      "EPOCH 4:\n",
      "LOSS train 4.6222360124213645e-05 valid 0.00010148368892259896\n",
      "EPOCH 5:\n",
      "LOSS train 4.381084407354935e-05 valid 9.086779755307361e-05\n",
      "EPOCH 6:\n",
      "LOSS train 3.4825521155445126e-05 valid 7.693022052990273e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.651789291443352e-05 valid 9.477167623117566e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.4828902853164565e-05 valid 9.116797446040437e-05\n",
      "(3, 0, False, 32, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.5790063487482652 valid 5.845155828865245e-05\n",
      "EPOCH 2:\n",
      "LOSS train 3.340913935999085e-05 valid 7.583662954857573e-05\n",
      "EPOCH 3:\n",
      "LOSS train 3.1925312454438364e-05 valid 8.89081129571423e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.6049671789904263e-05 valid 0.00010743712482508272\n",
      "EPOCH 5:\n",
      "LOSS train 3.4196902499892625e-05 valid 7.652490603504702e-05\n",
      "EPOCH 6:\n",
      "LOSS train 2.9832402856455628e-05 valid 6.247403507586569e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.2507464316682934e-05 valid 6.888330972287804e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.259325728308386e-05 valid 0.0001121354871429503\n",
      "EPOCH 9:\n",
      "LOSS train 3.7238275971841386e-05 valid 6.147207750473171e-05\n",
      "EPOCH 10:\n",
      "LOSS train 3.508399955480539e-05 valid 9.495479753240943e-05\n",
      "EPOCH 11:\n",
      "LOSS train 4.092658889914496e-05 valid 0.00010096288315253332\n",
      "EPOCH 12:\n",
      "LOSS train 4.1259714216486956e-05 valid 0.00010103626846102998\n",
      "(3, 0, False, 32, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.6281867945621967 valid 1.0501463293621782e-05\n",
      "EPOCH 2:\n",
      "LOSS train 2.4638902037626502e-05 valid 2.1030156858614646e-05\n",
      "EPOCH 3:\n",
      "LOSS train 3.617046681128953e-05 valid 2.634241354826372e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.8163801951903295e-05 valid 0.00029702141182497144\n",
      "EPOCH 5:\n",
      "LOSS train 2.7881286075134067e-05 valid 2.737930481089279e-05\n",
      "EPOCH 6:\n",
      "LOSS train 1.9376310960467142e-05 valid 2.8024347557220608e-05\n",
      "EPOCH 7:\n",
      "LOSS train 9.330209059687971e-06 valid 1.4780322089791298e-05\n",
      "EPOCH 8:\n",
      "LOSS train 1.779147900903938e-05 valid 1.6965806935331784e-05\n",
      "EPOCH 9:\n",
      "LOSS train 5.777381820775442e-06 valid 6.868831860629143e-06\n",
      "EPOCH 10:\n",
      "LOSS train 5.973555684086803e-06 valid 7.47672356737894e-06\n",
      "EPOCH 11:\n",
      "LOSS train 7.597971726845955e-06 valid 7.405672477034386e-06\n",
      "EPOCH 12:\n",
      "LOSS train 7.273416750103853e-06 valid 1.7595901226741262e-05\n",
      "EPOCH 13:\n",
      "LOSS train 2.1842614577446716e-05 valid 4.551783058559522e-05\n",
      "EPOCH 14:\n",
      "LOSS train 4.5388120414241905e-05 valid 0.00010248872422380373\n",
      "EPOCH 15:\n",
      "LOSS train 4.133595907929042e-05 valid 0.00010105294495588169\n",
      "EPOCH 16:\n",
      "LOSS train 4.1264320993704094e-05 valid 0.00010103757813340053\n",
      "(3, 0, False, 32, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.5949076569575568 valid 5.653010157402605e-05\n",
      "EPOCH 2:\n",
      "LOSS train 6.883516077130901e-05 valid 5.55180995434057e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.263179111532244e-05 valid 0.000125383710837923\n",
      "EPOCH 4:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 4.387801534187259e-05 valid 0.00010025542724179104\n",
      "EPOCH 5:\n",
      "LOSS train 3.12975479635942e-05 valid 5.8699479268398136e-05\n",
      "EPOCH 6:\n",
      "LOSS train 2.7467471202514642e-05 valid 6.523707270389423e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.2092610402530825e-05 valid 6.06557987339329e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.245123584767563e-05 valid 0.0001017989925458096\n",
      "EPOCH 9:\n",
      "LOSS train 3.724078894828599e-05 valid 5.6890065025072545e-05\n",
      "EPOCH 10:\n",
      "LOSS train 3.469169882740525e-05 valid 9.242678788723424e-05\n",
      "EPOCH 11:\n",
      "LOSS train 4.076485147802103e-05 valid 0.00010092533193528652\n",
      "EPOCH 12:\n",
      "LOSS train 4.1257830362536714e-05 valid 0.00010103597742272541\n",
      "EPOCH 13:\n",
      "LOSS train 4.126344758365505e-05 valid 0.0001010372579912655\n",
      "EPOCH 14:\n",
      "LOSS train 4.126350853845411e-05 valid 0.00010103719250764698\n",
      "EPOCH 15:\n",
      "LOSS train 4.126350925380724e-05 valid 0.00010103719978360459\n",
      "EPOCH 16:\n",
      "LOSS train 4.126351298328143e-05 valid 0.0001010373089229688\n",
      "EPOCH 17:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "EPOCH 18:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "EPOCH 19:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "EPOCH 20:\n",
      "LOSS train 4.126351331538213e-05 valid 0.0001010373089229688\n",
      "(3, 0, False, 64, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.63726180992872e-05 valid 2.4893545287341112e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.6539947327776045e-06 valid 1.3606070297100814e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.111579552067072e-06 valid 1.264296997760539e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.343209661882985e-06 valid 1.9185761175322114e-06\n",
      "(3, 0, False, 64, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 5.0634135004343774e-05 valid 9.231183503288776e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.663921999217273e-05 valid 4.365244603832252e-05\n",
      "EPOCH 3:\n",
      "LOSS train 6.735106561695054e-05 valid 4.4715623516822234e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.458793450262308e-05 valid 7.362643373198807e-06\n",
      "EPOCH 5:\n",
      "LOSS train 9.977467599276556e-06 valid 4.883328074356541e-06\n",
      "EPOCH 6:\n",
      "LOSS train 7.434119905250087e-06 valid 5.060354396846378e-06\n",
      "EPOCH 7:\n",
      "LOSS train 7.133879260768379e-06 valid 5.395133030106081e-06\n",
      "EPOCH 8:\n",
      "LOSS train 8.131559785626589e-06 valid 5.72877615923062e-06\n",
      "(3, 0, False, 64, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 5.3293104932819046e-05 valid 2.5097110665228684e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.6406731841537822e-06 valid 3.264932956881239e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.1918334664000503e-06 valid 2.556061645009322e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.3017234896375102e-06 valid 3.1912263693811838e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.6517273276315786e-06 valid 2.0007566945423605e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.4082014003409715e-06 valid 1.4313801557364059e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.9710461765508404e-06 valid 1.525362563370436e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.227419618079327e-06 valid 1.2203337291794014e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.303986830283413e-06 valid 1.1905352721441886e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.192141064483983e-06 valid 1.1669906143652042e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.1526103352229421e-06 valid 1.1138090485474095e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.1072928395728242e-06 valid 1.0581616152194329e-06\n",
      "(3, 0, False, 64, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 6.787631018276902e-05 valid 3.7128629628568888e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.63192481349775e-06 valid 2.7961607429460855e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.3267735018874206e-06 valid 2.6644263471098384e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.8371195688086128e-06 valid 2.215614131273469e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.368665435541785e-06 valid 1.9015262751054252e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.3226737812610447e-06 valid 1.6514595699845813e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.3789208946926451e-06 valid 1.4554682365996996e-06\n",
      "EPOCH 8:\n",
      "LOSS train 9.214788663703169e-07 valid 1.5901251799732563e-06\n",
      "EPOCH 9:\n",
      "LOSS train 9.485892568482724e-07 valid 9.717147122501046e-07\n",
      "EPOCH 10:\n",
      "LOSS train 7.941217597968165e-07 valid 7.251762212945323e-07\n",
      "EPOCH 11:\n",
      "LOSS train 6.723701753420842e-07 valid 5.139471568327281e-07\n",
      "EPOCH 12:\n",
      "LOSS train 4.775674710628433e-07 valid 5.150724291524966e-07\n",
      "EPOCH 13:\n",
      "LOSS train 6.245066595261873e-07 valid 1.274615556212666e-06\n",
      "EPOCH 14:\n",
      "LOSS train 8.615903633247037e-07 valid 5.451252036436927e-07\n",
      "EPOCH 15:\n",
      "LOSS train 4.6644960659710386e-07 valid 1.1531940344866598e-06\n",
      "EPOCH 16:\n",
      "LOSS train 4.2635041431910963e-07 valid 1.499201630394964e-06\n",
      "(3, 0, False, 64, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.0615099190039822e-05 valid 1.740449306453229e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.6229751273230603e-06 valid 1.907940031742328e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.4508487211261672e-06 valid 1.0531615544095985e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.3163682803104466e-06 valid 8.57635257034417e-07\n",
      "EPOCH 5:\n",
      "LOSS train 1.2871627354164958e-06 valid 1.933455223479541e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.1236176296020668e-06 valid 9.318184766016202e-07\n",
      "EPOCH 7:\n",
      "LOSS train 7.660640916018938e-07 valid 7.63437583373161e-07\n",
      "EPOCH 8:\n",
      "LOSS train 8.097927557507648e-07 valid 5.033027719036909e-07\n",
      "EPOCH 9:\n",
      "LOSS train 5.661687089900644e-07 valid 6.168393156258389e-07\n",
      "EPOCH 10:\n",
      "LOSS train 4.5448691424805865e-07 valid 8.509964004588255e-07\n",
      "EPOCH 11:\n",
      "LOSS train 3.2105276681894074e-07 valid 3.7532311125687556e-07\n",
      "EPOCH 12:\n",
      "LOSS train 9.254269111575671e-07 valid 4.3606931399153837e-07\n",
      "EPOCH 13:\n",
      "LOSS train 5.906123972431351e-07 valid 4.6093876449049276e-07\n",
      "EPOCH 14:\n",
      "LOSS train 5.250068536178126e-07 valid 2.307609747731476e-06\n",
      "EPOCH 15:\n",
      "LOSS train 4.3158974020638304e-07 valid 8.444509944638412e-07\n",
      "EPOCH 16:\n",
      "LOSS train 3.880486386532977e-07 valid 1.1265544799243798e-06\n",
      "EPOCH 17:\n",
      "LOSS train 4.28830912054034e-07 valid 5.826825599797303e-07\n",
      "EPOCH 18:\n",
      "LOSS train 2.846165777352669e-07 valid 6.679405828435847e-07\n",
      "EPOCH 19:\n",
      "LOSS train 3.7573937299375483e-07 valid 6.310773983386753e-07\n",
      "EPOCH 20:\n",
      "LOSS train 4.783485506944916e-07 valid 1.1993893167527858e-06\n",
      "(3, 0, False, 64, 0.0025, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00014776873440357346 valid 3.3729133974702563e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.1305603575817893e-06 valid 2.602479526103707e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.1155767348203618e-05 valid 2.0047812085977057e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.0353276443576817e-06 valid 2.3331579086516285e-06\n",
      "(3, 0, False, 64, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00011358366841363541 valid 7.863893551984802e-05\n",
      "EPOCH 2:\n",
      "LOSS train 0.00011608473560430719 valid 7.864421786507592e-05\n",
      "EPOCH 3:\n",
      "LOSS train 0.00011578274495702838 valid 7.863883365644142e-05\n",
      "EPOCH 4:\n",
      "LOSS train 0.00011571118372561693 valid 7.863496284699067e-05\n",
      "EPOCH 5:\n",
      "LOSS train 0.00011568730988397935 valid 7.863253995310515e-05\n",
      "EPOCH 6:\n",
      "LOSS train 0.00011567826487248645 valid 7.863099745009094e-05\n",
      "EPOCH 7:\n",
      "LOSS train 0.00011567458696132437 valid 7.863014616305009e-05\n",
      "EPOCH 8:\n",
      "LOSS train 0.00011567305382133113 valid 7.862965139793232e-05\n",
      "(3, 0, False, 64, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 2.5921543328958022e-05 valid 1.899350877465622e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.4202027480399203e-06 valid 1.5780765352246817e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.420353129853733e-06 valid 1.1067227205785457e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.8309648127647231e-06 valid 1.0325500170438318e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.5630107903320895e-06 valid 2.1920616291026818e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.0813150396287538e-06 valid 7.009188038864522e-07\n",
      "EPOCH 7:\n",
      "LOSS train 2.5113135148357523e-06 valid 1.1562811778276227e-05\n",
      "EPOCH 8:\n",
      "LOSS train 6.704219489773024e-06 valid 8.194452334464586e-07\n",
      "EPOCH 9:\n",
      "LOSS train 1.088051394765874e-06 valid 9.37423578761809e-07\n",
      "EPOCH 10:\n",
      "LOSS train 1.372879730608843e-06 valid 9.126953273153049e-07\n",
      "EPOCH 11:\n",
      "LOSS train 1.3386292870902046e-06 valid 8.616743798484094e-07\n",
      "EPOCH 12:\n",
      "LOSS train 1.1512236303522935e-06 valid 8.534209996469144e-07\n",
      "(3, 0, False, 64, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00026624616921073596 valid 5.301299552229466e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.919661258092523e-06 valid 3.85131443181308e-06\n",
      "EPOCH 3:\n",
      "LOSS train 2.4896721572355427e-06 valid 3.046827032449073e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.7989222004467928e-06 valid 3.358375124662416e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.984677459773214e-06 valid 3.80991355086735e-06\n",
      "EPOCH 6:\n",
      "LOSS train 2.764568575807877e-06 valid 2.9640066259162268e-06\n",
      "EPOCH 7:\n",
      "LOSS train 2.849872583163142e-06 valid 2.4690921236469876e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.4809423017021097e-06 valid 2.0835634586546803e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.4265687707759434e-06 valid 1.944454197655432e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.056158541040062e-06 valid 1.736608851388155e-06\n",
      "EPOCH 11:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.8321167564006322e-06 valid 1.45705996601464e-06\n",
      "EPOCH 12:\n",
      "LOSS train 3.4970244510858692e-06 valid 9.572503358867834e-07\n",
      "EPOCH 13:\n",
      "LOSS train 6.559222609832151e-07 valid 1.1768489684982342e-06\n",
      "EPOCH 14:\n",
      "LOSS train 9.233996632756682e-07 valid 1.1290219390502898e-06\n",
      "EPOCH 15:\n",
      "LOSS train 9.717771548527327e-07 valid 9.465020411880687e-07\n",
      "EPOCH 16:\n",
      "LOSS train 9.032402066617752e-07 valid 8.057550076046027e-07\n",
      "(3, 0, False, 64, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 3.94061797568e-05 valid 4.377108325570589e-06\n",
      "EPOCH 2:\n",
      "LOSS train 4.883761869722205e-06 valid 3.1468489396502264e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.4084838979096254e-06 valid 1.183696326734207e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.748833638470189e-06 valid 1.339555524282332e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.616407233151905e-06 valid 9.438203960598912e-07\n",
      "EPOCH 6:\n",
      "LOSS train 1.5852221824869509e-06 valid 7.676695759073482e-07\n",
      "EPOCH 7:\n",
      "LOSS train 1.3514673219365311e-06 valid 6.503570944005332e-07\n",
      "EPOCH 8:\n",
      "LOSS train 1.2188463934400354e-06 valid 4.88045202473586e-07\n",
      "EPOCH 9:\n",
      "LOSS train 1.0529769818412392e-06 valid 4.5636357981493347e-07\n",
      "EPOCH 10:\n",
      "LOSS train 1.094233613280386e-06 valid 4.908754931420845e-07\n",
      "EPOCH 11:\n",
      "LOSS train 6.589157907308809e-07 valid 1.0324872619094094e-06\n",
      "EPOCH 12:\n",
      "LOSS train 7.294869975476065e-07 valid 7.578929626106401e-07\n",
      "EPOCH 13:\n",
      "LOSS train 7.327709326236667e-07 valid 7.783591513543797e-07\n",
      "EPOCH 14:\n",
      "LOSS train 8.4763072151349e-07 valid 5.480488880493795e-07\n",
      "EPOCH 15:\n",
      "LOSS train 4.970634415845074e-07 valid 8.996742622002785e-07\n",
      "EPOCH 16:\n",
      "LOSS train 6.710020193233143e-07 valid 2.6350019766141486e-07\n",
      "EPOCH 17:\n",
      "LOSS train 3.6045958895044495e-07 valid 3.5713745205612213e-07\n",
      "EPOCH 18:\n",
      "LOSS train 5.565012144483137e-07 valid 3.6091799415771675e-07\n",
      "EPOCH 19:\n",
      "LOSS train 4.1406855001843084e-07 valid 5.926371500208916e-07\n",
      "EPOCH 20:\n",
      "LOSS train 4.2146560354330854e-07 valid 3.078526162880735e-07\n",
      "(3, 0, False, 64, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0015813599977267789 valid 3.114214905508561e-06\n",
      "EPOCH 2:\n",
      "LOSS train 2.250563136358703e-06 valid 3.137748308290611e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.1105881810869214e-06 valid 8.687977242516354e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.2618762810110867e-06 valid 2.7491755645314697e-06\n",
      "(3, 0, False, 64, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0007971067596268448 valid 6.221697276487248e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.220197615743134e-06 valid 4.073330273968168e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.156377286203279e-06 valid 4.044571142003406e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.7064577066483e-06 valid 1.5197629181784578e-05\n",
      "EPOCH 5:\n",
      "LOSS train 5.592256871312659e-06 valid 1.3949745152785908e-05\n",
      "EPOCH 6:\n",
      "LOSS train 5.559592568833148e-06 valid 1.4212377209332772e-05\n",
      "EPOCH 7:\n",
      "LOSS train 5.109698749117073e-06 valid 6.288372333074221e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.5261814173653673e-06 valid 2.156447408196982e-06\n",
      "(3, 0, False, 64, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.000712790272685983 valid 6.402556755347177e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.625167612118134e-05 valid 0.00016111046716105193\n",
      "EPOCH 3:\n",
      "LOSS train 5.7244812804895e-05 valid 0.00021509212092496455\n",
      "EPOCH 4:\n",
      "LOSS train 6.56398979961847e-05 valid 0.00026384650846011937\n",
      "EPOCH 5:\n",
      "LOSS train 7.751419675138931e-05 valid 0.0002704013604670763\n",
      "EPOCH 6:\n",
      "LOSS train 8.246293997975087e-05 valid 0.000261889275861904\n",
      "EPOCH 7:\n",
      "LOSS train 8.381880881561755e-05 valid 0.0002554166130721569\n",
      "EPOCH 8:\n",
      "LOSS train 8.420975675987272e-05 valid 0.0002517675457056612\n",
      "EPOCH 9:\n",
      "LOSS train 8.434165694369816e-05 valid 0.0002498262911103666\n",
      "EPOCH 10:\n",
      "LOSS train 8.439311506656172e-05 valid 0.0002488081227056682\n",
      "EPOCH 11:\n",
      "LOSS train 8.441560140897e-05 valid 0.00024827662855386734\n",
      "EPOCH 12:\n",
      "LOSS train 8.442614770974818e-05 valid 0.00024800002574920654\n",
      "(3, 0, False, 64, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0005497782851765102 valid 7.325565547944279e-06\n",
      "EPOCH 2:\n",
      "LOSS train 3.6129027782040167e-06 valid 4.2043366192956455e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.7926202789207633e-06 valid 4.5239689825393725e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.550599112682382e-06 valid 2.6956049623549916e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.5907179005016742e-06 valid 4.196868303552037e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.122449084839673e-06 valid 4.047798938699998e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.491971676192793e-06 valid 2.4401881546509685e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.5385623538557852e-06 valid 2.0822512851736974e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.2055788083271503e-06 valid 1.015784619085025e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.7165641168611315e-06 valid 3.934984761144733e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.104136307628886e-06 valid 2.270513277835562e-06\n",
      "EPOCH 12:\n",
      "LOSS train 3.2038561234835484e-06 valid 1.101124894375971e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.7501276431655145e-06 valid 3.3522205740155187e-06\n",
      "EPOCH 14:\n",
      "LOSS train 2.020670403130003e-06 valid 1.136793343903264e-06\n",
      "EPOCH 15:\n",
      "LOSS train 2.2229069237241032e-06 valid 1.0888521728702472e-06\n",
      "EPOCH 16:\n",
      "LOSS train 3.739237914973267e-06 valid 3.6164969969831873e-06\n",
      "(3, 0, False, 64, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0022075667936722605 valid 4.450849519344047e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.011326715236156e-05 valid 0.0001789016678230837\n",
      "EPOCH 3:\n",
      "LOSS train 5.125689612235179e-05 valid 0.00017651218513492495\n",
      "EPOCH 4:\n",
      "LOSS train 5.1225030363509306e-05 valid 0.00019172429165337235\n",
      "EPOCH 5:\n",
      "LOSS train 5.239537415491213e-05 valid 0.00017101455887313932\n",
      "EPOCH 6:\n",
      "LOSS train 4.7796918090684397e-05 valid 9.693884203443304e-05\n",
      "EPOCH 7:\n",
      "LOSS train 4.2633420556772895e-05 valid 7.578889926662669e-05\n",
      "EPOCH 8:\n",
      "LOSS train 2.1545619024395655e-05 valid 2.8427059078239836e-05\n",
      "EPOCH 9:\n",
      "LOSS train 1.8607397573415567e-06 valid 1.2335624433035264e-06\n",
      "EPOCH 10:\n",
      "LOSS train 8.279633978417595e-07 valid 8.446456831734395e-07\n",
      "EPOCH 11:\n",
      "LOSS train 6.406221815896894e-07 valid 4.633986634416942e-07\n",
      "EPOCH 12:\n",
      "LOSS train 1.98618839619194e-06 valid 6.153862614155514e-06\n",
      "EPOCH 13:\n",
      "LOSS train 4.5745073955180935e-06 valid 2.5001468202390242e-06\n",
      "EPOCH 14:\n",
      "LOSS train 4.17251881882251e-06 valid 5.3414346439240035e-06\n",
      "EPOCH 15:\n",
      "LOSS train 2.561493763681583e-06 valid 1.894668798740895e-06\n",
      "EPOCH 16:\n",
      "LOSS train 2.0226490055333594e-06 valid 4.274312232155353e-06\n",
      "EPOCH 17:\n",
      "LOSS train 2.3839045706628257e-06 valid 7.147747055569198e-06\n",
      "EPOCH 18:\n",
      "LOSS train 3.0315653149536385e-06 valid 1.2439361853466835e-05\n",
      "EPOCH 19:\n",
      "LOSS train 4.300257752078732e-06 valid 7.518879556300817e-06\n",
      "EPOCH 20:\n",
      "LOSS train 4.168080968189349e-06 valid 6.236129593162332e-06\n",
      "(3, 0, False, 64, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.8820726020080282 valid 1.1863454346894287e-05\n",
      "EPOCH 2:\n",
      "LOSS train 8.141341350073132e-06 valid 1.0874527106352616e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.1676418950875764e-05 valid 1.1888634617207572e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.1403868992556532e-05 valid 1.0603755981719587e-05\n",
      "(3, 0, False, 64, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 1.1828485896561185 valid 5.075928493170068e-05\n",
      "EPOCH 2:\n",
      "LOSS train 2.9692494171745387e-05 valid 7.123230898287147e-05\n",
      "EPOCH 3:\n",
      "LOSS train 3.152016906429166e-05 valid 5.194942423258908e-05\n",
      "EPOCH 4:\n",
      "LOSS train 4.7767085794595615e-05 valid 6.540051981573924e-05\n",
      "EPOCH 5:\n",
      "LOSS train 5.938433661945822e-05 valid 6.0507230955408886e-05\n",
      "EPOCH 6:\n",
      "LOSS train 5.7480639200234604e-05 valid 5.833172326674685e-05\n",
      "EPOCH 7:\n",
      "LOSS train 5.780745077095201e-05 valid 6.26703113084659e-05\n",
      "EPOCH 8:\n",
      "LOSS train 5.471422830879793e-05 valid 5.1179300498915836e-05\n",
      "(3, 0, False, 64, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.4397823000312773 valid 0.00023910231539048254\n",
      "EPOCH 2:\n",
      "LOSS train 7.884887477709985e-05 valid 9.48774231801508e-06\n",
      "EPOCH 3:\n",
      "LOSS train 5.7392804575064974e-05 valid 1.9088442059000954e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.090773119500269e-05 valid 1.0991150702466257e-05\n",
      "EPOCH 5:\n",
      "LOSS train 2.4982867820796516e-05 valid 3.151504279230721e-05\n",
      "EPOCH 6:\n",
      "LOSS train 1.3065318244196723e-05 valid 3.5502011996868532e-06\n",
      "EPOCH 7:\n",
      "LOSS train 6.0704902964510934e-06 valid 3.0365631573658902e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.2236060365068696e-06 valid 1.142800670095312e-06\n",
      "EPOCH 9:\n",
      "LOSS train 3.4444670656251214e-06 valid 1.291935404879041e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.4189791709279301e-05 valid 2.43602858063241e-06\n",
      "EPOCH 11:\n",
      "LOSS train 5.00763724058663e-06 valid 4.152668680035276e-06\n",
      "EPOCH 12:\n",
      "LOSS train 9.260599809750573e-05 valid 5.322320430423133e-05\n",
      "(3, 0, False, 64, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.27368285729218506 valid 2.111556204908993e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.400128010400829e-06 valid 4.570340934151318e-06\n",
      "EPOCH 3:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 3.1738892499524404e-06 valid 4.486588295549154e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.261095090846889e-06 valid 7.155373623390915e-06\n",
      "EPOCH 5:\n",
      "LOSS train 9.587177601708383e-06 valid 3.238800672988873e-06\n",
      "EPOCH 6:\n",
      "LOSS train 7.51598183040605e-06 valid 8.932648597692605e-06\n",
      "EPOCH 7:\n",
      "LOSS train 5.277866778344102e-06 valid 5.619739113171818e-06\n",
      "EPOCH 8:\n",
      "LOSS train 6.246078932598417e-06 valid 2.541916956033674e-06\n",
      "EPOCH 9:\n",
      "LOSS train 5.188593753676971e-05 valid 0.00011138184345327318\n",
      "EPOCH 10:\n",
      "LOSS train 3.961221539628698e-05 valid 6.492729880847037e-05\n",
      "EPOCH 11:\n",
      "LOSS train 3.6760030173359695e-05 valid 6.316732469713315e-05\n",
      "EPOCH 12:\n",
      "LOSS train 3.768250969180603e-05 valid 8.570896898163483e-05\n",
      "EPOCH 13:\n",
      "LOSS train 4.68216529525808e-05 valid 5.18718698003795e-05\n",
      "EPOCH 14:\n",
      "LOSS train 3.8312615510933666e-05 valid 5.6941153161460534e-05\n",
      "EPOCH 15:\n",
      "LOSS train 3.2826636534598936e-05 valid 5.247596345725469e-05\n",
      "EPOCH 16:\n",
      "LOSS train 4.1361974355764975e-05 valid 5.4671116231475025e-05\n",
      "(3, 0, False, 64, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.3973535064762638 valid 1.0078484592668246e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.620609743925386e-06 valid 9.865401807473972e-06\n",
      "EPOCH 3:\n",
      "LOSS train 1.3098733627354858e-05 valid 3.1704890716355294e-05\n",
      "EPOCH 4:\n",
      "LOSS train 3.765900795636055e-05 valid 8.687276931595989e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.115182273199504e-05 valid 1.1262588486715686e-05\n",
      "EPOCH 6:\n",
      "LOSS train 2.4790733966982818e-05 valid 2.397779462626204e-05\n",
      "EPOCH 7:\n",
      "LOSS train 2.4080228367096748e-05 valid 2.941823004221078e-05\n",
      "EPOCH 8:\n",
      "LOSS train 1.4952618660345838e-05 valid 2.7337240680935793e-05\n",
      "EPOCH 9:\n",
      "LOSS train 1.1661758776279968e-05 valid 2.4296323317685165e-05\n",
      "EPOCH 10:\n",
      "LOSS train 1.2156799125671564e-05 valid 1.6742840671213344e-05\n",
      "EPOCH 11:\n",
      "LOSS train 0.0009389872383181513 valid 5.080892515252344e-05\n",
      "EPOCH 12:\n",
      "LOSS train 2.9923588175477315e-05 valid 5.95897035964299e-05\n",
      "EPOCH 13:\n",
      "LOSS train 2.8783082128375053e-05 valid 7.568057480966672e-05\n",
      "EPOCH 14:\n",
      "LOSS train 3.092005447726194e-05 valid 5.927105667069554e-05\n",
      "EPOCH 15:\n",
      "LOSS train 3.496012356501787e-05 valid 5.0756647397065535e-05\n",
      "EPOCH 16:\n",
      "LOSS train 4.3030169176688656e-05 valid 5.2933290135115385e-05\n",
      "EPOCH 17:\n",
      "LOSS train 4.774725272066011e-05 valid 5.568612687056884e-05\n",
      "EPOCH 18:\n",
      "LOSS train 5.0161855394876455e-05 valid 5.732429417548701e-05\n",
      "EPOCH 19:\n",
      "LOSS train 5.115027520732814e-05 valid 5.7860899687511846e-05\n",
      "EPOCH 20:\n",
      "LOSS train 5.0800748284542105e-05 valid 5.5175980378407985e-05\n",
      "(3, 0, False, 128, 0.000625, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0004701027605311613 valid 2.3386059183394536e-05\n",
      "EPOCH 2:\n",
      "LOSS train 6.672588863575052e-05 valid 5.651867832057178e-05\n",
      "EPOCH 3:\n",
      "LOSS train 7.136182759020506e-05 valid 3.568627289496362e-05\n",
      "EPOCH 4:\n",
      "LOSS train 4.560749582092957e-05 valid 7.605661812704057e-05\n",
      "(3, 0, False, 128, 0.000625, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00044523713560992327 valid 0.0001803341438062489\n",
      "EPOCH 2:\n",
      "LOSS train 8.767019139324468e-05 valid 0.00029343570349738\n",
      "EPOCH 3:\n",
      "LOSS train 0.00013268629459709132 valid 0.0002762609510682523\n",
      "EPOCH 4:\n",
      "LOSS train 0.0001885160251959732 valid 3.9411563193425536e-05\n",
      "EPOCH 5:\n",
      "LOSS train 2.028854912959483e-05 valid 4.095084477739874e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.3328661283695133e-06 valid 2.5796152840484865e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.8900689299327819e-06 valid 2.3906077331048436e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.6863644257997493e-06 valid 2.5384124455740675e-06\n",
      "(3, 0, False, 128, 0.000625, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00011680305025410176 valid 5.4672291298629716e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.3399045787186988e-05 valid 1.1806904467448476e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.618521732523307e-06 valid 9.494587516201136e-07\n",
      "EPOCH 4:\n",
      "LOSS train 5.2919475466112905e-06 valid 7.005524480518943e-07\n",
      "EPOCH 5:\n",
      "LOSS train 4.721421318453597e-06 valid 7.016699896666978e-07\n",
      "EPOCH 6:\n",
      "LOSS train 4.103036232696543e-06 valid 8.0416589298693e-07\n",
      "EPOCH 7:\n",
      "LOSS train 3.994852636756259e-06 valid 9.88745682661829e-07\n",
      "EPOCH 8:\n",
      "LOSS train 3.959122855133398e-06 valid 1.525997618045949e-06\n",
      "EPOCH 9:\n",
      "LOSS train 4.157014141019067e-06 valid 1.875275984275504e-06\n",
      "EPOCH 10:\n",
      "LOSS train 4.260349238382307e-06 valid 1.970485300262226e-06\n",
      "EPOCH 11:\n",
      "LOSS train 4.413249421484211e-06 valid 9.473831141804112e-07\n",
      "EPOCH 12:\n",
      "LOSS train 3.785075791160425e-06 valid 2.24166183215857e-06\n",
      "(3, 0, False, 128, 0.000625, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0002950885854268319 valid 9.80220011115307e-06\n",
      "EPOCH 2:\n",
      "LOSS train 1.1539588317409762e-05 valid 1.4787851796427276e-05\n",
      "EPOCH 3:\n",
      "LOSS train 4.036413989652341e-06 valid 1.6153753676917404e-05\n",
      "EPOCH 4:\n",
      "LOSS train 2.5900895340854766e-06 valid 6.1867172007623594e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.487369663041011e-06 valid 4.141756562603405e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.4905761432113575e-06 valid 2.503602217984735e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.5433824379587291e-06 valid 2.0621369003492873e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.2850767655196482e-06 valid 3.0200415039871586e-06\n",
      "EPOCH 9:\n",
      "LOSS train 1.1611252830823052e-06 valid 2.2535480184160406e-06\n",
      "EPOCH 10:\n",
      "LOSS train 9.002611396609965e-07 valid 1.5367189689641236e-06\n",
      "EPOCH 11:\n",
      "LOSS train 1.0300644434000178e-06 valid 1.2845091532653896e-06\n",
      "EPOCH 12:\n",
      "LOSS train 9.948725652629416e-07 valid 1.1598564242376597e-06\n",
      "EPOCH 13:\n",
      "LOSS train 9.957490530326663e-07 valid 1.3540068266593153e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.4666336554114423e-06 valid 1.3926281781095895e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.5544880810565017e-06 valid 1.2446604387150728e-06\n",
      "EPOCH 16:\n",
      "LOSS train 1.315349543795412e-06 valid 1.5806889450686867e-06\n",
      "(3, 0, False, 128, 0.000625, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00035208997375570154 valid 3.7609817809425294e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.213634725322316e-06 valid 4.617041668097954e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.547077147518453e-06 valid 3.966058557125507e-06\n",
      "EPOCH 4:\n",
      "LOSS train 4.100572877235731e-06 valid 4.584956059261458e-06\n",
      "EPOCH 5:\n",
      "LOSS train 4.1763738897067744e-06 valid 7.002560778346378e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.437680052509852e-06 valid 7.522708528995281e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.995982695692339e-06 valid 6.3318116190203e-06\n",
      "EPOCH 8:\n",
      "LOSS train 3.413183057657073e-06 valid 4.848971002502367e-06\n",
      "EPOCH 9:\n",
      "LOSS train 2.8331343454502903e-06 valid 3.6821122648689197e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.8749324031669228e-06 valid 1.7553218185639707e-06\n",
      "EPOCH 11:\n",
      "LOSS train 2.2721891686202425e-06 valid 1.649381829338381e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.7420974716772308e-06 valid 1.28789122300077e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.6564405409284161e-06 valid 1.5439968592545483e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.925006545723667e-06 valid 2.678472355910344e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.6215945620517006e-06 valid 3.4703327855822863e-06\n",
      "EPOCH 16:\n",
      "LOSS train 1.760672879476404e-06 valid 2.0557210973493056e-06\n",
      "EPOCH 17:\n",
      "LOSS train 1.6282588949539532e-06 valid 2.359850668653962e-06\n",
      "EPOCH 18:\n",
      "LOSS train 1.5256162731048334e-06 valid 2.027529944825801e-06\n",
      "EPOCH 19:\n",
      "LOSS train 1.5559068567269505e-06 valid 1.7734417951942305e-06\n",
      "EPOCH 20:\n",
      "LOSS train 1.5776962133918538e-06 valid 1.5514898450419423e-06\n",
      "(3, 0, False, 128, 0.0025, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0003487601070884102 valid 8.053881174419075e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.5499735997023498e-05 valid 7.3328146754647605e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.6024592593884713e-06 valid 2.717901224968955e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.6707730582185323e-06 valid 5.60386843062588e-06\n",
      "(3, 0, False, 128, 0.0025, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.55973145880215e-05 valid 2.70146756520262e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.112492767733821e-06 valid 1.9659196368593257e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.234106276920361e-06 valid 3.3393096146028256e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.5131030636048494e-06 valid 7.3150913522113115e-06\n",
      "EPOCH 5:\n",
      "LOSS train 2.6551369928160355e-06 valid 1.9974115730292397e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.7257010804756096e-06 valid 1.915129587359843e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.8071474405892378e-06 valid 1.939789626703714e-06\n",
      "EPOCH 8:\n",
      "LOSS train 2.641922328710028e-06 valid 1.4287430758486153e-06\n",
      "(3, 0, False, 128, 0.0025, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.00015978669814261173 valid 1.1009160516550764e-05\n",
      "EPOCH 2:\n",
      "LOSS train 8.299961964314701e-06 valid 8.05638683232246e-06\n",
      "EPOCH 3:\n",
      "LOSS train 9.725493268476221e-06 valid 4.5844535634387285e-06\n",
      "EPOCH 4:\n",
      "LOSS train 4.763409340407915e-06 valid 4.736980372399557e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.862292992462431e-06 valid 7.352217380685033e-06\n",
      "EPOCH 6:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 6.674442068009102e-06 valid 5.867448635399342e-06\n",
      "EPOCH 7:\n",
      "LOSS train 3.681168827565077e-06 valid 4.118836841371376e-06\n",
      "EPOCH 8:\n",
      "LOSS train 0.00012091580136955832 valid 4.1810406401054934e-05\n",
      "EPOCH 9:\n",
      "LOSS train 0.00015670313584053997 valid 0.00010840457980521023\n",
      "EPOCH 10:\n",
      "LOSS train 0.00012834765088046659 valid 9.090659295907244e-05\n",
      "EPOCH 11:\n",
      "LOSS train 0.00011519328974883916 valid 8.142164733726531e-05\n",
      "EPOCH 12:\n",
      "LOSS train 0.00010779081398316011 valid 7.769677904434502e-05\n",
      "(3, 0, False, 128, 0.0025, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 5.686699329516225e-05 valid 1.3014342584938277e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.292504706706223e-06 valid 7.629557330801617e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.152757503200212e-06 valid 2.381974809395615e-06\n",
      "EPOCH 4:\n",
      "LOSS train 1.5865097633153352e-06 valid 1.0979896387652843e-06\n",
      "EPOCH 5:\n",
      "LOSS train 6.312836144931751e-07 valid 7.722577493041172e-07\n",
      "EPOCH 6:\n",
      "LOSS train 6.84861966099617e-07 valid 4.3535595750654466e-07\n",
      "EPOCH 7:\n",
      "LOSS train 9.11827082854632e-07 valid 1.6311613535435754e-06\n",
      "EPOCH 8:\n",
      "LOSS train 9.843480909033507e-07 valid 2.711585921133519e-06\n",
      "EPOCH 9:\n",
      "LOSS train 6.13332528999959e-06 valid 1.0634912541718222e-06\n",
      "EPOCH 10:\n",
      "LOSS train 9.993268412030143e-07 valid 8.381172733606945e-07\n",
      "EPOCH 11:\n",
      "LOSS train 9.706517961475094e-07 valid 1.003306579150376e-06\n",
      "EPOCH 12:\n",
      "LOSS train 1.8362586678874157e-05 valid 0.0002769183774944395\n",
      "EPOCH 13:\n",
      "LOSS train 0.00012532941357917121 valid 6.459823634941131e-05\n",
      "EPOCH 14:\n",
      "LOSS train 8.138733182400581e-05 valid 5.929871622356586e-05\n",
      "EPOCH 15:\n",
      "LOSS train 7.569929238565225e-05 valid 5.757399048889056e-05\n",
      "EPOCH 16:\n",
      "LOSS train 7.328147653409194e-05 valid 5.6819528253981844e-05\n",
      "(3, 0, False, 128, 0.0025, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 9.78766435285586e-05 valid 2.6778750907396898e-05\n",
      "EPOCH 2:\n",
      "LOSS train 8.104214868348195e-06 valid 4.567882115225075e-06\n",
      "EPOCH 3:\n",
      "LOSS train 6.509118829357674e-06 valid 6.433826001739362e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.4579215109289896e-06 valid 3.6065082440472906e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.8905240529434617e-06 valid 2.532855432946235e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.5753933097127231e-06 valid 1.1978008842561394e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.6778953957314474e-06 valid 1.0959651035591378e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.4149760438042769e-06 valid 5.804525926578208e-07\n",
      "EPOCH 9:\n",
      "LOSS train 8.453672775102237e-07 valid 1.0907195928666624e-06\n",
      "EPOCH 10:\n",
      "LOSS train 8.528029027086396e-07 valid 6.167630317577277e-07\n",
      "EPOCH 11:\n",
      "LOSS train 9.466608488812958e-07 valid 1.2432246876414865e-06\n",
      "EPOCH 12:\n",
      "LOSS train 5.912800692623176e-07 valid 1.8433468085277127e-06\n",
      "EPOCH 13:\n",
      "LOSS train 1.1011526764992702e-06 valid 1.536699642201711e-06\n",
      "EPOCH 14:\n",
      "LOSS train 4.7051259730705e-06 valid 1.3485413319358486e-06\n",
      "EPOCH 15:\n",
      "LOSS train 1.1637856395443492e-06 valid 1.1541051208041608e-06\n",
      "EPOCH 16:\n",
      "LOSS train 8.738400028285909e-07 valid 1.8060723050439265e-06\n",
      "EPOCH 17:\n",
      "LOSS train 8.925682632342429e-07 valid 9.014196393763996e-07\n",
      "EPOCH 18:\n",
      "LOSS train 1.0212098883604715e-06 valid 5.986711926198041e-07\n",
      "EPOCH 19:\n",
      "LOSS train 5.553856459203356e-07 valid 9.379862717651122e-07\n",
      "EPOCH 20:\n",
      "LOSS train 1.0920287458939606e-06 valid 1.04779644516384e-06\n",
      "(3, 0, False, 128, 0.01, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0004325759631018537 valid 4.010473276139237e-05\n",
      "EPOCH 2:\n",
      "LOSS train 0.0001129263145843233 valid 9.804642468225211e-05\n",
      "EPOCH 3:\n",
      "LOSS train 0.00013723216081472725 valid 9.376632078783587e-05\n",
      "EPOCH 4:\n",
      "LOSS train 0.00010855432669243648 valid 1.8185251974500716e-05\n",
      "(3, 0, False, 128, 0.01, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.006194534634072323 valid 1.921406692417804e-05\n",
      "EPOCH 2:\n",
      "LOSS train 7.739883358568349e-06 valid 1.7843656678451225e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.3533442361819849e-05 valid 4.696259111369727e-06\n",
      "EPOCH 4:\n",
      "LOSS train 9.8413685799727e-06 valid 8.83084794622846e-06\n",
      "EPOCH 5:\n",
      "LOSS train 1.0414097334145344e-05 valid 3.99407463191892e-06\n",
      "EPOCH 6:\n",
      "LOSS train 4.8119648797024275e-06 valid 3.657567276604823e-06\n",
      "EPOCH 7:\n",
      "LOSS train 5.660170165058944e-06 valid 5.035160484112566e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.543362833225128e-06 valid 7.696183274674695e-06\n",
      "(3, 0, False, 128, 0.01, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.0007807630660138556 valid 5.628054714179598e-05\n",
      "EPOCH 2:\n",
      "LOSS train 8.242270877483101e-05 valid 6.821323040639982e-05\n",
      "EPOCH 3:\n",
      "LOSS train 0.00010307174003936314 valid 8.291078120237216e-05\n",
      "EPOCH 4:\n",
      "LOSS train 0.0001236221495004979 valid 9.610267443349585e-05\n",
      "EPOCH 5:\n",
      "LOSS train 0.00013991246817152345 valid 0.00010505583486519754\n",
      "EPOCH 6:\n",
      "LOSS train 0.0001503321707769919 valid 0.00011009151785401627\n",
      "EPOCH 7:\n",
      "LOSS train 0.00015611984314908682 valid 0.00011268244270468131\n",
      "EPOCH 8:\n",
      "LOSS train 0.0001591240827471581 valid 0.00011397965863579884\n",
      "EPOCH 9:\n",
      "LOSS train 0.00016064945038014107 valid 0.00011462915426818654\n",
      "EPOCH 10:\n",
      "LOSS train 0.00016142343693803442 valid 0.00011495706712594256\n",
      "EPOCH 11:\n",
      "LOSS train 0.00016181895759974891 valid 0.00011512445053085685\n",
      "EPOCH 12:\n",
      "LOSS train 0.00016202279675195014 valid 0.00011521041597006842\n",
      "(3, 0, False, 128, 0.01, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.005401960728148257 valid 4.2888630559900776e-05\n",
      "EPOCH 2:\n",
      "LOSS train 2.8429974356265692e-05 valid 5.989387318550143e-06\n",
      "EPOCH 3:\n",
      "LOSS train 3.399631236749924e-06 valid 2.831970277838991e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.0050247956814335e-06 valid 1.657412667555036e-06\n",
      "EPOCH 5:\n",
      "LOSS train 5.334916067241368e-06 valid 1.0574915449979017e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.286775554024987e-06 valid 1.0076769285660703e-06\n",
      "EPOCH 7:\n",
      "LOSS train 8.760936208279459e-06 valid 5.132436399435392e-06\n",
      "EPOCH 8:\n",
      "LOSS train 8.4184884082809e-06 valid 5.431455429061316e-05\n",
      "EPOCH 9:\n",
      "LOSS train 5.091575230421471e-05 valid 0.00013221205153968185\n",
      "EPOCH 10:\n",
      "LOSS train 0.00023541848910432216 valid 0.00013017882884014398\n",
      "EPOCH 11:\n",
      "LOSS train 0.00017630542428803304 valid 0.00011821128282463178\n",
      "EPOCH 12:\n",
      "LOSS train 0.0001634943040006721 valid 0.00011422249372117221\n",
      "EPOCH 13:\n",
      "LOSS train 0.0001591057953984994 valid 0.00011275474389549345\n",
      "EPOCH 14:\n",
      "LOSS train 0.00015800073519399606 valid 0.00011278397141722962\n",
      "EPOCH 15:\n",
      "LOSS train 0.0001586273455510915 valid 0.00011344714584993199\n",
      "EPOCH 16:\n",
      "LOSS train 0.0001598152271317866 valid 0.0001141719549195841\n",
      "(3, 0, False, 128, 0.01, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.010975662585652381 valid 5.8896122936857864e-05\n",
      "EPOCH 2:\n",
      "LOSS train 3.9645109271865276e-05 valid 4.468232509680092e-05\n",
      "EPOCH 3:\n",
      "LOSS train 8.136562908599207e-06 valid 5.807439720229013e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.8063231611361224e-06 valid 2.505453039702843e-06\n",
      "EPOCH 5:\n",
      "LOSS train 3.0217174110825797e-06 valid 2.654428044479573e-06\n",
      "EPOCH 6:\n",
      "LOSS train 3.6708403747931458e-06 valid 4.9508676056575496e-06\n",
      "EPOCH 7:\n",
      "LOSS train 4.615549421502858e-06 valid 6.0647066675301176e-06\n",
      "EPOCH 8:\n",
      "LOSS train 4.841333107701585e-06 valid 5.895496997254668e-06\n",
      "EPOCH 9:\n",
      "LOSS train 4.597389343500966e-06 valid 5.264237188384868e-06\n",
      "EPOCH 10:\n",
      "LOSS train 4.2650852661996505e-06 valid 4.509282462095143e-06\n",
      "EPOCH 11:\n",
      "LOSS train 3.896460405468224e-06 valid 3.6886142424918944e-06\n",
      "EPOCH 12:\n",
      "LOSS train 2.9786931275591532e-06 valid 1.692597152214148e-06\n",
      "EPOCH 13:\n",
      "LOSS train 2.7344595313650776e-06 valid 3.4127056096622255e-06\n",
      "EPOCH 14:\n",
      "LOSS train 1.0862792327717744e-06 valid 1.9193319076293847e-06\n",
      "EPOCH 15:\n",
      "LOSS train 9.75013745434196e-07 valid 1.0248178341498715e-06\n",
      "EPOCH 16:\n",
      "LOSS train 6.821363977924853e-07 valid 6.143411610537441e-07\n",
      "EPOCH 17:\n",
      "LOSS train 6.823952062631307e-07 valid 7.664320946787484e-07\n",
      "EPOCH 18:\n",
      "LOSS train 6.067970228736047e-07 valid 1.1328867230986361e-06\n",
      "EPOCH 19:\n",
      "LOSS train 6.521987664006209e-07 valid 1.783045490810764e-06\n",
      "EPOCH 20:\n",
      "LOSS train 1.174140432120234e-06 valid 5.871393113920931e-07\n",
      "(3, 0, False, 128, 0.04, 4, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 4.918254962723162 valid 0.0004991907626390457\n",
      "EPOCH 2:\n",
      "LOSS train 0.00010820563184675611 valid 2.467608283041045e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.8083540291972647e-05 valid 1.2145772416261025e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.6478780962510383e-05 valid 1.1645222002698574e-05\n",
      "(3, 0, False, 128, 0.04, 8, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.4815208957002251 valid 2.8103198928874917e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.013411571086065e-05 valid 1.0654973266355228e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.7273687857444895e-05 valid 2.6298754164599814e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.5785336074176814e-05 valid 2.8213735276949592e-05\n",
      "EPOCH 5:\n",
      "LOSS train 1.7833590057942685e-05 valid 1.7021347957779653e-05\n",
      "EPOCH 6:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.8913371920826832e-05 valid 1.816130316001363e-05\n",
      "EPOCH 7:\n",
      "LOSS train 2.1512891215234268e-05 valid 1.549678017909173e-05\n",
      "EPOCH 8:\n",
      "LOSS train 2.2177423618460746e-05 valid 1.3997176210978068e-05\n",
      "(3, 0, False, 128, 0.04, 12, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.3743421210042767 valid 5.0870767154265195e-05\n",
      "EPOCH 2:\n",
      "LOSS train 5.4254670159053535e-05 valid 5.109783523948863e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.589572409701656e-05 valid 5.141003930475563e-05\n",
      "EPOCH 4:\n",
      "LOSS train 5.7675445251081006e-05 valid 5.1816499762935564e-05\n",
      "EPOCH 5:\n",
      "LOSS train 5.973383290954198e-05 valid 5.236419019638561e-05\n",
      "EPOCH 6:\n",
      "LOSS train 6.220801615481177e-05 valid 5.3180818213149905e-05\n",
      "EPOCH 7:\n",
      "LOSS train 6.527391548727657e-05 valid 5.444404814625159e-05\n",
      "EPOCH 8:\n",
      "LOSS train 6.918253963369628e-05 valid 5.635142224491574e-05\n",
      "EPOCH 9:\n",
      "LOSS train 7.43101275153138e-05 valid 5.923968637944199e-05\n",
      "EPOCH 10:\n",
      "LOSS train 8.124454609085198e-05 valid 6.38415731373243e-05\n",
      "EPOCH 11:\n",
      "LOSS train 9.09515641622645e-05 valid 7.13524641469121e-05\n",
      "EPOCH 12:\n",
      "LOSS train 0.00010509177993511618 valid 8.324903319589794e-05\n",
      "(3, 0, False, 128, 0.04, 16, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 0.932322153854817 valid 8.160831202985719e-05\n",
      "EPOCH 2:\n",
      "LOSS train 3.996644262343514e-05 valid 5.586931001744233e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.854154833121379e-05 valid 3.082278271904215e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.372553360289606e-05 valid 1.0588209988782182e-05\n",
      "EPOCH 5:\n",
      "LOSS train 2.0748509336290008e-05 valid 9.382155440107454e-06\n",
      "EPOCH 6:\n",
      "LOSS train 1.4633418752351673e-05 valid 1.9251780031481758e-05\n",
      "EPOCH 7:\n",
      "LOSS train 3.242046916004851e-05 valid 7.668562830076553e-06\n",
      "EPOCH 8:\n",
      "LOSS train 1.2185086620228162e-05 valid 1.5382729543489404e-05\n",
      "EPOCH 9:\n",
      "LOSS train 2.368257770024737e-05 valid 8.70506300998386e-06\n",
      "EPOCH 10:\n",
      "LOSS train 1.1961437713042028e-05 valid 5.141345354786608e-06\n",
      "EPOCH 11:\n",
      "LOSS train 6.943425772920925e-06 valid 6.113799372542417e-06\n",
      "EPOCH 12:\n",
      "LOSS train 5.157806683331246e-06 valid 2.915406639658613e-06\n",
      "EPOCH 13:\n",
      "LOSS train 6.943722226147153e-06 valid 2.648130475790822e-06\n",
      "EPOCH 14:\n",
      "LOSS train 7.507151137096682e-06 valid 2.7114710974274203e-06\n",
      "EPOCH 15:\n",
      "LOSS train 5.147756516746079e-06 valid 4.094922587682959e-06\n",
      "EPOCH 16:\n",
      "LOSS train 6.6074340686277556e-06 valid 3.223425437681726e-06\n",
      "(3, 0, False, 128, 0.04, 20, 0)\n",
      "EPOCH 1:\n",
      "LOSS train 2.8018455699375884 valid 0.0012594634899869561\n",
      "EPOCH 2:\n",
      "LOSS train 0.00017458575889317044 valid 5.138609776622616e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.2310637219063415e-05 valid 3.7801142752869055e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.1904345194404141e-05 valid 1.4083853784541134e-05\n",
      "EPOCH 5:\n",
      "LOSS train 8.491261893040405e-06 valid 1.4116576494416222e-05\n",
      "EPOCH 6:\n",
      "LOSS train 6.700365988247531e-06 valid 8.84757173480466e-06\n",
      "EPOCH 7:\n",
      "LOSS train 1.1887436440712553e-05 valid 1.3060777746431995e-05\n",
      "EPOCH 8:\n",
      "LOSS train 3.3819918716558764e-05 valid 1.1354130947438534e-05\n",
      "EPOCH 9:\n",
      "LOSS train 2.8420951551750703e-05 valid 6.792762633267557e-06\n",
      "EPOCH 10:\n",
      "LOSS train 2.1896942117178333e-05 valid 2.2518208425026387e-05\n",
      "EPOCH 11:\n",
      "LOSS train 2.4479349393788364e-05 valid 1.4678493243991397e-05\n",
      "EPOCH 12:\n",
      "LOSS train 1.9126594181995215e-05 valid 1.52524789882591e-05\n",
      "EPOCH 13:\n",
      "LOSS train 2.2401889706929432e-05 valid 1.0433674106025137e-05\n",
      "EPOCH 14:\n",
      "LOSS train 2.0441093576521582e-05 valid 8.31745273899287e-06\n",
      "EPOCH 15:\n",
      "LOSS train 2.0820372909349498e-05 valid 1.1240226740483195e-05\n",
      "EPOCH 16:\n",
      "LOSS train 2.5297131130024664e-05 valid 4.9966078222496435e-05\n",
      "EPOCH 17:\n",
      "LOSS train 2.1109865041962714e-05 valid 7.359315077337669e-06\n",
      "EPOCH 18:\n",
      "LOSS train 1.2250036720588406e-05 valid 2.0207458874210715e-05\n",
      "EPOCH 19:\n",
      "LOSS train 1.3737868665126196e-05 valid 1.0278527952323202e-05\n",
      "EPOCH 20:\n",
      "LOSS train 2.0444760809374213e-05 valid 0.00010348356590839103\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "nbs_hidden = [3]\n",
    "dors = [0]#,0.05,0.1]#,0.05]\n",
    "relu_outs=[False]\n",
    "\n",
    "batch_sizes = [32,64,128]\n",
    "learning_rates = [0.0025*4**i for i in range(-1,3,1)]\n",
    "nbs_e = [4,8,12,16,20] # ,8]\n",
    "negative_penalisations = [0]\n",
    "\n",
    "\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_test\"\n",
    "\n",
    "hp_sets = ((nb_h,dor,relu_out,bs,lr,nb_e,np) for nb_h in nbs_hidden for dor in dors for relu_out in relu_outs for bs in batch_sizes for lr in learning_rates for nb_e in nbs_e for np in negative_penalisations)\n",
    "\n",
    "\n",
    "for hp_set in hp_sets:\n",
    "    print(hp_set)\n",
    "    nb_hidden,dor,relu_out,bs,lr,nb_e,np = hp_set[0],hp_set[1],hp_set[2],hp_set[3],hp_set[4],hp_set[5],hp_set[6]\n",
    "    \n",
    "    #Create training and validation loaders based on batch size\n",
    "    training_loader = DataLoader(train,batch_size=bs)\n",
    "    validation_loader = DataLoader(train,batch_size=bs)\n",
    "    \n",
    "    #Initialize loss functions\n",
    "    loss_fn = NN_classes.create_loss_fn(penalize_negative=np)\n",
    "    loss_t_mse = torch.nn.MSELoss()\n",
    "    \n",
    "    #Create model based on hyperparameter set\n",
    "    m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor,relu_out=relu_out)\n",
    "    #Create model name for saving and loading\n",
    "    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro_{bs}bs\"\n",
    "    #Create optimizer based on learning rate \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    #Train the actual model \n",
    "    t_start_train = time.perf_counter()\n",
    "    train_loss_1 = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)[0]\n",
    "    t_stop_train = time.perf_counter()\n",
    "    \n",
    "    #In the following loop, we retreive the models from saved locations and calculate losses \n",
    "    for mt in [\"min_val\",\"all_epochs\"]:\n",
    "        t_start_eval = time.perf_counter()\n",
    "        path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "        \n",
    "        #Retreive model state and set to evaluation mode\n",
    "        m.load_state_dict(torch.load(path))\n",
    "        m.eval()\n",
    "        \n",
    "        #Calculate losses\n",
    "        test_predictions = m(d_ft_in[\"test\"].float())\n",
    "        test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "        test_loss_t_mse = loss_t_mse(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "\n",
    "        train_predictions = m(d_ft_in[\"train\"].float())\n",
    "        train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "        train_loss_t_mse = loss_t_mse(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "\n",
    "        validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "        validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "        validation_loss_t_mse = loss_t_mse(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "        t_stop_eval = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        #Calculate some calculation times \n",
    "        t_train = t_stop_train - t_start_train\n",
    "        t_eval = t_stop_eval - t_start_eval\n",
    "        \n",
    "        #Finally, save all desired values in a dataframe\n",
    "        r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                        \"Dor\": dor,\n",
    "                        \"Relu_out\": relu_out,\n",
    "                        \"Batch_size\": bs,\n",
    "                        \"Lr\":lr,\n",
    "                        \"Epochs\": nb_e,\n",
    "                        \"Np\": np,\n",
    "                        \"Min_val\":mt,\n",
    "                        \"Tr_l\":train_loss.item(),\n",
    "                        \"Te_l\":test_loss.item(),\n",
    "                        \"V_l\": validation_loss.item(),\n",
    "                        \"Tr_l_t_mse\":train_loss_t_mse.item(),\n",
    "                        \"Te_l_t_mse\":test_loss_t_mse.item(),\n",
    "                        \"V_l_t_mse\": validation_loss_t_mse.item(),\n",
    "                        \"Tr_l_ret\": train_loss_1.item(),\n",
    "                        \"Train_time\": t_train,\n",
    "                        \"Eval_time\": t_eval\n",
    "                         }\n",
    "                        ,index = [i])\n",
    "        i+=1\n",
    "        results = pd.concat([results,r])\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d669c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/All_Exec_split_by_exec_penalize_test_bs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1039d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Dor</th>\n",
       "      <th>Relu_out</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Np</th>\n",
       "      <th>Min_val</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "      <th>V_l</th>\n",
       "      <th>Tr_l_t_mse</th>\n",
       "      <th>Te_l_t_mse</th>\n",
       "      <th>V_l_t_mse</th>\n",
       "      <th>Tr_l_ret</th>\n",
       "      <th>Train_time</th>\n",
       "      <th>Eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.302867e-06</td>\n",
       "      <td>42.083591</td>\n",
       "      <td>0.517924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.302867e-06</td>\n",
       "      <td>42.083591</td>\n",
       "      <td>0.642527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.475169e-07</td>\n",
       "      <td>85.445304</td>\n",
       "      <td>0.652487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.475169e-07</td>\n",
       "      <td>85.445304</td>\n",
       "      <td>0.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>5.333926e-07</td>\n",
       "      <td>5.596858e-07</td>\n",
       "      <td>5.029923e-07</td>\n",
       "      <td>5.333926e-07</td>\n",
       "      <td>5.596858e-07</td>\n",
       "      <td>5.029923e-07</td>\n",
       "      <td>5.330584e-07</td>\n",
       "      <td>127.637386</td>\n",
       "      <td>0.707812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>1.214640e-05</td>\n",
       "      <td>5.728190e-06</td>\n",
       "      <td>4.182092e-06</td>\n",
       "      <td>1.214640e-05</td>\n",
       "      <td>5.728190e-06</td>\n",
       "      <td>4.182092e-06</td>\n",
       "      <td>1.209004e-05</td>\n",
       "      <td>46.640115</td>\n",
       "      <td>0.744612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>2.067072e-06</td>\n",
       "      <td>1.423654e-06</td>\n",
       "      <td>1.417735e-06</td>\n",
       "      <td>2.067072e-06</td>\n",
       "      <td>1.423654e-06</td>\n",
       "      <td>1.417735e-06</td>\n",
       "      <td>2.058935e-06</td>\n",
       "      <td>64.312629</td>\n",
       "      <td>0.788203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>7.431158e-05</td>\n",
       "      <td>7.501100e-05</td>\n",
       "      <td>7.485546e-05</td>\n",
       "      <td>7.431158e-05</td>\n",
       "      <td>7.501100e-05</td>\n",
       "      <td>7.485546e-05</td>\n",
       "      <td>2.058935e-06</td>\n",
       "      <td>64.312629</td>\n",
       "      <td>0.832030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>8.329134e-06</td>\n",
       "      <td>4.492334e-06</td>\n",
       "      <td>3.095181e-06</td>\n",
       "      <td>8.329134e-06</td>\n",
       "      <td>4.492334e-06</td>\n",
       "      <td>3.095181e-06</td>\n",
       "      <td>8.276066e-06</td>\n",
       "      <td>97.969096</td>\n",
       "      <td>0.845513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>3.661258e-05</td>\n",
       "      <td>3.276078e-05</td>\n",
       "      <td>3.129241e-05</td>\n",
       "      <td>3.661258e-05</td>\n",
       "      <td>3.276078e-05</td>\n",
       "      <td>3.129241e-05</td>\n",
       "      <td>8.276066e-06</td>\n",
       "      <td>97.969096</td>\n",
       "      <td>0.893772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model_type  Dor  Relu_out  Batch_size        Lr  Epochs  Np     Min_val   \n",
       "0             3    0     False          32  0.000625       4   0     min_val  \\\n",
       "1             3    0     False          32  0.000625       4   0  all_epochs   \n",
       "2             3    0     False          32  0.000625       8   0     min_val   \n",
       "3             3    0     False          32  0.000625       8   0  all_epochs   \n",
       "4             3    0     False          32  0.000625      12   0     min_val   \n",
       "..          ...  ...       ...         ...       ...     ...  ..         ...   \n",
       "155           3    0     False         256  0.040000      12   0  all_epochs   \n",
       "156           3    0     False         256  0.040000      16   0     min_val   \n",
       "157           3    0     False         256  0.040000      16   0  all_epochs   \n",
       "158           3    0     False         256  0.040000      20   0     min_val   \n",
       "159           3    0     False         256  0.040000      20   0  all_epochs   \n",
       "\n",
       "             Tr_l          Te_l           V_l    Tr_l_t_mse    Te_l_t_mse   \n",
       "0    3.303962e-06  2.959594e-06  2.868163e-06  3.303962e-06  2.959594e-06  \\\n",
       "1    3.303962e-06  2.959594e-06  2.868163e-06  3.303962e-06  2.959594e-06   \n",
       "2    2.476374e-07  2.540816e-07  2.362306e-07  2.476374e-07  2.540816e-07   \n",
       "3    2.476374e-07  2.540816e-07  2.362306e-07  2.476374e-07  2.540816e-07   \n",
       "4    5.333926e-07  5.596858e-07  5.029923e-07  5.333926e-07  5.596858e-07   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "155  1.214640e-05  5.728190e-06  4.182092e-06  1.214640e-05  5.728190e-06   \n",
       "156  2.067072e-06  1.423654e-06  1.417735e-06  2.067072e-06  1.423654e-06   \n",
       "157  7.431158e-05  7.501100e-05  7.485546e-05  7.431158e-05  7.501100e-05   \n",
       "158  8.329134e-06  4.492334e-06  3.095181e-06  8.329134e-06  4.492334e-06   \n",
       "159  3.661258e-05  3.276078e-05  3.129241e-05  3.661258e-05  3.276078e-05   \n",
       "\n",
       "        V_l_t_mse      Tr_l_ret  Train_time  Eval_time  \n",
       "0    2.868163e-06  3.302867e-06   42.083591   0.517924  \n",
       "1    2.868163e-06  3.302867e-06   42.083591   0.642527  \n",
       "2    2.362306e-07  2.475169e-07   85.445304   0.652487  \n",
       "3    2.362306e-07  2.475169e-07   85.445304   0.705018  \n",
       "4    5.029923e-07  5.330584e-07  127.637386   0.707812  \n",
       "..            ...           ...         ...        ...  \n",
       "155  4.182092e-06  1.209004e-05   46.640115   0.744612  \n",
       "156  1.417735e-06  2.058935e-06   64.312629   0.788203  \n",
       "157  7.485546e-05  2.058935e-06   64.312629   0.832030  \n",
       "158  3.095181e-06  8.276066e-06   97.969096   0.845513  \n",
       "159  3.129241e-05  8.276066e-06   97.969096   0.893772  \n",
       "\n",
       "[160 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94194d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00220871776342392\n",
      "  batch 101 loss: 0.16216120034456252\n",
      "  batch 201 loss: 0.1183598280698061\n",
      "  batch 301 loss: 0.08425441972911357\n",
      "  batch 401 loss: 0.05820752337574959\n",
      "  batch 501 loss: 0.03904968816787004\n",
      "LOSS train 0.0835415490914181 valid 0.021907396614551544\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00020583266392350198\n",
      "  batch 101 loss: 0.017425798047333955\n",
      "  batch 201 loss: 0.010554323750548066\n",
      "  batch 301 loss: 0.006139044179581106\n",
      "  batch 401 loss: 0.0033966082870028913\n",
      "  batch 501 loss: 0.001813527726335451\n",
      "LOSS train 0.006940791329586151 valid 0.0007492901058867574\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.876293824054301e-06\n",
      "  batch 101 loss: 0.0005462150709354319\n",
      "  batch 201 loss: 0.00027569316422159317\n",
      "  batch 301 loss: 0.00014765006807465397\n",
      "  batch 401 loss: 8.689398763571888e-05\n",
      "  batch 501 loss: 6.443287287368094e-05\n",
      "LOSS train 0.00020099731244344293 valid 5.401807356975041e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.811530849721749e-08\n",
      "  batch 101 loss: 5.455433858060133e-05\n",
      "  batch 201 loss: 5.296258362250228e-05\n",
      "  batch 301 loss: 5.254254743704223e-05\n",
      "  batch 401 loss: 5.150836962457106e-05\n",
      "  batch 501 loss: 5.1737077992584094e-05\n",
      "LOSS train 5.1943729378290486e-05 valid 5.075108492746949e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.1202889911364763e-07\n",
      "  batch 101 loss: 5.255351850792067e-05\n",
      "  batch 201 loss: 5.264064142465941e-05\n",
      "  batch 301 loss: 5.272099095236626e-05\n",
      "  batch 401 loss: 5.192854359847843e-05\n",
      "  batch 501 loss: 5.21048267637525e-05\n",
      "LOSS train 5.177909656321204e-05 valid 5.077867172076367e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.208011210313998e-07\n",
      "  batch 101 loss: 5.2895049011567605e-05\n",
      "  batch 201 loss: 5.300076869389159e-05\n",
      "  batch 301 loss: 5.309731618581281e-05\n",
      "  batch 401 loss: 5.234709327851306e-05\n",
      "  batch 501 loss: 5.2512613619910554e-05\n",
      "LOSS train 5.2164811822939416e-05 valid 5.0812552217394114e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2764234017813578e-07\n",
      "  batch 101 loss: 5.335097021998081e-05\n",
      "  batch 201 loss: 5.3476235016205465e-05\n",
      "  batch 301 loss: 5.35969935299363e-05\n",
      "  batch 401 loss: 5.290316927130334e-05\n",
      "  batch 501 loss: 5.3053210176585705e-05\n",
      "LOSS train 5.2677212399881406e-05 valid 5.087270255899057e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.3664906620979309e-07\n",
      "  batch 101 loss: 5.396236386332021e-05\n",
      "  batch 201 loss: 5.411209191152011e-05\n",
      "  batch 301 loss: 5.426733410786255e-05\n",
      "  batch 401 loss: 5.364694596210029e-05\n",
      "  batch 501 loss: 5.377556730309152e-05\n",
      "LOSS train 5.336391610859231e-05 valid 5.0974966143257916e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.4828503228491175e-07\n",
      "  batch 101 loss: 5.478019304064219e-05\n",
      "  batch 201 loss: 5.496159463291406e-05\n",
      "  batch 301 loss: 5.5164165760288595e-05\n",
      "  batch 401 loss: 5.463604336910066e-05\n",
      "  batch 501 loss: 5.473946947859076e-05\n",
      "LOSS train 5.428044206694279e-05 valid 5.1143364544259384e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.6317864467273465e-07\n",
      "  batch 101 loss: 5.5861956598164396e-05\n",
      "  batch 201 loss: 5.608424118690891e-05\n",
      "  batch 301 loss: 5.6346601668337825e-05\n",
      "  batch 401 loss: 5.592833083937876e-05\n",
      "  batch 501 loss: 5.6004369907896036e-05\n",
      "LOSS train 5.548419173219169e-05 valid 5.1412695029284805e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.821020279952791e-07\n",
      "  batch 101 loss: 5.725649378291564e-05\n",
      "  batch 201 loss: 5.752464186116413e-05\n",
      "  batch 301 loss: 5.7853364260154195e-05\n",
      "  batch 401 loss: 5.755366678386054e-05\n",
      "  batch 501 loss: 5.7593957244534977e-05\n",
      "LOSS train 5.700890743134837e-05 valid 5.1819275540765375e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.053334148717113e-07\n",
      "  batch 101 loss: 5.896270398807246e-05\n",
      "  batch 201 loss: 5.92657490142301e-05\n",
      "  batch 301 loss: 5.96514393146208e-05\n",
      "  batch 401 loss: 5.94568910128146e-05\n",
      "  batch 501 loss: 5.943518297499395e-05\n",
      "LOSS train 5.881227343472187e-05 valid 5.236922515905462e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 3.3163727493956683e-07\n",
      "  batch 101 loss: 6.086515668357606e-05\n",
      "  batch 201 loss: 6.116743819802651e-05\n",
      "  batch 301 loss: 6.157467919365445e-05\n",
      "  batch 401 loss: 6.144086119093117e-05\n",
      "  batch 501 loss: 6.131383835963788e-05\n",
      "LOSS train 6.0719989968316306e-05 valid 5.299939584801905e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 3.577905590645969e-07\n",
      "  batch 101 loss: 6.271059531172796e-05\n",
      "  batch 201 loss: 6.29618408947863e-05\n",
      "  batch 301 loss: 6.333904713756055e-05\n",
      "  batch 401 loss: 6.320603598396702e-05\n",
      "  batch 501 loss: 6.293956968875136e-05\n",
      "LOSS train 6.24520392533208e-05 valid 5.358645648811944e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 3.7977282772772015e-07\n",
      "  batch 101 loss: 6.421880390917068e-05\n",
      "  batch 201 loss: 6.438523337237712e-05\n",
      "  batch 301 loss: 6.46973610855639e-05\n",
      "  batch 401 loss: 6.45247519514669e-05\n",
      "  batch 501 loss: 6.412140648535569e-05\n",
      "LOSS train 6.377652269343414e-05 valid 5.403011164162308e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 3.9530237700091675e-07\n",
      "  batch 101 loss: 6.525926164158591e-05\n",
      "  batch 201 loss: 6.534210750032799e-05\n",
      "  batch 301 loss: 6.558778242379049e-05\n",
      "  batch 401 loss: 6.536873424920487e-05\n",
      "  batch 501 loss: 6.486173814664654e-05\n",
      "LOSS train 6.4642422476872e-05 valid 5.43128298886586e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.6441522297536724e-08\n",
      "  batch 101 loss: 8.138549323348343e-05\n",
      "  batch 201 loss: 6.281209279165978e-05\n",
      "  batch 301 loss: 4.282480228710028e-05\n",
      "  batch 401 loss: 6.949773828637262e-05\n",
      "  batch 501 loss: 7.509535906137899e-05\n",
      "LOSS train 6.606333077479341e-05 valid 5.5898686696309596e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.692129004979506e-07\n",
      "  batch 101 loss: 7.039112063011999e-05\n",
      "  batch 201 loss: 7.189974750872352e-05\n",
      "  batch 301 loss: 8.212376491428586e-05\n",
      "  batch 401 loss: 7.326809649384814e-05\n",
      "  batch 501 loss: 6.59462705812075e-05\n",
      "LOSS train 7.208330825005838e-05 valid 5.6018172472249717e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.575110506266355e-07\n",
      "  batch 101 loss: 6.98494387415849e-05\n",
      "  batch 201 loss: 6.931801423434081e-05\n",
      "  batch 301 loss: 6.907657892497809e-05\n",
      "  batch 401 loss: 6.850456848496833e-05\n",
      "  batch 501 loss: 6.746815072801838e-05\n",
      "LOSS train 6.802597554019133e-05 valid 5.535698073799722e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.3788240873254835e-07\n",
      "  batch 101 loss: 6.780754644751141e-05\n",
      "  batch 201 loss: 6.750717529939721e-05\n",
      "  batch 301 loss: 6.74364557107765e-05\n",
      "  batch 401 loss: 6.697000940221187e-05\n",
      "  batch 501 loss: 6.613957205445331e-05\n",
      "LOSS train 6.642061517515885e-05 valid 5.478229286381975e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.200426337774843e-07\n",
      "  batch 101 loss: 6.675886350876681e-05\n",
      "  batch 201 loss: 6.660541561359423e-05\n",
      "  batch 301 loss: 6.666552501883417e-05\n",
      "  batch 401 loss: 6.630807810324768e-05\n",
      "  batch 501 loss: 6.562408495938143e-05\n",
      "LOSS train 6.569042802148158e-05 valid 5.4590269428445026e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 4.13890193158295e-07\n",
      "  batch 101 loss: 6.645001392826089e-05\n",
      "  batch 201 loss: 6.638833968281688e-05\n",
      "  batch 301 loss: 6.65242372906505e-05\n",
      "  batch 401 loss: 6.622823508223519e-05\n",
      "  batch 501 loss: 6.559753473084129e-05\n",
      "LOSS train 6.555762447861931e-05 valid 5.4594660468865186e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 4.1403211071155965e-07\n",
      "  batch 101 loss: 6.648667723311518e-05\n",
      "  batch 201 loss: 6.64423116359103e-05\n",
      "  batch 301 loss: 6.658756076831196e-05\n",
      "  batch 401 loss: 6.629605129091942e-05\n",
      "  batch 501 loss: 6.566030014255375e-05\n",
      "LOSS train 6.561470922743786e-05 valid 5.462032640934922e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.148591324337758e-07\n",
      "  batch 101 loss: 6.654052171597868e-05\n",
      "  batch 201 loss: 6.649023792306253e-05\n",
      "  batch 301 loss: 6.663055174158217e-05\n",
      "  batch 401 loss: 6.633538886489987e-05\n",
      "  batch 501 loss: 6.569358001343062e-05\n",
      "LOSS train 6.565636035835542e-05 valid 5.463324851007201e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.1527498979121446e-07\n",
      "  batch 101 loss: 6.6567163325999e-05\n",
      "  batch 201 loss: 6.651369213614089e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 6.665150314347557e-05\n",
      "  batch 401 loss: 6.635455751165865e-05\n",
      "  batch 501 loss: 6.570979028765578e-05\n",
      "LOSS train 6.567674546578346e-05 valid 5.4639582231175154e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 4.154782072873786e-07\n",
      "  batch 101 loss: 6.658015587163391e-05\n",
      "  batch 201 loss: 6.6525141164675e-05\n",
      "  batch 301 loss: 6.666174095698807e-05\n",
      "  batch 401 loss: 6.636392460677598e-05\n",
      "  batch 501 loss: 6.571772140887333e-05\n",
      "LOSS train 6.568670473254108e-05 valid 5.464264904730953e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.155778879066929e-07\n",
      "  batch 101 loss: 6.658651874204224e-05\n",
      "  batch 201 loss: 6.653074269252101e-05\n",
      "  batch 301 loss: 6.666675071755889e-05\n",
      "  batch 401 loss: 6.636851582698e-05\n",
      "  batch 501 loss: 6.572160506948421e-05\n",
      "LOSS train 6.56915805305065e-05 valid 5.464418063638732e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.156267823418602e-07\n",
      "  batch 101 loss: 6.658963744484936e-05\n",
      "  batch 201 loss: 6.653349406860797e-05\n",
      "  batch 301 loss: 6.666921227406419e-05\n",
      "  batch 401 loss: 6.637077602135833e-05\n",
      "  batch 501 loss: 6.572351514478215e-05\n",
      "LOSS train 6.569397605893311e-05 valid 5.464489368023351e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 4.156507202424109e-07\n",
      "  batch 101 loss: 6.65911649912232e-05\n",
      "  batch 201 loss: 6.653484596427007e-05\n",
      "  batch 301 loss: 6.667042311619297e-05\n",
      "  batch 401 loss: 6.637188154854811e-05\n",
      "  batch 501 loss: 6.572444579887816e-05\n",
      "LOSS train 6.569514992073337e-05 valid 5.464530840981752e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 4.1566265281289814e-07\n",
      "  batch 101 loss: 6.659192077677289e-05\n",
      "  batch 201 loss: 6.653550773080496e-05\n",
      "  batch 301 loss: 6.667102259598323e-05\n",
      "  batch 401 loss: 6.637243714976648e-05\n",
      "  batch 501 loss: 6.572492098257499e-05\n",
      "LOSS train 6.56957345614601e-05 valid 5.4645461204927415e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 4.1566854633856567e-07\n",
      "  batch 101 loss: 6.659229590241012e-05\n",
      "  batch 201 loss: 6.653583855950273e-05\n",
      "  batch 301 loss: 6.667131496215007e-05\n",
      "  batch 401 loss: 6.637269911152543e-05\n",
      "  batch 501 loss: 6.572514402250818e-05\n",
      "LOSS train 6.569601897109804e-05 valid 5.464557034429163e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 4.1567142034182325e-07\n",
      "  batch 101 loss: 6.659247850166139e-05\n",
      "  batch 201 loss: 6.65360073071497e-05\n",
      "  batch 301 loss: 6.667146704330662e-05\n",
      "  batch 401 loss: 6.637284132921195e-05\n",
      "  batch 501 loss: 6.572525763658633e-05\n",
      "LOSS train 6.569616389226929e-05 valid 5.464557762024924e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0015540061891078948\n",
      "  batch 101 loss: 0.00283241987492147\n",
      "  batch 201 loss: 5.118737945849716e-05\n",
      "  batch 301 loss: 1.7965404158530873e-05\n",
      "  batch 401 loss: 1.3972431900697302e-05\n",
      "  batch 501 loss: 2.291816829043114e-06\n",
      "LOSS train 0.0007699120739329689 valid 3.175233359797858e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.225433440296911e-07\n",
      "  batch 101 loss: 3.04310435996058e-05\n",
      "  batch 201 loss: 7.037920711923107e-06\n",
      "  batch 301 loss: 3.519603377526437e-06\n",
      "  batch 401 loss: 4.061187272554889e-06\n",
      "  batch 501 loss: 2.6634798024360862e-06\n",
      "LOSS train 8.527422773727385e-06 valid 3.091480175498873e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.43010576418601e-07\n",
      "  batch 101 loss: 2.6342253672524406e-05\n",
      "  batch 201 loss: 2.9793827991397848e-06\n",
      "  batch 301 loss: 3.523557633542396e-06\n",
      "  batch 401 loss: 2.8988423107989546e-06\n",
      "  batch 501 loss: 2.7642661541449343e-06\n",
      "LOSS train 6.973236158523891e-06 valid 2.4682203729753383e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.424075061455369e-07\n",
      "  batch 101 loss: 3.4013175490485995e-05\n",
      "  batch 201 loss: 3.0484413217379823e-06\n",
      "  batch 301 loss: 2.896149196232045e-06\n",
      "  batch 401 loss: 3.3654591345566586e-06\n",
      "  batch 501 loss: 3.0008626475819257e-06\n",
      "LOSS train 8.382953720690134e-06 valid 3.1054547434905544e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.7302950406447053e-07\n",
      "  batch 101 loss: 4.159798429896e-05\n",
      "  batch 201 loss: 3.9613705513374955e-06\n",
      "  batch 301 loss: 2.6939196541775344e-06\n",
      "  batch 401 loss: 4.272485612659693e-06\n",
      "  batch 501 loss: 3.1403288529929796e-06\n",
      "LOSS train 9.959180842807935e-06 valid 2.552056685090065e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.974116356810555e-07\n",
      "  batch 101 loss: 3.954077238773834e-05\n",
      "  batch 201 loss: 5.137620355810668e-06\n",
      "  batch 301 loss: 4.3659156565922785e-06\n",
      "  batch 401 loss: 4.955362062730728e-06\n",
      "  batch 501 loss: 3.4413604232952366e-06\n",
      "LOSS train 1.0314180204191103e-05 valid 2.01592811208684e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.610533455386758e-07\n",
      "  batch 101 loss: 3.951398959372909e-05\n",
      "  batch 201 loss: 3.410241581605078e-06\n",
      "  batch 301 loss: 2.5345754565364587e-06\n",
      "  batch 401 loss: 4.570774194405658e-06\n",
      "  batch 501 loss: 4.574808718729173e-06\n",
      "LOSS train 9.890537213434062e-06 valid 1.7059313904610462e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.335932943737135e-07\n",
      "  batch 101 loss: 4.192692444007662e-05\n",
      "  batch 201 loss: 3.4189187022093394e-06\n",
      "  batch 301 loss: 2.5338981444633648e-06\n",
      "  batch 401 loss: 4.06424412318529e-06\n",
      "  batch 501 loss: 4.658038793223795e-06\n",
      "LOSS train 1.0271828813840013e-05 valid 1.616805457160808e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 8.770985004957765e-07\n",
      "  batch 101 loss: 4.633077261502194e-05\n",
      "  batch 201 loss: 4.1709693820735086e-06\n",
      "  batch 301 loss: 2.483355900153583e-06\n",
      "  batch 401 loss: 3.5869683756573068e-06\n",
      "  batch 501 loss: 4.848132997778976e-06\n",
      "LOSS train 1.107146006383652e-05 valid 1.5851459465920925e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 8.250029350165277e-07\n",
      "  batch 101 loss: 4.0720870631218984e-05\n",
      "  batch 201 loss: 3.897269614867582e-06\n",
      "  batch 301 loss: 2.628290601762728e-06\n",
      "  batch 401 loss: 3.2380623706274037e-06\n",
      "  batch 501 loss: 4.881997651295933e-06\n",
      "LOSS train 9.995768268979101e-06 valid 1.554523259983398e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.92007049312815e-07\n",
      "  batch 101 loss: 3.96304453323637e-05\n",
      "  batch 201 loss: 3.3094094591490377e-06\n",
      "  batch 301 loss: 2.9570759699026894e-06\n",
      "  batch 401 loss: 3.0415118749260727e-06\n",
      "  batch 501 loss: 4.345293456040622e-06\n",
      "LOSS train 9.602870342975857e-06 valid 1.4588299563911278e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.967330020619556e-07\n",
      "  batch 101 loss: 0.00024029937566524495\n",
      "  batch 201 loss: 2.548346532080359e-05\n",
      "  batch 301 loss: 5.844568814268314e-06\n",
      "  batch 401 loss: 5.322872146322766e-06\n",
      "  batch 501 loss: 2.698988603668795e-06\n",
      "LOSS train 4.847895616056267e-05 valid 1.5530338714597747e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 6.556398875545711e-07\n",
      "  batch 101 loss: 2.4063135365111064e-05\n",
      "  batch 201 loss: 2.234275157633192e-06\n",
      "  batch 301 loss: 3.1451452176156636e-06\n",
      "  batch 401 loss: 3.621661122181763e-06\n",
      "  batch 501 loss: 2.3607611159093267e-06\n",
      "LOSS train 6.424771062447967e-06 valid 1.5747436918900348e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 6.230724102351814e-07\n",
      "  batch 101 loss: 3.4679528991432565e-05\n",
      "  batch 201 loss: 2.1588319231113927e-06\n",
      "  batch 301 loss: 2.7080079139807366e-06\n",
      "  batch 401 loss: 3.6742341752926676e-06\n",
      "  batch 501 loss: 2.5808646427094574e-06\n",
      "LOSS train 8.228896439169757e-06 valid 1.3478171240421943e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 7.121364615159109e-07\n",
      "  batch 101 loss: 2.979538918111757e-05\n",
      "  batch 201 loss: 2.807785666334439e-06\n",
      "  batch 301 loss: 2.76715791613924e-06\n",
      "  batch 401 loss: 3.6393635386389177e-06\n",
      "  batch 501 loss: 3.2332020973058205e-06\n",
      "LOSS train 7.634340732920464e-06 valid 1.2681684893323109e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 7.165557326516136e-07\n",
      "  batch 101 loss: 2.3415028405224802e-05\n",
      "  batch 201 loss: 2.6415410286517726e-06\n",
      "  batch 301 loss: 2.762534386988591e-06\n",
      "  batch 401 loss: 3.351996131044643e-06\n",
      "  batch 501 loss: 2.5742228282865653e-06\n",
      "LOSS train 6.35447255620975e-06 valid 1.4642630048911087e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002648560330271721\n",
      "  batch 101 loss: 0.0004034156311308834\n",
      "  batch 201 loss: 3.196832052822174e-05\n",
      "  batch 301 loss: 2.1348583704821067e-05\n",
      "  batch 401 loss: 8.583994921309568e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 501 loss: 2.6321989221855803e-06\n",
      "LOSS train 0.00012633684148409592 valid 2.454861942169373e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.595113856950775e-08\n",
      "  batch 101 loss: 4.002243027372288e-06\n",
      "  batch 201 loss: 1.423041800308056e-06\n",
      "  batch 301 loss: 1.1373110351087235e-06\n",
      "  batch 401 loss: 1.931201853011544e-06\n",
      "  batch 501 loss: 1.743991708451631e-06\n",
      "LOSS train 2.1840234116734675e-06 valid 2.8916783776367083e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.973846105509438e-08\n",
      "  batch 101 loss: 4.116663947399957e-06\n",
      "  batch 201 loss: 1.3503300545636422e-06\n",
      "  batch 301 loss: 3.3149252671194063e-06\n",
      "  batch 401 loss: 1.360691420160265e-06\n",
      "  batch 501 loss: 1.2521881053828566e-06\n",
      "LOSS train 2.299007544967255e-06 valid 1.7874917830340564e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.714941602898761e-08\n",
      "  batch 101 loss: 4.848460049799996e-06\n",
      "  batch 201 loss: 3.0316521821305287e-06\n",
      "  batch 301 loss: 1.7403401980686796e-06\n",
      "  batch 401 loss: 2.0549777966039075e-06\n",
      "  batch 501 loss: 1.3757053494600768e-06\n",
      "LOSS train 2.464829638360855e-06 valid 2.556132812969736e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.701586476585362e-08\n",
      "  batch 101 loss: 6.426224763060873e-06\n",
      "  batch 201 loss: 1.995711005662315e-06\n",
      "  batch 301 loss: 2.041680877340468e-06\n",
      "  batch 401 loss: 2.816984614071316e-06\n",
      "  batch 501 loss: 2.120934514948658e-06\n",
      "LOSS train 2.77075364234522e-06 valid 1.3853986047251965e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.811307114025112e-08\n",
      "  batch 101 loss: 5.283123647998877e-06\n",
      "  batch 201 loss: 2.4253216083991447e-06\n",
      "  batch 301 loss: 5.145207134091834e-06\n",
      "  batch 401 loss: 3.0428077576516445e-06\n",
      "  batch 501 loss: 4.070014912826991e-06\n",
      "LOSS train 3.703179328846719e-06 valid 1.8092536038238904e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.420879228448029e-08\n",
      "  batch 101 loss: 3.842693952549325e-06\n",
      "  batch 201 loss: 2.049256834766311e-06\n",
      "  batch 301 loss: 2.118517578111323e-06\n",
      "  batch 401 loss: 2.790546661941562e-06\n",
      "  batch 501 loss: 1.8218735110053785e-06\n",
      "LOSS train 2.404373902854966e-06 valid 2.566574721640791e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2037761734973173e-07\n",
      "  batch 101 loss: 4.10945798861917e-06\n",
      "  batch 201 loss: 1.2032581547316567e-06\n",
      "  batch 301 loss: 1.933073099280591e-06\n",
      "  batch 401 loss: 1.4850603929517092e-06\n",
      "  batch 501 loss: 1.4401624083859588e-06\n",
      "LOSS train 2.110616840937569e-06 valid 1.290189970859501e-06\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 5.558424618357094e-08\n",
      "  batch 101 loss: 3.2543911612492595e-06\n",
      "  batch 201 loss: 2.4786521169062325e-06\n",
      "  batch 301 loss: 2.9778297974303314e-06\n",
      "  batch 401 loss: 1.8423113324672612e-06\n",
      "  batch 501 loss: 1.589338677234764e-06\n",
      "LOSS train 2.2995111994149113e-06 valid 1.8012482314588851e-06\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.11372469291382e-08\n",
      "  batch 101 loss: 1.7437201582026774e-06\n",
      "  batch 201 loss: 1.5578248465430987e-06\n",
      "  batch 301 loss: 1.4893368752666446e-06\n",
      "  batch 401 loss: 1.7361436707119536e-06\n",
      "  batch 501 loss: 1.51453698123305e-06\n",
      "LOSS train 1.5707837188161608e-06 valid 1.4669030861114152e-06\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.0699639005906648e-08\n",
      "  batch 101 loss: 2.39438862472241e-06\n",
      "  batch 201 loss: 1.6450056158134884e-06\n",
      "  batch 301 loss: 1.999502696037325e-06\n",
      "  batch 401 loss: 1.8853353770964532e-06\n",
      "  batch 501 loss: 1.707150080392239e-06\n",
      "LOSS train 1.8952090865462105e-06 valid 1.0174694580200594e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.677836280578049e-08\n",
      "  batch 101 loss: 1.2326744813151436e-06\n",
      "  batch 201 loss: 1.003344137870954e-06\n",
      "  batch 301 loss: 9.325857968178752e-07\n",
      "  batch 401 loss: 1.1626868796099644e-06\n",
      "  batch 501 loss: 7.910215017403743e-07\n",
      "LOSS train 9.83155031314342e-07 valid 9.796990525501315e-07\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.3900305475544883e-08\n",
      "  batch 101 loss: 1.1943467038122435e-06\n",
      "  batch 201 loss: 1.0507607605347858e-06\n",
      "  batch 301 loss: 8.599105377271599e-07\n",
      "  batch 401 loss: 1.2782777038466975e-06\n",
      "  batch 501 loss: 9.387768263025009e-07\n",
      "LOSS train 1.0137928737629012e-06 valid 8.52457787914318e-07\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 1.1539858633113908e-08\n",
      "  batch 101 loss: 1.5025863248041559e-06\n",
      "  batch 201 loss: 8.836975439407979e-07\n",
      "  batch 301 loss: 1.1944459777168958e-06\n",
      "  batch 401 loss: 1.089134250022994e-06\n",
      "  batch 501 loss: 1.2438869235609217e-06\n",
      "LOSS train 1.1119906012231538e-06 valid 4.914833766633819e-07\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 1.0563873047431116e-08\n",
      "  batch 101 loss: 5.977340196317016e-07\n",
      "  batch 201 loss: 8.078080945495003e-07\n",
      "  batch 301 loss: 1.3244477447216242e-06\n",
      "  batch 401 loss: 8.794355041175095e-07\n",
      "  batch 501 loss: 1.1511669630692723e-06\n",
      "LOSS train 9.379361579227955e-07 valid 8.209287898353068e-07\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 9.229029842572345e-09\n",
      "  batch 101 loss: 8.12150967846037e-07\n",
      "  batch 201 loss: 1.0881596366374424e-06\n",
      "  batch 301 loss: 1.3055770434533542e-06\n",
      "  batch 401 loss: 9.470729253280296e-07\n",
      "  batch 501 loss: 1.1344880195451878e-06\n",
      "LOSS train 1.01401482689615e-06 valid 5.076984166407783e-07\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0019672051072120666\n",
      "  batch 101 loss: 0.0035839258258795327\n",
      "  batch 201 loss: 4.49724816746766e-06\n",
      "  batch 301 loss: 4.87203961156979e-06\n",
      "  batch 401 loss: 1.0601620298018588e-05\n",
      "  batch 501 loss: 4.524480795566887e-06\n",
      "LOSS train 0.0009599113112842429 valid 1.2051367775711697e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.6523575292667374e-07\n",
      "  batch 101 loss: 1.0219012714003383e-05\n",
      "  batch 201 loss: 2.3599963648734957e-06\n",
      "  batch 301 loss: 2.1135621391010772e-06\n",
      "  batch 401 loss: 5.671072405277755e-06\n",
      "  batch 501 loss: 4.087123268590176e-06\n",
      "LOSS train 4.509261220805292e-06 valid 1.3841709005646408e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.2272855353076013e-07\n",
      "  batch 101 loss: 8.44160504499314e-06\n",
      "  batch 201 loss: 2.400894831282585e-06\n",
      "  batch 301 loss: 3.0221251898865378e-06\n",
      "  batch 401 loss: 6.207920922349785e-06\n",
      "  batch 501 loss: 4.142309607573224e-06\n",
      "LOSS train 4.502117540969818e-06 valid 1.1848530448332895e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.775960405822844e-07\n",
      "  batch 101 loss: 1.5082058694133593e-05\n",
      "  batch 201 loss: 2.554750530094907e-06\n",
      "  batch 301 loss: 3.3129692220512652e-06\n",
      "  batch 401 loss: 8.557986510595584e-06\n",
      "  batch 501 loss: 4.942459669052823e-06\n",
      "LOSS train 6.6240904865538175e-06 valid 1.0629559255903587e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.449486292898655e-07\n",
      "  batch 101 loss: 2.1227980520279745e-05\n",
      "  batch 201 loss: 6.198245953044079e-06\n",
      "  batch 301 loss: 6.162028381737628e-06\n",
      "  batch 401 loss: 8.590358454796387e-06\n",
      "  batch 501 loss: 1.6203625808941523e-05\n",
      "LOSS train 1.2489982361819466e-05 valid 1.844410871854052e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.278386334888637e-06\n",
      "  batch 101 loss: 2.458161208551246e-05\n",
      "  batch 201 loss: 1.4912609088639784e-05\n",
      "  batch 301 loss: 1.1111799262124578e-05\n",
      "  batch 401 loss: 1.3072712476969173e-05\n",
      "  batch 501 loss: 1.051060627219158e-05\n",
      "LOSS train 1.4282690220487668e-05 valid 1.7519447283120826e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0021546768257394e-06\n",
      "  batch 101 loss: 2.7886302885349323e-05\n",
      "  batch 201 loss: 1.2002540766502533e-05\n",
      "  batch 301 loss: 1.5954466426819636e-05\n",
      "  batch 401 loss: 1.2231704328655724e-05\n",
      "  batch 501 loss: 1.444090117217911e-05\n",
      "LOSS train 1.5884213559331293e-05 valid 1.8265523976879194e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.040259812725708e-06\n",
      "  batch 101 loss: 2.0687577024602887e-05\n",
      "  batch 201 loss: 1.1293736223478845e-05\n",
      "  batch 301 loss: 1.6387995121931453e-05\n",
      "  batch 401 loss: 1.6151492104938825e-05\n",
      "  batch 501 loss: 1.3283370959698004e-05\n",
      "LOSS train 1.4460463255224714e-05 valid 2.887823575292714e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 7.90407966633211e-09\n",
      "  batch 101 loss: 9.79354912033159e-06\n",
      "  batch 201 loss: 1.5701054176133768e-05\n",
      "  batch 301 loss: 5.949715072688377e-06\n",
      "  batch 401 loss: 1.5964318477017516e-05\n",
      "  batch 501 loss: 1.0674904876850632e-05\n",
      "LOSS train 1.0768203293534878e-05 valid 2.1644935259246267e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.0495097058083048e-08\n",
      "  batch 101 loss: 1.0035756390891493e-05\n",
      "  batch 201 loss: 1.1899799899879326e-05\n",
      "  batch 301 loss: 5.999845974429263e-06\n",
      "  batch 401 loss: 1.8254704784226305e-05\n",
      "  batch 501 loss: 6.982689874917014e-06\n",
      "LOSS train 9.807321977870395e-06 valid 8.16478222986916e-06\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 6.149878572614398e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 1.10278523427354e-05\n",
      "  batch 201 loss: 5.794633508173774e-06\n",
      "  batch 301 loss: 5.630273479937387e-06\n",
      "  batch 401 loss: 1.1395645855714066e-05\n",
      "  batch 501 loss: 5.643354879509843e-06\n",
      "LOSS train 7.427067833182455e-06 valid 6.883709829708096e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.190307663520798e-08\n",
      "  batch 101 loss: 8.977627167539027e-06\n",
      "  batch 201 loss: 5.449104569095198e-06\n",
      "  batch 301 loss: 4.885684838598081e-06\n",
      "  batch 401 loss: 1.0483772358043098e-05\n",
      "  batch 501 loss: 5.800544033149891e-06\n",
      "LOSS train 6.744180299765468e-06 valid 7.639236173417885e-06\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.3157741705072113e-08\n",
      "  batch 101 loss: 6.7807615019432884e-06\n",
      "  batch 201 loss: 4.09711113462663e-06\n",
      "  batch 301 loss: 4.5651659024770194e-06\n",
      "  batch 401 loss: 7.857106729147744e-06\n",
      "  batch 501 loss: 5.5235532408914875e-06\n",
      "LOSS train 5.51727438254768e-06 valid 2.9986292702233186e-06\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 4.692155016527977e-08\n",
      "  batch 101 loss: 5.813234492961783e-06\n",
      "  batch 201 loss: 3.691241552132851e-06\n",
      "  batch 301 loss: 3.807425546256127e-06\n",
      "  batch 401 loss: 4.831734209744809e-06\n",
      "  batch 501 loss: 3.715324723430058e-06\n",
      "LOSS train 4.2109574368910975e-06 valid 2.554924321884755e-06\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 3.162634129694197e-08\n",
      "  batch 101 loss: 4.724649632521505e-06\n",
      "  batch 201 loss: 3.4257293672013135e-06\n",
      "  batch 301 loss: 3.2871884096152824e-06\n",
      "  batch 401 loss: 4.447401875609102e-06\n",
      "  batch 501 loss: 3.1763684530972114e-06\n",
      "LOSS train 3.68515456410532e-06 valid 3.0806015729467617e-06\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 5.990149247736554e-09\n",
      "  batch 101 loss: 3.272142672017253e-06\n",
      "  batch 201 loss: 3.316970971702915e-06\n",
      "  batch 301 loss: 3.041857664811687e-06\n",
      "  batch 401 loss: 4.957879612277339e-06\n",
      "  batch 501 loss: 3.9214846992763345e-06\n",
      "LOSS train 3.531545419222323e-06 valid 6.517738711409038e-06\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00045495521277189255\n",
      "  batch 101 loss: 0.01858087623069082\n",
      "  batch 201 loss: 3.8617798294353635e-05\n",
      "  batch 301 loss: 1.911413151674424e-05\n",
      "  batch 401 loss: 1.3657978162200379e-05\n",
      "  batch 501 loss: 4.686122356076794e-06\n",
      "LOSS train 0.0032897713446854828 valid 6.726071660523303e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2246665392012802e-07\n",
      "  batch 101 loss: 5.793344370630393e-06\n",
      "  batch 201 loss: 1.6997515703565114e-06\n",
      "  batch 301 loss: 1.8183122358550463e-06\n",
      "  batch 401 loss: 7.217977666158504e-06\n",
      "  batch 501 loss: 2.2470126351947782e-06\n",
      "LOSS train 3.4597943291714534e-06 valid 4.423277005116688e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.54155223350972e-08\n",
      "  batch 101 loss: 5.746389335854474e-06\n",
      "  batch 201 loss: 1.4145326481695974e-06\n",
      "  batch 301 loss: 2.160083039939309e-06\n",
      "  batch 401 loss: 4.640494557577313e-06\n",
      "  batch 501 loss: 2.2117221777762098e-06\n",
      "LOSS train 2.9886719691375346e-06 valid 4.881403128820239e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.0114064682275058e-07\n",
      "  batch 101 loss: 5.753035622717562e-06\n",
      "  batch 201 loss: 1.211677168839742e-06\n",
      "  batch 301 loss: 4.937321009634843e-06\n",
      "  batch 401 loss: 4.006765727098127e-06\n",
      "  batch 501 loss: 2.31554612824425e-06\n",
      "LOSS train 3.4540012216470713e-06 valid 9.536172910884488e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.614193701650947e-07\n",
      "  batch 101 loss: 1.0555380003722802e-05\n",
      "  batch 201 loss: 1.083634174534609e-06\n",
      "  batch 301 loss: 2.997871321213097e-06\n",
      "  batch 401 loss: 2.7614650767304737e-06\n",
      "  batch 501 loss: 2.280467958541976e-06\n",
      "LOSS train 4.017385285668135e-06 valid 1.020179388433462e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.854098031297326e-07\n",
      "  batch 101 loss: 5.728188168916404e-06\n",
      "  batch 201 loss: 1.2359230252911857e-06\n",
      "  batch 301 loss: 3.781039481864923e-06\n",
      "  batch 401 loss: 2.30616162710362e-06\n",
      "  batch 501 loss: 1.8734524923047501e-06\n",
      "LOSS train 3.18189031492422e-06 valid 8.40926259115804e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.299930176581256e-07\n",
      "  batch 101 loss: 5.317493820555796e-06\n",
      "  batch 201 loss: 9.062428893003016e-07\n",
      "  batch 301 loss: 3.9329934126897115e-06\n",
      "  batch 401 loss: 2.498455149577694e-06\n",
      "  batch 501 loss: 1.8239284250398668e-06\n",
      "LOSS train 3.000889631787333e-06 valid 2.4820574253681116e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3120972653268836e-07\n",
      "  batch 101 loss: 3.6428409357824874e-06\n",
      "  batch 201 loss: 1.0603613674220469e-06\n",
      "  batch 301 loss: 3.264165623875215e-06\n",
      "  batch 401 loss: 1.9065064756773608e-06\n",
      "  batch 501 loss: 1.7836272687077326e-06\n",
      "LOSS train 2.3330615964304738e-06 valid 1.9883930235664593e-06\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 9.868430424830876e-08\n",
      "  batch 101 loss: 2.9517143775592556e-06\n",
      "  batch 201 loss: 1.3672055109026359e-06\n",
      "  batch 301 loss: 3.394381101173849e-06\n",
      "  batch 401 loss: 1.2157076022845104e-06\n",
      "  batch 501 loss: 1.7034223850487252e-06\n",
      "LOSS train 2.139859425103533e-06 valid 1.5322718809329672e-06\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.576414661030867e-08\n",
      "  batch 101 loss: 1.7459073016112825e-06\n",
      "  batch 201 loss: 1.5889469423768788e-06\n",
      "  batch 301 loss: 1.9028117036157254e-06\n",
      "  batch 401 loss: 1.1656577730434491e-06\n",
      "  batch 501 loss: 2.6887291775778976e-06\n",
      "LOSS train 1.7362245110076352e-06 valid 8.195225404961093e-07\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.8593258321052416e-08\n",
      "  batch 101 loss: 1.6397973762138917e-06\n",
      "  batch 201 loss: 1.5014980517236153e-06\n",
      "  batch 301 loss: 1.4261256069403316e-06\n",
      "  batch 401 loss: 2.0439899742541456e-06\n",
      "  batch 501 loss: 1.8449192093328293e-06\n",
      "LOSS train 1.600452437046094e-06 valid 1.0391121350039612e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.190767301930464e-08\n",
      "  batch 101 loss: 2.1734044952381735e-06\n",
      "  batch 201 loss: 1.6716627030177732e-06\n",
      "  batch 301 loss: 1.8091989457502678e-06\n",
      "  batch 401 loss: 2.1251368002594973e-06\n",
      "  batch 501 loss: 1.6977282945163098e-06\n",
      "LOSS train 1.916476198804876e-06 valid 1.4283435803008615e-06\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 4.248201548762154e-08\n",
      "  batch 101 loss: 3.3264394625120986e-06\n",
      "  batch 201 loss: 6.658887889159359e-06\n",
      "  batch 301 loss: 1.935207201242406e-06\n",
      "  batch 401 loss: 9.99516998092531e-07\n",
      "  batch 501 loss: 1.5818653831445317e-06\n",
      "LOSS train 2.6198394821191553e-06 valid 1.2541199794213753e-06\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 2.702122174014221e-08\n",
      "  batch 101 loss: 2.219331693993354e-06\n",
      "  batch 201 loss: 1.1397844303218108e-06\n",
      "  batch 301 loss: 1.7191899243584886e-06\n",
      "  batch 401 loss: 2.3958000440416073e-06\n",
      "  batch 501 loss: 2.6686534293673958e-06\n",
      "LOSS train 2.05021206097057e-06 valid 2.4943831249402137e-06\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 7.115986591088586e-08\n",
      "  batch 101 loss: 2.266326793431972e-06\n",
      "  batch 201 loss: 1.4436086521385505e-06\n",
      "  batch 301 loss: 1.952089592833772e-06\n",
      "  batch 401 loss: 2.0293404902105295e-06\n",
      "  batch 501 loss: 2.5651825959016607e-06\n",
      "LOSS train 2.034640711746961e-06 valid 1.4331001239042962e-06\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 4.228866146149812e-08\n",
      "  batch 101 loss: 2.7246106041189934e-06\n",
      "  batch 201 loss: 2.01024268733363e-06\n",
      "  batch 301 loss: 1.9040970106942723e-06\n",
      "  batch 401 loss: 2.594898303129867e-06\n",
      "  batch 501 loss: 4.1827563404694956e-06\n",
      "LOSS train 2.523738806768222e-06 valid 2.6488742150831968e-06\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0017971032857894897\n",
      "  batch 101 loss: 0.00871034726871585\n",
      "  batch 201 loss: 2.3364000830952137e-05\n",
      "  batch 301 loss: 4.23943497764867e-06\n",
      "  batch 401 loss: 7.741983421851729e-06\n",
      "  batch 501 loss: 5.745829211036834e-06\n",
      "LOSS train 0.00181598769229434 valid 2.561793735367246e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8762992112897336e-07\n",
      "  batch 101 loss: 1.8660723699213123e-05\n",
      "  batch 201 loss: 3.941834181659942e-06\n",
      "  batch 301 loss: 3.4303751715469843e-06\n",
      "  batch 401 loss: 4.873154121582957e-06\n",
      "  batch 501 loss: 4.243281209141969e-06\n",
      "LOSS train 6.4267628849636495e-06 valid 1.8228942280984484e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File trained_models/RTS24_AC_12w_split_by_exec_pn/min_val/model_OE_2h_20e_0.000625lr_0dor_0np_False_ro.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOE_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_hidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124me_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mnp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelu_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ro\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m---> 21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multiple_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolder_to_save\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m saved_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mt \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_val\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\OneDrive - KU Leuven\\JointResearch\\05. Models\\03.ANN_CostEstimator_For_LB\\training_methods.py:106\u001b[0m, in \u001b[0;36mtrain_multiple_epochs\u001b[1;34m(nb_epochs, model, training_loader, validation_loader, loss_fn, optimizer, model_name, folder)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(folder \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    105\u001b[0m             min_val_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/min_val/model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder,model_name)\n\u001b[1;32m--> 106\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     epoch_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    113\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/all_epochs/model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder,model_name)\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File trained_models/RTS24_AC_12w_split_by_exec_pn/min_val/model_OE_2h_20e_0.000625lr_0dor_0np_False_ro.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0025/4*4**i for i in range(3)]\n",
    "negative_penalisations = [0,0.00001,0.0001,0.001,0.01]\n",
    "\n",
    "nbs_e = [16,20]#,8,12,16]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [2,3]\n",
    "dors = [0]#,0.05,0.1]#,0.05]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_pn\"\n",
    "for np in negative_penalisations: \n",
    "    loss_fn = NN_classes.create_loss_fn(penalize_negative=np)\n",
    "    #loss_fn = torch.nn.MSELoss()\n",
    "    for relu_out in [False,True]:\n",
    "        for nb_e in nbs_e:\n",
    "            for lr in learning_rates:\n",
    "                for nb_hidden in nbs_hidden: \n",
    "                    for dor in dors:\n",
    "                        m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor,relu_out=relu_out)\n",
    "                        m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro\"\n",
    "                        optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                        train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                        saved_models = dict()\n",
    "\n",
    "                        for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                            path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                            model = m\n",
    "                            m.load_state_dict(torch.load(path))\n",
    "                            m.eval()\n",
    "\n",
    "                            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                            test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "                            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                            train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "                            validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                            validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "\n",
    "                            if mt == \"min_val\": \n",
    "                                min_val = True\n",
    "                            else: \n",
    "                                min_val = False\n",
    "\n",
    "                            r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                              \"Min_val\":min_val,\n",
    "                                              \"Epochs\": nb_e,\n",
    "                                              \"Lr\":lr,\n",
    "                                              \"Dor\": dor,\n",
    "                                              \"Tr_l\":train_loss.item(),\n",
    "                                              \"Te_l\":test_loss.item(),\n",
    "                                              \"V_l\": validation_loss.item(),\n",
    "                                             \"Np\": np,\n",
    "                                             \"Relu_out\": relu_out}\n",
    "                                             ,index = [i]\n",
    "                            )\n",
    "                            i+=1\n",
    "                            results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f652ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Dor</th>\n",
       "      <th>Relu_out</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Np</th>\n",
       "      <th>Min_val</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "      <th>V_l</th>\n",
       "      <th>Tr_l_t_mse</th>\n",
       "      <th>Te_l_t_mse</th>\n",
       "      <th>V_l_t_mse</th>\n",
       "      <th>Tr_l_ret</th>\n",
       "      <th>Train_time</th>\n",
       "      <th>Eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.472839</td>\n",
       "      <td>0.077540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.472839</td>\n",
       "      <td>0.094477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.685800</td>\n",
       "      <td>0.083754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.685800</td>\n",
       "      <td>0.093732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.437337</td>\n",
       "      <td>0.095970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.437337</td>\n",
       "      <td>0.131914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.491558</td>\n",
       "      <td>0.059747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.491558</td>\n",
       "      <td>0.091356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.274385</td>\n",
       "      <td>0.081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.274385</td>\n",
       "      <td>0.089255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.111628</td>\n",
       "      <td>0.091645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.111628</td>\n",
       "      <td>0.084888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.809986</td>\n",
       "      <td>0.074580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.809986</td>\n",
       "      <td>0.084149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.200448</td>\n",
       "      <td>0.119675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.200448</td>\n",
       "      <td>0.132856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.525011</td>\n",
       "      <td>0.107901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.525011</td>\n",
       "      <td>0.136302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>2.818310</td>\n",
       "      <td>0.077888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>2.818310</td>\n",
       "      <td>0.099340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.562706</td>\n",
       "      <td>0.091024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.562706</td>\n",
       "      <td>0.138826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model_type  Dor  Relu_out  Batch_size        Lr  Epochs  Np     Min_val   \n",
       "0            3    0     False          32  0.000625       4   0     min_val  \\\n",
       "1            3    0     False          32  0.000625       4   0  all_epochs   \n",
       "2            3    0     False          32  0.002500       4   0     min_val   \n",
       "3            3    0     False          32  0.002500       4   0  all_epochs   \n",
       "4            3    0     False          32  0.010000       4   0     min_val   \n",
       "5            3    0     False          32  0.010000       4   0  all_epochs   \n",
       "6            3    0     False          32  0.040000       4   0     min_val   \n",
       "7            3    0     False          32  0.040000       4   0  all_epochs   \n",
       "8            3    0     False          64  0.000625       4   0     min_val   \n",
       "9            3    0     False          64  0.000625       4   0  all_epochs   \n",
       "10           3    0     False          64  0.002500       4   0     min_val   \n",
       "11           3    0     False          64  0.002500       4   0  all_epochs   \n",
       "12           3    0     False          64  0.010000       4   0     min_val   \n",
       "13           3    0     False          64  0.010000       4   0  all_epochs   \n",
       "14           3    0     False          64  0.040000       4   0     min_val   \n",
       "15           3    0     False          64  0.040000       4   0  all_epochs   \n",
       "16           3    0     False         128  0.000625       4   0     min_val   \n",
       "17           3    0     False         128  0.000625       4   0  all_epochs   \n",
       "18           3    0     False         128  0.002500       4   0     min_val   \n",
       "19           3    0     False         128  0.002500       4   0  all_epochs   \n",
       "20           3    0     False         128  0.010000       4   0     min_val   \n",
       "21           3    0     False         128  0.010000       4   0  all_epochs   \n",
       "\n",
       "        Tr_l      Te_l       V_l  Tr_l_t_mse  Te_l_t_mse  V_l_t_mse  Tr_l_ret   \n",
       "0   0.000002  0.000002  0.000002    0.000002    0.000002   0.000002  0.000002  \\\n",
       "1   0.000002  0.000002  0.000002    0.000002    0.000002   0.000002  0.000002   \n",
       "2   0.000003  0.000002  0.000002    0.000003    0.000002   0.000002  0.000003   \n",
       "3   0.000003  0.000002  0.000002    0.000003    0.000002   0.000002  0.000003   \n",
       "4   0.000007  0.000005  0.000004    0.000007    0.000005   0.000004  0.000007   \n",
       "5   0.000007  0.000005  0.000004    0.000007    0.000005   0.000004  0.000007   \n",
       "6   0.000010  0.000005  0.000002    0.000010    0.000005   0.000002  0.000010   \n",
       "7   0.000015  0.000006  0.000003    0.000015    0.000006   0.000003  0.000010   \n",
       "8   0.000041  0.000041  0.000040    0.000041    0.000041   0.000040  0.000040   \n",
       "9   0.000121  0.000116  0.000127    0.000121    0.000116   0.000127  0.000040   \n",
       "10  0.000009  0.000006  0.000006    0.000009    0.000006   0.000006  0.000008   \n",
       "11  0.000009  0.000006  0.000006    0.000009    0.000006   0.000006  0.000008   \n",
       "12  0.000035  0.000032  0.000031    0.000035    0.000032   0.000031  0.000035   \n",
       "13  0.000044  0.000039  0.000037    0.000044    0.000039   0.000037  0.000035   \n",
       "14  0.000014  0.000014  0.000030    0.000014    0.000014   0.000030  0.000014   \n",
       "15  0.000014  0.000014  0.000030    0.000014    0.000014   0.000030  0.000014   \n",
       "16  0.000052  0.000052  0.000054    0.000052    0.000052   0.000054  0.000051   \n",
       "17  0.000151  0.000153  0.000153    0.000151    0.000153   0.000153  0.000051   \n",
       "18  0.000299  0.000315  0.000315    0.000299    0.000315   0.000315  0.000294   \n",
       "19  0.000460  0.000513  0.000508    0.000460    0.000513   0.000508  0.000294   \n",
       "20  0.000057  0.000057  0.000059    0.000057    0.000057   0.000059  0.000056   \n",
       "21  0.000114  0.000117  0.000121    0.000114    0.000117   0.000121  0.000056   \n",
       "\n",
       "    Train_time  Eval_time  \n",
       "0     4.472839   0.077540  \n",
       "1     4.472839   0.094477  \n",
       "2     4.685800   0.083754  \n",
       "3     4.685800   0.093732  \n",
       "4     7.437337   0.095970  \n",
       "5     7.437337   0.131914  \n",
       "6     5.491558   0.059747  \n",
       "7     5.491558   0.091356  \n",
       "8     3.274385   0.081493  \n",
       "9     3.274385   0.089255  \n",
       "10    3.111628   0.091645  \n",
       "11    3.111628   0.084888  \n",
       "12    3.809986   0.074580  \n",
       "13    3.809986   0.084149  \n",
       "14    4.200448   0.119675  \n",
       "15    4.200448   0.132856  \n",
       "16    2.525011   0.107901  \n",
       "17    2.525011   0.136302  \n",
       "18    2.818310   0.077888  \n",
       "19    2.818310   0.099340  \n",
       "20    2.562706   0.091024  \n",
       "21    2.562706   0.138826  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe247f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/All_Exec_split_by_exec_penalize_neg_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0f94c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m (\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mMin_val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (results\u001b[38;5;241m.\u001b[39mRelu_out \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m results[f]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "f = (results.Min_val == True) & (results.Relu_out == False)\n",
    "results[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (results.Epochs == 12)  & (results.Model_type != 0) \n",
    "sns.boxplot(y = \"Te_l\",x=\"Dor\",data = results[f],hue = \"Min_val\")\n",
    "plt.savefig(\"Figures/Split_by_exec/Min_val_effect_Testloss_fDor.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a759efd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f =  (results.Model_type == 0) & (results.Min_val == True)\n",
    "sns.boxplot(y = \"V_l\",x=\"Epochs\",data = results[f],hue = \"Lr\")\n",
    "plt.savefig(\"Figures/Split_by_exec/Lr_effect_Testloss_fEpochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859941d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00034164607524871826\n",
      "  batch 101 loss: 0.0002987160199381833\n",
      "LOSS train 0.00046863495772977727 valid 6.615156962652691e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1388824532332364e-07\n",
      "  batch 101 loss: 4.279915004374857e-06\n",
      "LOSS train 3.882639331988874e-06 valid 4.274984803487314e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.7142380026343745e-08\n",
      "  batch 101 loss: 4.714095463782542e-06\n",
      "LOSS train 4.845918405521013e-06 valid 8.368496310140472e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.067415334749967e-07\n",
      "  batch 101 loss: 9.704118892841507e-06\n",
      "LOSS train 9.098841420518179e-06 valid 1.0499067684577312e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2910639017936775e-07\n",
      "  batch 101 loss: 8.047440166905062e-06\n",
      "LOSS train 7.632603447944408e-06 valid 6.453345577028813e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4348146578413435e-07\n",
      "  batch 101 loss: 6.558642437823892e-06\n",
      "LOSS train 6.467442821458197e-06 valid 4.256539796188008e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.248340236605146e-08\n",
      "  batch 101 loss: 6.1425094669687045e-06\n",
      "LOSS train 6.336900537815176e-06 valid 4.042789441882633e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.169131549744634e-08\n",
      "  batch 101 loss: 5.983339131745424e-06\n",
      "LOSS train 6.172924947311109e-06 valid 4.524460564425681e-06\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.05 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005922958254814148\n",
      "  batch 101 loss: 0.0017523869141587056\n",
      "LOSS train 0.0020242822022996437 valid 0.00013990193838253617\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1807685950770974e-05\n",
      "  batch 101 loss: 0.0007025678611535113\n",
      "LOSS train 0.0006539419920729256 valid 3.089294477831572e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.049980780109763e-06\n",
      "  batch 101 loss: 0.0002934100830316311\n",
      "LOSS train 0.00027420853407387314 valid 8.263343625003472e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.201225142925978e-06\n",
      "  batch 101 loss: 0.0001229988248087466\n",
      "LOSS train 0.00011415099240771502 valid 2.0785542801604606e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.711556175607257e-07\n",
      "  batch 101 loss: 6.961023875192041e-05\n",
      "LOSS train 6.641250860889656e-05 valid 1.2547947335406207e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.643611802952365e-07\n",
      "  batch 101 loss: 4.652859639463713e-05\n",
      "LOSS train 4.482836706912652e-05 valid 1.7496835425845347e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.848715954925865e-07\n",
      "  batch 101 loss: 3.48935174770304e-05\n",
      "LOSS train 3.2981187260257635e-05 valid 9.343787496618461e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.013871875940822e-07\n",
      "  batch 101 loss: 2.6938014907500475e-05\n",
      "LOSS train 2.6516977179836185e-05 valid 1.499108475400135e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003759964555501938\n",
      "  batch 101 loss: 0.0002994551378594679\n",
      "LOSS train 0.0004980125270533485 valid 3.066094723180868e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.233185114164371e-07\n",
      "  batch 101 loss: 2.3100672784437393e-05\n",
      "LOSS train 1.828971353175092e-05 valid 5.12090355186956e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.323178927734262e-08\n",
      "  batch 101 loss: 1.5219171934859333e-05\n",
      "LOSS train 1.2941402010222649e-05 valid 4.169864951109048e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.2489332271507e-08\n",
      "  batch 101 loss: 1.201928494879212e-05\n",
      "LOSS train 1.0501387999067735e-05 valid 5.3815638239029795e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3004389074922073e-07\n",
      "  batch 101 loss: 1.1259929774496413e-05\n",
      "LOSS train 1.0635669727176109e-05 valid 5.129483724886086e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2970699572179001e-07\n",
      "  batch 101 loss: 9.978570278548205e-06\n",
      "LOSS train 1.0972103528247216e-05 valid 2.3275651983567514e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.492273991578258e-08\n",
      "  batch 101 loss: 6.416301563518801e-06\n",
      "LOSS train 7.143410925165556e-06 valid 1.5495916159125045e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.3757895582530184e-08\n",
      "  batch 101 loss: 5.631845216953479e-06\n",
      "LOSS train 6.252941732593145e-06 valid 1.602147676749155e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.05 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.1198659922229126e-06\n",
      "  batch 101 loss: 0.0006333536207603174\n",
      "LOSS train 0.0004843319122805719 valid 7.497398473788053e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.627797559602186e-07\n",
      "  batch 101 loss: 8.850092912325636e-05\n",
      "LOSS train 9.681447429673187e-05 valid 5.2603452786570415e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0451241400442086e-07\n",
      "  batch 101 loss: 0.00013104593766911422\n",
      "LOSS train 0.00013360057275920315 valid 6.870205834275112e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.950898336479441e-07\n",
      "  batch 101 loss: 0.00013878705189654283\n",
      "LOSS train 0.00013746411305362886 valid 7.302850281121209e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.970096678240225e-07\n",
      "  batch 101 loss: 0.00013051096957042318\n",
      "LOSS train 0.00013020883218409603 valid 7.943673699628562e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.125953349401243e-06\n",
      "  batch 101 loss: 0.00012879935582532198\n",
      "LOSS train 0.00012868645641875268 valid 7.725907926214859e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.451952064409852e-07\n",
      "  batch 101 loss: 0.00012520438244791876\n",
      "LOSS train 0.0001248416180423039 valid 7.843563798815012e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.82089768513106e-07\n",
      "  batch 101 loss: 0.0001242020824338397\n",
      "LOSS train 0.0001238926775927112 valid 7.970364822540432e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.001558859497308731\n",
      "  batch 101 loss: 0.005186513712633314\n",
      "LOSS train 0.004936050064179985 valid 8.999645797302946e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.814881973899901e-07\n",
      "  batch 101 loss: 5.0159332658950007e-05\n",
      "LOSS train 4.703553167347594e-05 valid 4.639695180230774e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.430719712516293e-06\n",
      "  batch 101 loss: 2.501594723753442e-05\n",
      "LOSS train 2.19400943598327e-05 valid 2.1612640921375714e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.538089367793873e-07\n",
      "  batch 101 loss: 1.0706216955327363e-05\n",
      "LOSS train 8.868750437754998e-06 valid 1.7195869077113457e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.948681402718648e-07\n",
      "  batch 101 loss: 1.2491017996438813e-05\n",
      "LOSS train 1.0224801394366399e-05 valid 2.0096551452297717e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.271369395311922e-07\n",
      "  batch 101 loss: 9.39893874971176e-06\n",
      "LOSS train 7.933744244875899e-06 valid 1.791460636013653e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.35157193755731e-07\n",
      "  batch 101 loss: 1.2398367850323666e-05\n",
      "LOSS train 1.0327998213813067e-05 valid 2.206591670983471e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.978040230227635e-07\n",
      "  batch 101 loss: 9.679812596345982e-06\n",
      "LOSS train 8.259995970306956e-06 valid 2.086797212541569e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.05 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.350731123238802e-05\n",
      "  batch 101 loss: 7.162675751317238e-05\n",
      "LOSS train 8.467759913230151e-05 valid 0.0002419580123387277\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.040080476552248e-07\n",
      "  batch 101 loss: 8.283613414732826e-05\n",
      "LOSS train 8.413587737068886e-05 valid 0.0002600552688818425\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.028914557537064e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 8.349045927786846e-05\n",
      "LOSS train 8.473249215222182e-05 valid 0.00026572568458504975\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0685541201382875e-06\n",
      "  batch 101 loss: 8.358249860577871e-05\n",
      "LOSS train 8.481747875356945e-05 valid 0.00026821994106285274\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.0860618931474164e-06\n",
      "  batch 101 loss: 8.359412293373224e-05\n",
      "LOSS train 8.482635637699415e-05 valid 0.00026957711088471115\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0956062033073977e-06\n",
      "  batch 101 loss: 8.419897172757374e-05\n",
      "LOSS train 8.527024099659193e-05 valid 0.00027045802562497556\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1018083023373038e-06\n",
      "  batch 101 loss: 8.358560233659773e-05\n",
      "LOSS train 8.481361840040336e-05 valid 0.00027101245359517634\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1057133815484121e-06\n",
      "  batch 101 loss: 8.357661963202645e-05\n",
      "LOSS train 8.480413396891824e-05 valid 0.00027139612939208746\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.2990431170910595e-05\n",
      "  batch 101 loss: 0.007805943408557141\n",
      "LOSS train 0.005747665538861493 valid 4.2484050936764106e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8268736059544606e-07\n",
      "  batch 101 loss: 8.301740681417868e-05\n",
      "LOSS train 8.033088703372295e-05 valid 0.00010289065539836884\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.17837678064825e-07\n",
      "  batch 101 loss: 0.00022230391290520402\n",
      "LOSS train 0.00017162923652939136 valid 7.296604599105194e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.987531716935336e-08\n",
      "  batch 101 loss: 3.4348710817084796e-05\n",
      "LOSS train 3.292066638919195e-05 valid 6.374463555403054e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.022118552413303e-08\n",
      "  batch 101 loss: 2.536394498520167e-05\n",
      "LOSS train 2.3394936250832518e-05 valid 6.77164935041219e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4851939340587704e-07\n",
      "  batch 101 loss: 1.840903212610101e-05\n",
      "LOSS train 1.679027712745572e-05 valid 2.2540629288414493e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.177058948087506e-08\n",
      "  batch 101 loss: 1.056158576574262e-05\n",
      "LOSS train 9.818336009532083e-06 valid 9.886070984066464e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.5841300056490584e-09\n",
      "  batch 101 loss: 5.26026486312503e-06\n",
      "LOSS train 4.806640704896215e-06 valid 6.970905815251172e-06\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.05 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005871903896331787\n",
      "  batch 101 loss: 0.0037287083886621986\n",
      "LOSS train 0.003240747868323896 valid 5.29523313161917e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.856662951875478e-06\n",
      "  batch 101 loss: 0.00016238674721535062\n",
      "LOSS train 0.00014129626918866649 valid 4.4139633246231824e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.177985389716923e-07\n",
      "  batch 101 loss: 7.568057643766225e-05\n",
      "LOSS train 7.288980431652433e-05 valid 5.804567990708165e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.2960932660207616e-08\n",
      "  batch 101 loss: 0.00010113231340483253\n",
      "LOSS train 9.823808144438194e-05 valid 7.378833106486127e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3905653304391308e-08\n",
      "  batch 101 loss: 8.790069935685096e-05\n",
      "LOSS train 8.388547479025343e-05 valid 0.000104082930192817\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0352472600061446e-07\n",
      "  batch 101 loss: 7.341942575294524e-05\n",
      "LOSS train 7.061245153895172e-05 valid 0.0001280449505429715\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.072174356726464e-07\n",
      "  batch 101 loss: 6.473507615510243e-05\n",
      "LOSS train 6.273131256284946e-05 valid 0.00014535507943946868\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9930180971859955e-07\n",
      "  batch 101 loss: 5.992781000713876e-05\n",
      "LOSS train 5.8497523609830284e-05 valid 0.00016045761003624648\n"
     ]
    }
   ],
   "source": [
    "##Old loop\n",
    "\n",
    "learning_rates = [0.0025*4**i for i in range(2)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [8]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [2,3]\n",
    "dors = [0,0.05]#,0.1,0.2,0.4]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_nl\"\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            for dor in dors:\n",
    "                m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor)\n",
    "                m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor\"\n",
    "                optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                saved_models = dict()\n",
    "\n",
    "                for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                    path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                    model = m\n",
    "                    m.load_state_dict(torch.load(path))\n",
    "                    m.eval()\n",
    "\n",
    "                    test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                    test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "                    train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                    train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "                    validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                    validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "\n",
    "                    if mt == \"min_val\": \n",
    "                        min_val = True\n",
    "                    else: \n",
    "                        min_val = False\n",
    "\n",
    "                    r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                      \"Min_val\":min_val,\n",
    "                                      \"Epochs\": nb_e,\n",
    "                                      \"Lr\":lr,\n",
    "                                      \"Dor\": dor,\n",
    "                                      \"Tr_l\":train_loss.item(),\n",
    "                                      \"Te_l\":test_loss.item(),\n",
    "                                      \"V_l\": validation_loss.item()}\n",
    "                                     ,index = [i]\n",
    "                    )\n",
    "                    i+=1\n",
    "                    results = pd.concat([results,r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
