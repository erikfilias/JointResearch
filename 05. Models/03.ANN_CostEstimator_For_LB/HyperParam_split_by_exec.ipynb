{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c87f6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00015625, 0.000625, 0.0025, 0.01]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rates = [0.0025 * 4 ** i for i in range(-2, 2, 1)]\n",
    "learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb899fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataLoading\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import NN_classes\n",
    "import training_methods\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061f4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"../Data/RTS24_AC_12w\"\n",
    "all_executions = DataLoading.list_executions(folder=\"../Data/RTS24_AC_12w\",per = period,sc=sc)\n",
    "len(all_executions)\n",
    "executions_start = 0 \n",
    "executions_end = 5\n",
    "executions = all_executions[executions_start:executions_end]\n",
    "te_s = 0.3\n",
    "val_s = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa0d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_102_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_103_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_105_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_104_cac1_2030.csv\n",
      "1227\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6671c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te_by_exec(dfs_in,dfs_out,executions,te_s,val_s,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93163ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out,maxs = DataLoading.concat_and_normalize_split_by_exec(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f72c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "# training_loader = DataLoader(train,batch_size=64)\n",
    "# validation_loader = DataLoader(train,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7f15b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0, False, 32, 0.000625, 4, 0)\n",
      "Creating model ebjective estimator only\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ")\n",
      "EPOCH 1:\n",
      "LOSS train 0.00022529731097475022 valid 2.076899545500055e-05\n",
      "EPOCH 2:\n",
      "LOSS train 9.825708821589628e-06 valid 6.438444415834965e-06\n",
      "EPOCH 3:\n",
      "LOSS train 4.622206362593224e-06 valid 2.2204185370355844e-06\n",
      "EPOCH 4:\n",
      "LOSS train 3.538390321623775e-06 valid 2.04037723960937e-06\n",
      "(3, 0, False, 32, 0.0025, 4, 0)\n",
      "Creating model ebjective estimator only\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ")\n",
      "EPOCH 1:\n",
      "LOSS train 8.646395744138178e-05 valid 5.2024923206772655e-05\n",
      "EPOCH 2:\n",
      "LOSS train 2.799585175301609e-05 valid 6.665130058536306e-05\n",
      "EPOCH 3:\n",
      "LOSS train 2.7286462478517616e-05 valid 5.6802738981787115e-05\n",
      "EPOCH 4:\n",
      "LOSS train 2.557952579385475e-05 valid 4.677675315178931e-05\n",
      "(3, 0, False, 32, 0.01, 4, 0)\n",
      "Creating model ebjective estimator only\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ")\n",
      "EPOCH 1:\n",
      "LOSS train 0.025610406077150913 valid 4.8558766138739884e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.9775533639738297e-05 valid 1.0598533663142007e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.518340691564931e-05 valid 9.399540431331843e-06\n",
      "EPOCH 4:\n",
      "LOSS train 8.68372585029836e-06 valid 1.599330425960943e-05\n",
      "(3, 0, False, 32, 0.04, 4, 0)\n",
      "Creating model ebjective estimator only\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ")\n",
      "EPOCH 1:\n",
      "LOSS train 1.1357198367948287 valid 1.4300336260930635e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.0642319645398026e-05 valid 1.0876065971388016e-05\n",
      "EPOCH 3:\n",
      "LOSS train 5.3577572380810745e-06 valid 2.2434219317801762e-06\n",
      "EPOCH 4:\n",
      "LOSS train 5.228388924037607e-06 valid 2.8615977498702705e-06\n",
      "(3, 0, False, 64, 0.000625, 4, 0)\n",
      "Creating model ebjective estimator only\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ")\n",
      "EPOCH 1:\n",
      "LOSS train 0.001640027004009755 valid 4.276297477190383e-05\n",
      "EPOCH 2:\n",
      "LOSS train 1.6239743952583936e-05 valid 3.5230892535764724e-05\n",
      "EPOCH 3:\n",
      "LOSS train 1.3668314478388272e-05 valid 4.082409941474907e-05\n",
      "EPOCH 4:\n",
      "LOSS train 1.2238284673491064e-05 valid 4.079139034729451e-05\n",
      "(3, 0, False, 64, 0.0025, 4, 0)\n",
      "Creating model ebjective estimator only\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ")\n",
      "EPOCH 1:\n",
      "LOSS train 0.0027113040311428777 valid 7.828359230188653e-05\n",
      "EPOCH 2:\n",
      "LOSS train 4.0213381744844966e-05 valid 7.577763608423993e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File trained_models/RTS24_AC_12w_split_by_exec_test/min_val\\model_OE_3h_4e_0.0025lr_0dor_0np_False_ro_64bs.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#Train the actual model \u001b[39;00m\n\u001b[0;32m     37\u001b[0m t_start_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m---> 38\u001b[0m train_loss_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multiple_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolder_to_save\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m t_stop_train \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#In the following loop, we retreive the models from saved locations and calculate losses \u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive - KU Leuven\\JointResearch\\05. Models\\03.ANN_CostEstimator_For_LB\\training_methods.py:109\u001b[0m, in \u001b[0;36mtrain_multiple_epochs\u001b[1;34m(nb_epochs, model, training_loader, validation_loader, loss_fn, optimizer, model_name, folder)\u001b[0m\n\u001b[0;32m    107\u001b[0m             os\u001b[38;5;241m.\u001b[39mmakedirs(min_val_model_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m    108\u001b[0m             min_val_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(min_val_model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name))\n\u001b[1;32m--> 109\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     epoch_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# model_path = 'trained_models/{}/all_epochs/model_{}.pth'.format(folder,model_name)if folder is not None:\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File trained_models/RTS24_AC_12w_split_by_exec_test/min_val\\model_OE_3h_4e_0.0025lr_0dor_0np_False_ro_64bs.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "i=0\n",
    "nbs_hidden = [3]\n",
    "dors = [0]#,0.05,0.1]#,0.05]\n",
    "relu_outs=[False]\n",
    "\n",
    "batch_sizes = [32,64,128]\n",
    "learning_rates = [0.0025*4**i for i in range(-1,3,1)]\n",
    "nbs_e = [4]#,8,12,16,20] # ,8]\n",
    "negative_penalisations = [0]\n",
    "\n",
    "\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_test\"\n",
    "\n",
    "hp_sets = ((nb_h,dor,relu_out,bs,lr,nb_e,np) for nb_h in nbs_hidden for dor in dors for relu_out in relu_outs for bs in batch_sizes for lr in learning_rates for nb_e in nbs_e for np in negative_penalisations)\n",
    "\n",
    "\n",
    "for hp_set in hp_sets:\n",
    "    print(hp_set)\n",
    "    nb_hidden,dor,relu_out,bs,lr,nb_e,np = hp_set[0],hp_set[1],hp_set[2],hp_set[3],hp_set[4],hp_set[5],hp_set[6]\n",
    "    \n",
    "    #Create training and validation loaders based on batch size\n",
    "    training_loader = DataLoader(train,batch_size=bs)\n",
    "    validation_loader = DataLoader(validation,batch_size=bs)\n",
    "    \n",
    "    #Initialize loss functions\n",
    "    loss_fn = NN_classes.create_loss_fn(penalize_negative=np)\n",
    "    loss_t_mse = torch.nn.MSELoss()\n",
    "    \n",
    "    #Create model based on hyperparameter set\n",
    "    m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor,relu_out=relu_out)\n",
    "    #Create model name for saving and loading\n",
    "    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro_{bs}bs\"\n",
    "    #Create optimizer based on learning rate \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "    #Train the actual model \n",
    "    t_start_train = time.perf_counter()\n",
    "    train_loss_1 = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)[0]\n",
    "    t_stop_train = time.perf_counter()\n",
    "    \n",
    "    #In the following loop, we retreive the models from saved locations and calculate losses \n",
    "    for mt in [\"min_val\",\"all_epochs\"]:\n",
    "        t_start_eval = time.perf_counter()\n",
    "        path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "        \n",
    "        #Retreive model state and set to evaluation mode\n",
    "        m.load_state_dict(torch.load(path))\n",
    "        m.eval()\n",
    "        \n",
    "        #Calculate losses\n",
    "        test_predictions = m(d_ft_in[\"test\"].float())\n",
    "        test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "        test_loss_t_mse = loss_t_mse(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "\n",
    "        train_predictions = m(d_ft_in[\"train\"].float())\n",
    "        train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "        train_loss_t_mse = loss_t_mse(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "\n",
    "        validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "        validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "        validation_loss_t_mse = loss_t_mse(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "        t_stop_eval = time.perf_counter()\n",
    "        \n",
    "        \n",
    "        #Calculate some calculation times \n",
    "        t_train = t_stop_train - t_start_train\n",
    "        t_eval = t_stop_eval - t_start_eval\n",
    "        \n",
    "        #Finally, save all desired values in a dataframe\n",
    "        r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                        \"Dor\": dor,\n",
    "                        \"Relu_out\": relu_out,\n",
    "                        \"Batch_size\": bs,\n",
    "                        \"Lr\":lr,\n",
    "                        \"Epochs\": nb_e,\n",
    "                        \"Np\": np,\n",
    "                        \"Min_val\":mt,\n",
    "                        \"Tr_l\":train_loss.item(),\n",
    "                        \"Te_l\":test_loss.item(),\n",
    "                        \"V_l\": validation_loss.item(),\n",
    "                        \"Tr_l_t_mse\":train_loss_t_mse.item(),\n",
    "                        \"Te_l_t_mse\":test_loss_t_mse.item(),\n",
    "                        \"V_l_t_mse\": validation_loss_t_mse.item(),\n",
    "                        \"Tr_l_ret\": train_loss_1.item(),\n",
    "                        \"Train_time\": t_train,\n",
    "                        \"Eval_time\": t_eval\n",
    "                         }\n",
    "                        ,index = [i])\n",
    "        i+=1\n",
    "        results = pd.concat([results,r])\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d669c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/All_Exec_split_by_exec_penalize_test_bs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1039d995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Dor</th>\n",
       "      <th>Relu_out</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Np</th>\n",
       "      <th>Min_val</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "      <th>V_l</th>\n",
       "      <th>Tr_l_t_mse</th>\n",
       "      <th>Te_l_t_mse</th>\n",
       "      <th>V_l_t_mse</th>\n",
       "      <th>Tr_l_ret</th>\n",
       "      <th>Train_time</th>\n",
       "      <th>Eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.302867e-06</td>\n",
       "      <td>42.083591</td>\n",
       "      <td>0.517924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.303962e-06</td>\n",
       "      <td>2.959594e-06</td>\n",
       "      <td>2.868163e-06</td>\n",
       "      <td>3.302867e-06</td>\n",
       "      <td>42.083591</td>\n",
       "      <td>0.642527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.475169e-07</td>\n",
       "      <td>85.445304</td>\n",
       "      <td>0.652487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.476374e-07</td>\n",
       "      <td>2.540816e-07</td>\n",
       "      <td>2.362306e-07</td>\n",
       "      <td>2.475169e-07</td>\n",
       "      <td>85.445304</td>\n",
       "      <td>0.705018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>5.333926e-07</td>\n",
       "      <td>5.596858e-07</td>\n",
       "      <td>5.029923e-07</td>\n",
       "      <td>5.333926e-07</td>\n",
       "      <td>5.596858e-07</td>\n",
       "      <td>5.029923e-07</td>\n",
       "      <td>5.330584e-07</td>\n",
       "      <td>127.637386</td>\n",
       "      <td>0.707812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>1.214640e-05</td>\n",
       "      <td>5.728190e-06</td>\n",
       "      <td>4.182092e-06</td>\n",
       "      <td>1.214640e-05</td>\n",
       "      <td>5.728190e-06</td>\n",
       "      <td>4.182092e-06</td>\n",
       "      <td>1.209004e-05</td>\n",
       "      <td>46.640115</td>\n",
       "      <td>0.744612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>2.067072e-06</td>\n",
       "      <td>1.423654e-06</td>\n",
       "      <td>1.417735e-06</td>\n",
       "      <td>2.067072e-06</td>\n",
       "      <td>1.423654e-06</td>\n",
       "      <td>1.417735e-06</td>\n",
       "      <td>2.058935e-06</td>\n",
       "      <td>64.312629</td>\n",
       "      <td>0.788203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>7.431158e-05</td>\n",
       "      <td>7.501100e-05</td>\n",
       "      <td>7.485546e-05</td>\n",
       "      <td>7.431158e-05</td>\n",
       "      <td>7.501100e-05</td>\n",
       "      <td>7.485546e-05</td>\n",
       "      <td>2.058935e-06</td>\n",
       "      <td>64.312629</td>\n",
       "      <td>0.832030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>8.329134e-06</td>\n",
       "      <td>4.492334e-06</td>\n",
       "      <td>3.095181e-06</td>\n",
       "      <td>8.329134e-06</td>\n",
       "      <td>4.492334e-06</td>\n",
       "      <td>3.095181e-06</td>\n",
       "      <td>8.276066e-06</td>\n",
       "      <td>97.969096</td>\n",
       "      <td>0.845513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>256</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>3.661258e-05</td>\n",
       "      <td>3.276078e-05</td>\n",
       "      <td>3.129241e-05</td>\n",
       "      <td>3.661258e-05</td>\n",
       "      <td>3.276078e-05</td>\n",
       "      <td>3.129241e-05</td>\n",
       "      <td>8.276066e-06</td>\n",
       "      <td>97.969096</td>\n",
       "      <td>0.893772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Model_type  Dor  Relu_out  Batch_size        Lr  Epochs  Np     Min_val   \n",
       "0             3    0     False          32  0.000625       4   0     min_val  \\\n",
       "1             3    0     False          32  0.000625       4   0  all_epochs   \n",
       "2             3    0     False          32  0.000625       8   0     min_val   \n",
       "3             3    0     False          32  0.000625       8   0  all_epochs   \n",
       "4             3    0     False          32  0.000625      12   0     min_val   \n",
       "..          ...  ...       ...         ...       ...     ...  ..         ...   \n",
       "155           3    0     False         256  0.040000      12   0  all_epochs   \n",
       "156           3    0     False         256  0.040000      16   0     min_val   \n",
       "157           3    0     False         256  0.040000      16   0  all_epochs   \n",
       "158           3    0     False         256  0.040000      20   0     min_val   \n",
       "159           3    0     False         256  0.040000      20   0  all_epochs   \n",
       "\n",
       "             Tr_l          Te_l           V_l    Tr_l_t_mse    Te_l_t_mse   \n",
       "0    3.303962e-06  2.959594e-06  2.868163e-06  3.303962e-06  2.959594e-06  \\\n",
       "1    3.303962e-06  2.959594e-06  2.868163e-06  3.303962e-06  2.959594e-06   \n",
       "2    2.476374e-07  2.540816e-07  2.362306e-07  2.476374e-07  2.540816e-07   \n",
       "3    2.476374e-07  2.540816e-07  2.362306e-07  2.476374e-07  2.540816e-07   \n",
       "4    5.333926e-07  5.596858e-07  5.029923e-07  5.333926e-07  5.596858e-07   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "155  1.214640e-05  5.728190e-06  4.182092e-06  1.214640e-05  5.728190e-06   \n",
       "156  2.067072e-06  1.423654e-06  1.417735e-06  2.067072e-06  1.423654e-06   \n",
       "157  7.431158e-05  7.501100e-05  7.485546e-05  7.431158e-05  7.501100e-05   \n",
       "158  8.329134e-06  4.492334e-06  3.095181e-06  8.329134e-06  4.492334e-06   \n",
       "159  3.661258e-05  3.276078e-05  3.129241e-05  3.661258e-05  3.276078e-05   \n",
       "\n",
       "        V_l_t_mse      Tr_l_ret  Train_time  Eval_time  \n",
       "0    2.868163e-06  3.302867e-06   42.083591   0.517924  \n",
       "1    2.868163e-06  3.302867e-06   42.083591   0.642527  \n",
       "2    2.362306e-07  2.475169e-07   85.445304   0.652487  \n",
       "3    2.362306e-07  2.475169e-07   85.445304   0.705018  \n",
       "4    5.029923e-07  5.330584e-07  127.637386   0.707812  \n",
       "..            ...           ...         ...        ...  \n",
       "155  4.182092e-06  1.209004e-05   46.640115   0.744612  \n",
       "156  1.417735e-06  2.058935e-06   64.312629   0.788203  \n",
       "157  7.485546e-05  2.058935e-06   64.312629   0.832030  \n",
       "158  3.095181e-06  8.276066e-06   97.969096   0.845513  \n",
       "159  3.129241e-05  8.276066e-06   97.969096   0.893772  \n",
       "\n",
       "[160 rows x 17 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d94194d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00220871776342392\n",
      "  batch 101 loss: 0.16216120034456252\n",
      "  batch 201 loss: 0.1183598280698061\n",
      "  batch 301 loss: 0.08425441972911357\n",
      "  batch 401 loss: 0.05820752337574959\n",
      "  batch 501 loss: 0.03904968816787004\n",
      "LOSS train 0.0835415490914181 valid 0.021907396614551544\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00020583266392350198\n",
      "  batch 101 loss: 0.017425798047333955\n",
      "  batch 201 loss: 0.010554323750548066\n",
      "  batch 301 loss: 0.006139044179581106\n",
      "  batch 401 loss: 0.0033966082870028913\n",
      "  batch 501 loss: 0.001813527726335451\n",
      "LOSS train 0.006940791329586151 valid 0.0007492901058867574\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.876293824054301e-06\n",
      "  batch 101 loss: 0.0005462150709354319\n",
      "  batch 201 loss: 0.00027569316422159317\n",
      "  batch 301 loss: 0.00014765006807465397\n",
      "  batch 401 loss: 8.689398763571888e-05\n",
      "  batch 501 loss: 6.443287287368094e-05\n",
      "LOSS train 0.00020099731244344293 valid 5.401807356975041e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.811530849721749e-08\n",
      "  batch 101 loss: 5.455433858060133e-05\n",
      "  batch 201 loss: 5.296258362250228e-05\n",
      "  batch 301 loss: 5.254254743704223e-05\n",
      "  batch 401 loss: 5.150836962457106e-05\n",
      "  batch 501 loss: 5.1737077992584094e-05\n",
      "LOSS train 5.1943729378290486e-05 valid 5.075108492746949e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.1202889911364763e-07\n",
      "  batch 101 loss: 5.255351850792067e-05\n",
      "  batch 201 loss: 5.264064142465941e-05\n",
      "  batch 301 loss: 5.272099095236626e-05\n",
      "  batch 401 loss: 5.192854359847843e-05\n",
      "  batch 501 loss: 5.21048267637525e-05\n",
      "LOSS train 5.177909656321204e-05 valid 5.077867172076367e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.208011210313998e-07\n",
      "  batch 101 loss: 5.2895049011567605e-05\n",
      "  batch 201 loss: 5.300076869389159e-05\n",
      "  batch 301 loss: 5.309731618581281e-05\n",
      "  batch 401 loss: 5.234709327851306e-05\n",
      "  batch 501 loss: 5.2512613619910554e-05\n",
      "LOSS train 5.2164811822939416e-05 valid 5.0812552217394114e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2764234017813578e-07\n",
      "  batch 101 loss: 5.335097021998081e-05\n",
      "  batch 201 loss: 5.3476235016205465e-05\n",
      "  batch 301 loss: 5.35969935299363e-05\n",
      "  batch 401 loss: 5.290316927130334e-05\n",
      "  batch 501 loss: 5.3053210176585705e-05\n",
      "LOSS train 5.2677212399881406e-05 valid 5.087270255899057e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.3664906620979309e-07\n",
      "  batch 101 loss: 5.396236386332021e-05\n",
      "  batch 201 loss: 5.411209191152011e-05\n",
      "  batch 301 loss: 5.426733410786255e-05\n",
      "  batch 401 loss: 5.364694596210029e-05\n",
      "  batch 501 loss: 5.377556730309152e-05\n",
      "LOSS train 5.336391610859231e-05 valid 5.0974966143257916e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.4828503228491175e-07\n",
      "  batch 101 loss: 5.478019304064219e-05\n",
      "  batch 201 loss: 5.496159463291406e-05\n",
      "  batch 301 loss: 5.5164165760288595e-05\n",
      "  batch 401 loss: 5.463604336910066e-05\n",
      "  batch 501 loss: 5.473946947859076e-05\n",
      "LOSS train 5.428044206694279e-05 valid 5.1143364544259384e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.6317864467273465e-07\n",
      "  batch 101 loss: 5.5861956598164396e-05\n",
      "  batch 201 loss: 5.608424118690891e-05\n",
      "  batch 301 loss: 5.6346601668337825e-05\n",
      "  batch 401 loss: 5.592833083937876e-05\n",
      "  batch 501 loss: 5.6004369907896036e-05\n",
      "LOSS train 5.548419173219169e-05 valid 5.1412695029284805e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.821020279952791e-07\n",
      "  batch 101 loss: 5.725649378291564e-05\n",
      "  batch 201 loss: 5.752464186116413e-05\n",
      "  batch 301 loss: 5.7853364260154195e-05\n",
      "  batch 401 loss: 5.755366678386054e-05\n",
      "  batch 501 loss: 5.7593957244534977e-05\n",
      "LOSS train 5.700890743134837e-05 valid 5.1819275540765375e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.053334148717113e-07\n",
      "  batch 101 loss: 5.896270398807246e-05\n",
      "  batch 201 loss: 5.92657490142301e-05\n",
      "  batch 301 loss: 5.96514393146208e-05\n",
      "  batch 401 loss: 5.94568910128146e-05\n",
      "  batch 501 loss: 5.943518297499395e-05\n",
      "LOSS train 5.881227343472187e-05 valid 5.236922515905462e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 3.3163727493956683e-07\n",
      "  batch 101 loss: 6.086515668357606e-05\n",
      "  batch 201 loss: 6.116743819802651e-05\n",
      "  batch 301 loss: 6.157467919365445e-05\n",
      "  batch 401 loss: 6.144086119093117e-05\n",
      "  batch 501 loss: 6.131383835963788e-05\n",
      "LOSS train 6.0719989968316306e-05 valid 5.299939584801905e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 3.577905590645969e-07\n",
      "  batch 101 loss: 6.271059531172796e-05\n",
      "  batch 201 loss: 6.29618408947863e-05\n",
      "  batch 301 loss: 6.333904713756055e-05\n",
      "  batch 401 loss: 6.320603598396702e-05\n",
      "  batch 501 loss: 6.293956968875136e-05\n",
      "LOSS train 6.24520392533208e-05 valid 5.358645648811944e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 3.7977282772772015e-07\n",
      "  batch 101 loss: 6.421880390917068e-05\n",
      "  batch 201 loss: 6.438523337237712e-05\n",
      "  batch 301 loss: 6.46973610855639e-05\n",
      "  batch 401 loss: 6.45247519514669e-05\n",
      "  batch 501 loss: 6.412140648535569e-05\n",
      "LOSS train 6.377652269343414e-05 valid 5.403011164162308e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 3.9530237700091675e-07\n",
      "  batch 101 loss: 6.525926164158591e-05\n",
      "  batch 201 loss: 6.534210750032799e-05\n",
      "  batch 301 loss: 6.558778242379049e-05\n",
      "  batch 401 loss: 6.536873424920487e-05\n",
      "  batch 501 loss: 6.486173814664654e-05\n",
      "LOSS train 6.4642422476872e-05 valid 5.43128298886586e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.6441522297536724e-08\n",
      "  batch 101 loss: 8.138549323348343e-05\n",
      "  batch 201 loss: 6.281209279165978e-05\n",
      "  batch 301 loss: 4.282480228710028e-05\n",
      "  batch 401 loss: 6.949773828637262e-05\n",
      "  batch 501 loss: 7.509535906137899e-05\n",
      "LOSS train 6.606333077479341e-05 valid 5.5898686696309596e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.692129004979506e-07\n",
      "  batch 101 loss: 7.039112063011999e-05\n",
      "  batch 201 loss: 7.189974750872352e-05\n",
      "  batch 301 loss: 8.212376491428586e-05\n",
      "  batch 401 loss: 7.326809649384814e-05\n",
      "  batch 501 loss: 6.59462705812075e-05\n",
      "LOSS train 7.208330825005838e-05 valid 5.6018172472249717e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.575110506266355e-07\n",
      "  batch 101 loss: 6.98494387415849e-05\n",
      "  batch 201 loss: 6.931801423434081e-05\n",
      "  batch 301 loss: 6.907657892497809e-05\n",
      "  batch 401 loss: 6.850456848496833e-05\n",
      "  batch 501 loss: 6.746815072801838e-05\n",
      "LOSS train 6.802597554019133e-05 valid 5.535698073799722e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.3788240873254835e-07\n",
      "  batch 101 loss: 6.780754644751141e-05\n",
      "  batch 201 loss: 6.750717529939721e-05\n",
      "  batch 301 loss: 6.74364557107765e-05\n",
      "  batch 401 loss: 6.697000940221187e-05\n",
      "  batch 501 loss: 6.613957205445331e-05\n",
      "LOSS train 6.642061517515885e-05 valid 5.478229286381975e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.200426337774843e-07\n",
      "  batch 101 loss: 6.675886350876681e-05\n",
      "  batch 201 loss: 6.660541561359423e-05\n",
      "  batch 301 loss: 6.666552501883417e-05\n",
      "  batch 401 loss: 6.630807810324768e-05\n",
      "  batch 501 loss: 6.562408495938143e-05\n",
      "LOSS train 6.569042802148158e-05 valid 5.4590269428445026e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 4.13890193158295e-07\n",
      "  batch 101 loss: 6.645001392826089e-05\n",
      "  batch 201 loss: 6.638833968281688e-05\n",
      "  batch 301 loss: 6.65242372906505e-05\n",
      "  batch 401 loss: 6.622823508223519e-05\n",
      "  batch 501 loss: 6.559753473084129e-05\n",
      "LOSS train 6.555762447861931e-05 valid 5.4594660468865186e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 4.1403211071155965e-07\n",
      "  batch 101 loss: 6.648667723311518e-05\n",
      "  batch 201 loss: 6.64423116359103e-05\n",
      "  batch 301 loss: 6.658756076831196e-05\n",
      "  batch 401 loss: 6.629605129091942e-05\n",
      "  batch 501 loss: 6.566030014255375e-05\n",
      "LOSS train 6.561470922743786e-05 valid 5.462032640934922e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.148591324337758e-07\n",
      "  batch 101 loss: 6.654052171597868e-05\n",
      "  batch 201 loss: 6.649023792306253e-05\n",
      "  batch 301 loss: 6.663055174158217e-05\n",
      "  batch 401 loss: 6.633538886489987e-05\n",
      "  batch 501 loss: 6.569358001343062e-05\n",
      "LOSS train 6.565636035835542e-05 valid 5.463324851007201e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.1527498979121446e-07\n",
      "  batch 101 loss: 6.6567163325999e-05\n",
      "  batch 201 loss: 6.651369213614089e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 6.665150314347557e-05\n",
      "  batch 401 loss: 6.635455751165865e-05\n",
      "  batch 501 loss: 6.570979028765578e-05\n",
      "LOSS train 6.567674546578346e-05 valid 5.4639582231175154e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 4.154782072873786e-07\n",
      "  batch 101 loss: 6.658015587163391e-05\n",
      "  batch 201 loss: 6.6525141164675e-05\n",
      "  batch 301 loss: 6.666174095698807e-05\n",
      "  batch 401 loss: 6.636392460677598e-05\n",
      "  batch 501 loss: 6.571772140887333e-05\n",
      "LOSS train 6.568670473254108e-05 valid 5.464264904730953e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.155778879066929e-07\n",
      "  batch 101 loss: 6.658651874204224e-05\n",
      "  batch 201 loss: 6.653074269252101e-05\n",
      "  batch 301 loss: 6.666675071755889e-05\n",
      "  batch 401 loss: 6.636851582698e-05\n",
      "  batch 501 loss: 6.572160506948421e-05\n",
      "LOSS train 6.56915805305065e-05 valid 5.464418063638732e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.156267823418602e-07\n",
      "  batch 101 loss: 6.658963744484936e-05\n",
      "  batch 201 loss: 6.653349406860797e-05\n",
      "  batch 301 loss: 6.666921227406419e-05\n",
      "  batch 401 loss: 6.637077602135833e-05\n",
      "  batch 501 loss: 6.572351514478215e-05\n",
      "LOSS train 6.569397605893311e-05 valid 5.464489368023351e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 4.156507202424109e-07\n",
      "  batch 101 loss: 6.65911649912232e-05\n",
      "  batch 201 loss: 6.653484596427007e-05\n",
      "  batch 301 loss: 6.667042311619297e-05\n",
      "  batch 401 loss: 6.637188154854811e-05\n",
      "  batch 501 loss: 6.572444579887816e-05\n",
      "LOSS train 6.569514992073337e-05 valid 5.464530840981752e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 4.1566265281289814e-07\n",
      "  batch 101 loss: 6.659192077677289e-05\n",
      "  batch 201 loss: 6.653550773080496e-05\n",
      "  batch 301 loss: 6.667102259598323e-05\n",
      "  batch 401 loss: 6.637243714976648e-05\n",
      "  batch 501 loss: 6.572492098257499e-05\n",
      "LOSS train 6.56957345614601e-05 valid 5.4645461204927415e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 4.1566854633856567e-07\n",
      "  batch 101 loss: 6.659229590241012e-05\n",
      "  batch 201 loss: 6.653583855950273e-05\n",
      "  batch 301 loss: 6.667131496215007e-05\n",
      "  batch 401 loss: 6.637269911152543e-05\n",
      "  batch 501 loss: 6.572514402250818e-05\n",
      "LOSS train 6.569601897109804e-05 valid 5.464557034429163e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 4.1567142034182325e-07\n",
      "  batch 101 loss: 6.659247850166139e-05\n",
      "  batch 201 loss: 6.65360073071497e-05\n",
      "  batch 301 loss: 6.667146704330662e-05\n",
      "  batch 401 loss: 6.637284132921195e-05\n",
      "  batch 501 loss: 6.572525763658633e-05\n",
      "LOSS train 6.569616389226929e-05 valid 5.464557762024924e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0015540061891078948\n",
      "  batch 101 loss: 0.00283241987492147\n",
      "  batch 201 loss: 5.118737945849716e-05\n",
      "  batch 301 loss: 1.7965404158530873e-05\n",
      "  batch 401 loss: 1.3972431900697302e-05\n",
      "  batch 501 loss: 2.291816829043114e-06\n",
      "LOSS train 0.0007699120739329689 valid 3.175233359797858e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.225433440296911e-07\n",
      "  batch 101 loss: 3.04310435996058e-05\n",
      "  batch 201 loss: 7.037920711923107e-06\n",
      "  batch 301 loss: 3.519603377526437e-06\n",
      "  batch 401 loss: 4.061187272554889e-06\n",
      "  batch 501 loss: 2.6634798024360862e-06\n",
      "LOSS train 8.527422773727385e-06 valid 3.091480175498873e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.43010576418601e-07\n",
      "  batch 101 loss: 2.6342253672524406e-05\n",
      "  batch 201 loss: 2.9793827991397848e-06\n",
      "  batch 301 loss: 3.523557633542396e-06\n",
      "  batch 401 loss: 2.8988423107989546e-06\n",
      "  batch 501 loss: 2.7642661541449343e-06\n",
      "LOSS train 6.973236158523891e-06 valid 2.4682203729753383e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.424075061455369e-07\n",
      "  batch 101 loss: 3.4013175490485995e-05\n",
      "  batch 201 loss: 3.0484413217379823e-06\n",
      "  batch 301 loss: 2.896149196232045e-06\n",
      "  batch 401 loss: 3.3654591345566586e-06\n",
      "  batch 501 loss: 3.0008626475819257e-06\n",
      "LOSS train 8.382953720690134e-06 valid 3.1054547434905544e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.7302950406447053e-07\n",
      "  batch 101 loss: 4.159798429896e-05\n",
      "  batch 201 loss: 3.9613705513374955e-06\n",
      "  batch 301 loss: 2.6939196541775344e-06\n",
      "  batch 401 loss: 4.272485612659693e-06\n",
      "  batch 501 loss: 3.1403288529929796e-06\n",
      "LOSS train 9.959180842807935e-06 valid 2.552056685090065e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.974116356810555e-07\n",
      "  batch 101 loss: 3.954077238773834e-05\n",
      "  batch 201 loss: 5.137620355810668e-06\n",
      "  batch 301 loss: 4.3659156565922785e-06\n",
      "  batch 401 loss: 4.955362062730728e-06\n",
      "  batch 501 loss: 3.4413604232952366e-06\n",
      "LOSS train 1.0314180204191103e-05 valid 2.01592811208684e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.610533455386758e-07\n",
      "  batch 101 loss: 3.951398959372909e-05\n",
      "  batch 201 loss: 3.410241581605078e-06\n",
      "  batch 301 loss: 2.5345754565364587e-06\n",
      "  batch 401 loss: 4.570774194405658e-06\n",
      "  batch 501 loss: 4.574808718729173e-06\n",
      "LOSS train 9.890537213434062e-06 valid 1.7059313904610462e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.335932943737135e-07\n",
      "  batch 101 loss: 4.192692444007662e-05\n",
      "  batch 201 loss: 3.4189187022093394e-06\n",
      "  batch 301 loss: 2.5338981444633648e-06\n",
      "  batch 401 loss: 4.06424412318529e-06\n",
      "  batch 501 loss: 4.658038793223795e-06\n",
      "LOSS train 1.0271828813840013e-05 valid 1.616805457160808e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 8.770985004957765e-07\n",
      "  batch 101 loss: 4.633077261502194e-05\n",
      "  batch 201 loss: 4.1709693820735086e-06\n",
      "  batch 301 loss: 2.483355900153583e-06\n",
      "  batch 401 loss: 3.5869683756573068e-06\n",
      "  batch 501 loss: 4.848132997778976e-06\n",
      "LOSS train 1.107146006383652e-05 valid 1.5851459465920925e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 8.250029350165277e-07\n",
      "  batch 101 loss: 4.0720870631218984e-05\n",
      "  batch 201 loss: 3.897269614867582e-06\n",
      "  batch 301 loss: 2.628290601762728e-06\n",
      "  batch 401 loss: 3.2380623706274037e-06\n",
      "  batch 501 loss: 4.881997651295933e-06\n",
      "LOSS train 9.995768268979101e-06 valid 1.554523259983398e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.92007049312815e-07\n",
      "  batch 101 loss: 3.96304453323637e-05\n",
      "  batch 201 loss: 3.3094094591490377e-06\n",
      "  batch 301 loss: 2.9570759699026894e-06\n",
      "  batch 401 loss: 3.0415118749260727e-06\n",
      "  batch 501 loss: 4.345293456040622e-06\n",
      "LOSS train 9.602870342975857e-06 valid 1.4588299563911278e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.967330020619556e-07\n",
      "  batch 101 loss: 0.00024029937566524495\n",
      "  batch 201 loss: 2.548346532080359e-05\n",
      "  batch 301 loss: 5.844568814268314e-06\n",
      "  batch 401 loss: 5.322872146322766e-06\n",
      "  batch 501 loss: 2.698988603668795e-06\n",
      "LOSS train 4.847895616056267e-05 valid 1.5530338714597747e-05\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 6.556398875545711e-07\n",
      "  batch 101 loss: 2.4063135365111064e-05\n",
      "  batch 201 loss: 2.234275157633192e-06\n",
      "  batch 301 loss: 3.1451452176156636e-06\n",
      "  batch 401 loss: 3.621661122181763e-06\n",
      "  batch 501 loss: 2.3607611159093267e-06\n",
      "LOSS train 6.424771062447967e-06 valid 1.5747436918900348e-05\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 6.230724102351814e-07\n",
      "  batch 101 loss: 3.4679528991432565e-05\n",
      "  batch 201 loss: 2.1588319231113927e-06\n",
      "  batch 301 loss: 2.7080079139807366e-06\n",
      "  batch 401 loss: 3.6742341752926676e-06\n",
      "  batch 501 loss: 2.5808646427094574e-06\n",
      "LOSS train 8.228896439169757e-06 valid 1.3478171240421943e-05\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 7.121364615159109e-07\n",
      "  batch 101 loss: 2.979538918111757e-05\n",
      "  batch 201 loss: 2.807785666334439e-06\n",
      "  batch 301 loss: 2.76715791613924e-06\n",
      "  batch 401 loss: 3.6393635386389177e-06\n",
      "  batch 501 loss: 3.2332020973058205e-06\n",
      "LOSS train 7.634340732920464e-06 valid 1.2681684893323109e-05\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 7.165557326516136e-07\n",
      "  batch 101 loss: 2.3415028405224802e-05\n",
      "  batch 201 loss: 2.6415410286517726e-06\n",
      "  batch 301 loss: 2.762534386988591e-06\n",
      "  batch 401 loss: 3.351996131044643e-06\n",
      "  batch 501 loss: 2.5742228282865653e-06\n",
      "LOSS train 6.35447255620975e-06 valid 1.4642630048911087e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002648560330271721\n",
      "  batch 101 loss: 0.0004034156311308834\n",
      "  batch 201 loss: 3.196832052822174e-05\n",
      "  batch 301 loss: 2.1348583704821067e-05\n",
      "  batch 401 loss: 8.583994921309568e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 501 loss: 2.6321989221855803e-06\n",
      "LOSS train 0.00012633684148409592 valid 2.454861942169373e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.595113856950775e-08\n",
      "  batch 101 loss: 4.002243027372288e-06\n",
      "  batch 201 loss: 1.423041800308056e-06\n",
      "  batch 301 loss: 1.1373110351087235e-06\n",
      "  batch 401 loss: 1.931201853011544e-06\n",
      "  batch 501 loss: 1.743991708451631e-06\n",
      "LOSS train 2.1840234116734675e-06 valid 2.8916783776367083e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.973846105509438e-08\n",
      "  batch 101 loss: 4.116663947399957e-06\n",
      "  batch 201 loss: 1.3503300545636422e-06\n",
      "  batch 301 loss: 3.3149252671194063e-06\n",
      "  batch 401 loss: 1.360691420160265e-06\n",
      "  batch 501 loss: 1.2521881053828566e-06\n",
      "LOSS train 2.299007544967255e-06 valid 1.7874917830340564e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.714941602898761e-08\n",
      "  batch 101 loss: 4.848460049799996e-06\n",
      "  batch 201 loss: 3.0316521821305287e-06\n",
      "  batch 301 loss: 1.7403401980686796e-06\n",
      "  batch 401 loss: 2.0549777966039075e-06\n",
      "  batch 501 loss: 1.3757053494600768e-06\n",
      "LOSS train 2.464829638360855e-06 valid 2.556132812969736e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.701586476585362e-08\n",
      "  batch 101 loss: 6.426224763060873e-06\n",
      "  batch 201 loss: 1.995711005662315e-06\n",
      "  batch 301 loss: 2.041680877340468e-06\n",
      "  batch 401 loss: 2.816984614071316e-06\n",
      "  batch 501 loss: 2.120934514948658e-06\n",
      "LOSS train 2.77075364234522e-06 valid 1.3853986047251965e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.811307114025112e-08\n",
      "  batch 101 loss: 5.283123647998877e-06\n",
      "  batch 201 loss: 2.4253216083991447e-06\n",
      "  batch 301 loss: 5.145207134091834e-06\n",
      "  batch 401 loss: 3.0428077576516445e-06\n",
      "  batch 501 loss: 4.070014912826991e-06\n",
      "LOSS train 3.703179328846719e-06 valid 1.8092536038238904e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.420879228448029e-08\n",
      "  batch 101 loss: 3.842693952549325e-06\n",
      "  batch 201 loss: 2.049256834766311e-06\n",
      "  batch 301 loss: 2.118517578111323e-06\n",
      "  batch 401 loss: 2.790546661941562e-06\n",
      "  batch 501 loss: 1.8218735110053785e-06\n",
      "LOSS train 2.404373902854966e-06 valid 2.566574721640791e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2037761734973173e-07\n",
      "  batch 101 loss: 4.10945798861917e-06\n",
      "  batch 201 loss: 1.2032581547316567e-06\n",
      "  batch 301 loss: 1.933073099280591e-06\n",
      "  batch 401 loss: 1.4850603929517092e-06\n",
      "  batch 501 loss: 1.4401624083859588e-06\n",
      "LOSS train 2.110616840937569e-06 valid 1.290189970859501e-06\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 5.558424618357094e-08\n",
      "  batch 101 loss: 3.2543911612492595e-06\n",
      "  batch 201 loss: 2.4786521169062325e-06\n",
      "  batch 301 loss: 2.9778297974303314e-06\n",
      "  batch 401 loss: 1.8423113324672612e-06\n",
      "  batch 501 loss: 1.589338677234764e-06\n",
      "LOSS train 2.2995111994149113e-06 valid 1.8012482314588851e-06\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.11372469291382e-08\n",
      "  batch 101 loss: 1.7437201582026774e-06\n",
      "  batch 201 loss: 1.5578248465430987e-06\n",
      "  batch 301 loss: 1.4893368752666446e-06\n",
      "  batch 401 loss: 1.7361436707119536e-06\n",
      "  batch 501 loss: 1.51453698123305e-06\n",
      "LOSS train 1.5707837188161608e-06 valid 1.4669030861114152e-06\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.0699639005906648e-08\n",
      "  batch 101 loss: 2.39438862472241e-06\n",
      "  batch 201 loss: 1.6450056158134884e-06\n",
      "  batch 301 loss: 1.999502696037325e-06\n",
      "  batch 401 loss: 1.8853353770964532e-06\n",
      "  batch 501 loss: 1.707150080392239e-06\n",
      "LOSS train 1.8952090865462105e-06 valid 1.0174694580200594e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.677836280578049e-08\n",
      "  batch 101 loss: 1.2326744813151436e-06\n",
      "  batch 201 loss: 1.003344137870954e-06\n",
      "  batch 301 loss: 9.325857968178752e-07\n",
      "  batch 401 loss: 1.1626868796099644e-06\n",
      "  batch 501 loss: 7.910215017403743e-07\n",
      "LOSS train 9.83155031314342e-07 valid 9.796990525501315e-07\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.3900305475544883e-08\n",
      "  batch 101 loss: 1.1943467038122435e-06\n",
      "  batch 201 loss: 1.0507607605347858e-06\n",
      "  batch 301 loss: 8.599105377271599e-07\n",
      "  batch 401 loss: 1.2782777038466975e-06\n",
      "  batch 501 loss: 9.387768263025009e-07\n",
      "LOSS train 1.0137928737629012e-06 valid 8.52457787914318e-07\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 1.1539858633113908e-08\n",
      "  batch 101 loss: 1.5025863248041559e-06\n",
      "  batch 201 loss: 8.836975439407979e-07\n",
      "  batch 301 loss: 1.1944459777168958e-06\n",
      "  batch 401 loss: 1.089134250022994e-06\n",
      "  batch 501 loss: 1.2438869235609217e-06\n",
      "LOSS train 1.1119906012231538e-06 valid 4.914833766633819e-07\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 1.0563873047431116e-08\n",
      "  batch 101 loss: 5.977340196317016e-07\n",
      "  batch 201 loss: 8.078080945495003e-07\n",
      "  batch 301 loss: 1.3244477447216242e-06\n",
      "  batch 401 loss: 8.794355041175095e-07\n",
      "  batch 501 loss: 1.1511669630692723e-06\n",
      "LOSS train 9.379361579227955e-07 valid 8.209287898353068e-07\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 9.229029842572345e-09\n",
      "  batch 101 loss: 8.12150967846037e-07\n",
      "  batch 201 loss: 1.0881596366374424e-06\n",
      "  batch 301 loss: 1.3055770434533542e-06\n",
      "  batch 401 loss: 9.470729253280296e-07\n",
      "  batch 501 loss: 1.1344880195451878e-06\n",
      "LOSS train 1.01401482689615e-06 valid 5.076984166407783e-07\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0019672051072120666\n",
      "  batch 101 loss: 0.0035839258258795327\n",
      "  batch 201 loss: 4.49724816746766e-06\n",
      "  batch 301 loss: 4.87203961156979e-06\n",
      "  batch 401 loss: 1.0601620298018588e-05\n",
      "  batch 501 loss: 4.524480795566887e-06\n",
      "LOSS train 0.0009599113112842429 valid 1.2051367775711697e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.6523575292667374e-07\n",
      "  batch 101 loss: 1.0219012714003383e-05\n",
      "  batch 201 loss: 2.3599963648734957e-06\n",
      "  batch 301 loss: 2.1135621391010772e-06\n",
      "  batch 401 loss: 5.671072405277755e-06\n",
      "  batch 501 loss: 4.087123268590176e-06\n",
      "LOSS train 4.509261220805292e-06 valid 1.3841709005646408e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.2272855353076013e-07\n",
      "  batch 101 loss: 8.44160504499314e-06\n",
      "  batch 201 loss: 2.400894831282585e-06\n",
      "  batch 301 loss: 3.0221251898865378e-06\n",
      "  batch 401 loss: 6.207920922349785e-06\n",
      "  batch 501 loss: 4.142309607573224e-06\n",
      "LOSS train 4.502117540969818e-06 valid 1.1848530448332895e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.775960405822844e-07\n",
      "  batch 101 loss: 1.5082058694133593e-05\n",
      "  batch 201 loss: 2.554750530094907e-06\n",
      "  batch 301 loss: 3.3129692220512652e-06\n",
      "  batch 401 loss: 8.557986510595584e-06\n",
      "  batch 501 loss: 4.942459669052823e-06\n",
      "LOSS train 6.6240904865538175e-06 valid 1.0629559255903587e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.449486292898655e-07\n",
      "  batch 101 loss: 2.1227980520279745e-05\n",
      "  batch 201 loss: 6.198245953044079e-06\n",
      "  batch 301 loss: 6.162028381737628e-06\n",
      "  batch 401 loss: 8.590358454796387e-06\n",
      "  batch 501 loss: 1.6203625808941523e-05\n",
      "LOSS train 1.2489982361819466e-05 valid 1.844410871854052e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.278386334888637e-06\n",
      "  batch 101 loss: 2.458161208551246e-05\n",
      "  batch 201 loss: 1.4912609088639784e-05\n",
      "  batch 301 loss: 1.1111799262124578e-05\n",
      "  batch 401 loss: 1.3072712476969173e-05\n",
      "  batch 501 loss: 1.051060627219158e-05\n",
      "LOSS train 1.4282690220487668e-05 valid 1.7519447283120826e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0021546768257394e-06\n",
      "  batch 101 loss: 2.7886302885349323e-05\n",
      "  batch 201 loss: 1.2002540766502533e-05\n",
      "  batch 301 loss: 1.5954466426819636e-05\n",
      "  batch 401 loss: 1.2231704328655724e-05\n",
      "  batch 501 loss: 1.444090117217911e-05\n",
      "LOSS train 1.5884213559331293e-05 valid 1.8265523976879194e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.040259812725708e-06\n",
      "  batch 101 loss: 2.0687577024602887e-05\n",
      "  batch 201 loss: 1.1293736223478845e-05\n",
      "  batch 301 loss: 1.6387995121931453e-05\n",
      "  batch 401 loss: 1.6151492104938825e-05\n",
      "  batch 501 loss: 1.3283370959698004e-05\n",
      "LOSS train 1.4460463255224714e-05 valid 2.887823575292714e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 7.90407966633211e-09\n",
      "  batch 101 loss: 9.79354912033159e-06\n",
      "  batch 201 loss: 1.5701054176133768e-05\n",
      "  batch 301 loss: 5.949715072688377e-06\n",
      "  batch 401 loss: 1.5964318477017516e-05\n",
      "  batch 501 loss: 1.0674904876850632e-05\n",
      "LOSS train 1.0768203293534878e-05 valid 2.1644935259246267e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.0495097058083048e-08\n",
      "  batch 101 loss: 1.0035756390891493e-05\n",
      "  batch 201 loss: 1.1899799899879326e-05\n",
      "  batch 301 loss: 5.999845974429263e-06\n",
      "  batch 401 loss: 1.8254704784226305e-05\n",
      "  batch 501 loss: 6.982689874917014e-06\n",
      "LOSS train 9.807321977870395e-06 valid 8.16478222986916e-06\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 6.149878572614398e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 1.10278523427354e-05\n",
      "  batch 201 loss: 5.794633508173774e-06\n",
      "  batch 301 loss: 5.630273479937387e-06\n",
      "  batch 401 loss: 1.1395645855714066e-05\n",
      "  batch 501 loss: 5.643354879509843e-06\n",
      "LOSS train 7.427067833182455e-06 valid 6.883709829708096e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.190307663520798e-08\n",
      "  batch 101 loss: 8.977627167539027e-06\n",
      "  batch 201 loss: 5.449104569095198e-06\n",
      "  batch 301 loss: 4.885684838598081e-06\n",
      "  batch 401 loss: 1.0483772358043098e-05\n",
      "  batch 501 loss: 5.800544033149891e-06\n",
      "LOSS train 6.744180299765468e-06 valid 7.639236173417885e-06\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 1.3157741705072113e-08\n",
      "  batch 101 loss: 6.7807615019432884e-06\n",
      "  batch 201 loss: 4.09711113462663e-06\n",
      "  batch 301 loss: 4.5651659024770194e-06\n",
      "  batch 401 loss: 7.857106729147744e-06\n",
      "  batch 501 loss: 5.5235532408914875e-06\n",
      "LOSS train 5.51727438254768e-06 valid 2.9986292702233186e-06\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 4.692155016527977e-08\n",
      "  batch 101 loss: 5.813234492961783e-06\n",
      "  batch 201 loss: 3.691241552132851e-06\n",
      "  batch 301 loss: 3.807425546256127e-06\n",
      "  batch 401 loss: 4.831734209744809e-06\n",
      "  batch 501 loss: 3.715324723430058e-06\n",
      "LOSS train 4.2109574368910975e-06 valid 2.554924321884755e-06\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 3.162634129694197e-08\n",
      "  batch 101 loss: 4.724649632521505e-06\n",
      "  batch 201 loss: 3.4257293672013135e-06\n",
      "  batch 301 loss: 3.2871884096152824e-06\n",
      "  batch 401 loss: 4.447401875609102e-06\n",
      "  batch 501 loss: 3.1763684530972114e-06\n",
      "LOSS train 3.68515456410532e-06 valid 3.0806015729467617e-06\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 5.990149247736554e-09\n",
      "  batch 101 loss: 3.272142672017253e-06\n",
      "  batch 201 loss: 3.316970971702915e-06\n",
      "  batch 301 loss: 3.041857664811687e-06\n",
      "  batch 401 loss: 4.957879612277339e-06\n",
      "  batch 501 loss: 3.9214846992763345e-06\n",
      "LOSS train 3.531545419222323e-06 valid 6.517738711409038e-06\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00045495521277189255\n",
      "  batch 101 loss: 0.01858087623069082\n",
      "  batch 201 loss: 3.8617798294353635e-05\n",
      "  batch 301 loss: 1.911413151674424e-05\n",
      "  batch 401 loss: 1.3657978162200379e-05\n",
      "  batch 501 loss: 4.686122356076794e-06\n",
      "LOSS train 0.0032897713446854828 valid 6.726071660523303e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2246665392012802e-07\n",
      "  batch 101 loss: 5.793344370630393e-06\n",
      "  batch 201 loss: 1.6997515703565114e-06\n",
      "  batch 301 loss: 1.8183122358550463e-06\n",
      "  batch 401 loss: 7.217977666158504e-06\n",
      "  batch 501 loss: 2.2470126351947782e-06\n",
      "LOSS train 3.4597943291714534e-06 valid 4.423277005116688e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.54155223350972e-08\n",
      "  batch 101 loss: 5.746389335854474e-06\n",
      "  batch 201 loss: 1.4145326481695974e-06\n",
      "  batch 301 loss: 2.160083039939309e-06\n",
      "  batch 401 loss: 4.640494557577313e-06\n",
      "  batch 501 loss: 2.2117221777762098e-06\n",
      "LOSS train 2.9886719691375346e-06 valid 4.881403128820239e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.0114064682275058e-07\n",
      "  batch 101 loss: 5.753035622717562e-06\n",
      "  batch 201 loss: 1.211677168839742e-06\n",
      "  batch 301 loss: 4.937321009634843e-06\n",
      "  batch 401 loss: 4.006765727098127e-06\n",
      "  batch 501 loss: 2.31554612824425e-06\n",
      "LOSS train 3.4540012216470713e-06 valid 9.536172910884488e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.614193701650947e-07\n",
      "  batch 101 loss: 1.0555380003722802e-05\n",
      "  batch 201 loss: 1.083634174534609e-06\n",
      "  batch 301 loss: 2.997871321213097e-06\n",
      "  batch 401 loss: 2.7614650767304737e-06\n",
      "  batch 501 loss: 2.280467958541976e-06\n",
      "LOSS train 4.017385285668135e-06 valid 1.020179388433462e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.854098031297326e-07\n",
      "  batch 101 loss: 5.728188168916404e-06\n",
      "  batch 201 loss: 1.2359230252911857e-06\n",
      "  batch 301 loss: 3.781039481864923e-06\n",
      "  batch 401 loss: 2.30616162710362e-06\n",
      "  batch 501 loss: 1.8734524923047501e-06\n",
      "LOSS train 3.18189031492422e-06 valid 8.40926259115804e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.299930176581256e-07\n",
      "  batch 101 loss: 5.317493820555796e-06\n",
      "  batch 201 loss: 9.062428893003016e-07\n",
      "  batch 301 loss: 3.9329934126897115e-06\n",
      "  batch 401 loss: 2.498455149577694e-06\n",
      "  batch 501 loss: 1.8239284250398668e-06\n",
      "LOSS train 3.000889631787333e-06 valid 2.4820574253681116e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3120972653268836e-07\n",
      "  batch 101 loss: 3.6428409357824874e-06\n",
      "  batch 201 loss: 1.0603613674220469e-06\n",
      "  batch 301 loss: 3.264165623875215e-06\n",
      "  batch 401 loss: 1.9065064756773608e-06\n",
      "  batch 501 loss: 1.7836272687077326e-06\n",
      "LOSS train 2.3330615964304738e-06 valid 1.9883930235664593e-06\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 9.868430424830876e-08\n",
      "  batch 101 loss: 2.9517143775592556e-06\n",
      "  batch 201 loss: 1.3672055109026359e-06\n",
      "  batch 301 loss: 3.394381101173849e-06\n",
      "  batch 401 loss: 1.2157076022845104e-06\n",
      "  batch 501 loss: 1.7034223850487252e-06\n",
      "LOSS train 2.139859425103533e-06 valid 1.5322718809329672e-06\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.576414661030867e-08\n",
      "  batch 101 loss: 1.7459073016112825e-06\n",
      "  batch 201 loss: 1.5889469423768788e-06\n",
      "  batch 301 loss: 1.9028117036157254e-06\n",
      "  batch 401 loss: 1.1656577730434491e-06\n",
      "  batch 501 loss: 2.6887291775778976e-06\n",
      "LOSS train 1.7362245110076352e-06 valid 8.195225404961093e-07\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.8593258321052416e-08\n",
      "  batch 101 loss: 1.6397973762138917e-06\n",
      "  batch 201 loss: 1.5014980517236153e-06\n",
      "  batch 301 loss: 1.4261256069403316e-06\n",
      "  batch 401 loss: 2.0439899742541456e-06\n",
      "  batch 501 loss: 1.8449192093328293e-06\n",
      "LOSS train 1.600452437046094e-06 valid 1.0391121350039612e-06\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.190767301930464e-08\n",
      "  batch 101 loss: 2.1734044952381735e-06\n",
      "  batch 201 loss: 1.6716627030177732e-06\n",
      "  batch 301 loss: 1.8091989457502678e-06\n",
      "  batch 401 loss: 2.1251368002594973e-06\n",
      "  batch 501 loss: 1.6977282945163098e-06\n",
      "LOSS train 1.916476198804876e-06 valid 1.4283435803008615e-06\n",
      "EPOCH 13:\n",
      "  batch 1 loss: 4.248201548762154e-08\n",
      "  batch 101 loss: 3.3264394625120986e-06\n",
      "  batch 201 loss: 6.658887889159359e-06\n",
      "  batch 301 loss: 1.935207201242406e-06\n",
      "  batch 401 loss: 9.99516998092531e-07\n",
      "  batch 501 loss: 1.5818653831445317e-06\n",
      "LOSS train 2.6198394821191553e-06 valid 1.2541199794213753e-06\n",
      "EPOCH 14:\n",
      "  batch 1 loss: 2.702122174014221e-08\n",
      "  batch 101 loss: 2.219331693993354e-06\n",
      "  batch 201 loss: 1.1397844303218108e-06\n",
      "  batch 301 loss: 1.7191899243584886e-06\n",
      "  batch 401 loss: 2.3958000440416073e-06\n",
      "  batch 501 loss: 2.6686534293673958e-06\n",
      "LOSS train 2.05021206097057e-06 valid 2.4943831249402137e-06\n",
      "EPOCH 15:\n",
      "  batch 1 loss: 7.115986591088586e-08\n",
      "  batch 101 loss: 2.266326793431972e-06\n",
      "  batch 201 loss: 1.4436086521385505e-06\n",
      "  batch 301 loss: 1.952089592833772e-06\n",
      "  batch 401 loss: 2.0293404902105295e-06\n",
      "  batch 501 loss: 2.5651825959016607e-06\n",
      "LOSS train 2.034640711746961e-06 valid 1.4331001239042962e-06\n",
      "EPOCH 16:\n",
      "  batch 1 loss: 4.228866146149812e-08\n",
      "  batch 101 loss: 2.7246106041189934e-06\n",
      "  batch 201 loss: 2.01024268733363e-06\n",
      "  batch 301 loss: 1.9040970106942723e-06\n",
      "  batch 401 loss: 2.594898303129867e-06\n",
      "  batch 501 loss: 4.1827563404694956e-06\n",
      "LOSS train 2.523738806768222e-06 valid 2.6488742150831968e-06\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0017971032857894897\n",
      "  batch 101 loss: 0.00871034726871585\n",
      "  batch 201 loss: 2.3364000830952137e-05\n",
      "  batch 301 loss: 4.23943497764867e-06\n",
      "  batch 401 loss: 7.741983421851729e-06\n",
      "  batch 501 loss: 5.745829211036834e-06\n",
      "LOSS train 0.00181598769229434 valid 2.561793735367246e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8762992112897336e-07\n",
      "  batch 101 loss: 1.8660723699213123e-05\n",
      "  batch 201 loss: 3.941834181659942e-06\n",
      "  batch 301 loss: 3.4303751715469843e-06\n",
      "  batch 401 loss: 4.873154121582957e-06\n",
      "  batch 501 loss: 4.243281209141969e-06\n",
      "LOSS train 6.4267628849636495e-06 valid 1.8228942280984484e-05\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File trained_models/RTS24_AC_12w_split_by_exec_pn/min_val/model_OE_2h_20e_0.000625lr_0dor_0np_False_ro.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOE_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_hidden\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mh_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_e\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124me_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mdor_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mnp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelu_out\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ro\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m---> 21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_multiple_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mm_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfolder_to_save\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m saved_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mt \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_val\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\OneDrive - KU Leuven\\JointResearch\\05. Models\\03.ANN_CostEstimator_For_LB\\training_methods.py:106\u001b[0m, in \u001b[0;36mtrain_multiple_epochs\u001b[1;34m(nb_epochs, model, training_loader, validation_loader, loss_fn, optimizer, model_name, folder)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(folder \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    105\u001b[0m             min_val_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/min_val/model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder,model_name)\n\u001b[1;32m--> 106\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_val_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     epoch_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    113\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/all_epochs/model_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(folder,model_name)\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File trained_models/RTS24_AC_12w_split_by_exec_pn/min_val/model_OE_2h_20e_0.000625lr_0dor_0np_False_ro.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "learning_rates = [0.0025/4*4**i for i in range(3)]\n",
    "negative_penalisations = [0,0.00001,0.0001,0.001,0.01]\n",
    "\n",
    "nbs_e = [16,20]#,8,12,16]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [2,3]\n",
    "dors = [0]#,0.05,0.1]#,0.05]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_pn\"\n",
    "for np in negative_penalisations: \n",
    "    loss_fn = NN_classes.create_loss_fn(penalize_negative=np)\n",
    "    #loss_fn = torch.nn.MSELoss()\n",
    "    for relu_out in [False,True]:\n",
    "        for nb_e in nbs_e:\n",
    "            for lr in learning_rates:\n",
    "                for nb_hidden in nbs_hidden: \n",
    "                    for dor in dors:\n",
    "                        m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor,relu_out=relu_out)\n",
    "                        m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{np}np_{relu_out}_ro\"\n",
    "                        optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                        train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                        saved_models = dict()\n",
    "\n",
    "                        for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                            path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                            model = m\n",
    "                            m.load_state_dict(torch.load(path))\n",
    "                            m.eval()\n",
    "\n",
    "                            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                            test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "                            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                            train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "                            validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                            validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "\n",
    "                            if mt == \"min_val\": \n",
    "                                min_val = True\n",
    "                            else: \n",
    "                                min_val = False\n",
    "\n",
    "                            r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                              \"Min_val\":min_val,\n",
    "                                              \"Epochs\": nb_e,\n",
    "                                              \"Lr\":lr,\n",
    "                                              \"Dor\": dor,\n",
    "                                              \"Tr_l\":train_loss.item(),\n",
    "                                              \"Te_l\":test_loss.item(),\n",
    "                                              \"V_l\": validation_loss.item(),\n",
    "                                             \"Np\": np,\n",
    "                                             \"Relu_out\": relu_out}\n",
    "                                             ,index = [i]\n",
    "                            )\n",
    "                            i+=1\n",
    "                            results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f652ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Dor</th>\n",
       "      <th>Relu_out</th>\n",
       "      <th>Batch_size</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Np</th>\n",
       "      <th>Min_val</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "      <th>V_l</th>\n",
       "      <th>Tr_l_t_mse</th>\n",
       "      <th>Te_l_t_mse</th>\n",
       "      <th>V_l_t_mse</th>\n",
       "      <th>Tr_l_ret</th>\n",
       "      <th>Train_time</th>\n",
       "      <th>Eval_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.472839</td>\n",
       "      <td>0.077540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.472839</td>\n",
       "      <td>0.094477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.685800</td>\n",
       "      <td>0.083754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.685800</td>\n",
       "      <td>0.093732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.437337</td>\n",
       "      <td>0.095970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>7.437337</td>\n",
       "      <td>0.131914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.491558</td>\n",
       "      <td>0.059747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5.491558</td>\n",
       "      <td>0.091356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.274385</td>\n",
       "      <td>0.081493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3.274385</td>\n",
       "      <td>0.089255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.111628</td>\n",
       "      <td>0.091645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.111628</td>\n",
       "      <td>0.084888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.809986</td>\n",
       "      <td>0.074580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>3.809986</td>\n",
       "      <td>0.084149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.200448</td>\n",
       "      <td>0.119675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>4.200448</td>\n",
       "      <td>0.132856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.525011</td>\n",
       "      <td>0.107901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>2.525011</td>\n",
       "      <td>0.136302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>2.818310</td>\n",
       "      <td>0.077888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>2.818310</td>\n",
       "      <td>0.099340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>min_val</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.562706</td>\n",
       "      <td>0.091024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>all_epochs</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.562706</td>\n",
       "      <td>0.138826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model_type  Dor  Relu_out  Batch_size        Lr  Epochs  Np     Min_val   \n",
       "0            3    0     False          32  0.000625       4   0     min_val  \\\n",
       "1            3    0     False          32  0.000625       4   0  all_epochs   \n",
       "2            3    0     False          32  0.002500       4   0     min_val   \n",
       "3            3    0     False          32  0.002500       4   0  all_epochs   \n",
       "4            3    0     False          32  0.010000       4   0     min_val   \n",
       "5            3    0     False          32  0.010000       4   0  all_epochs   \n",
       "6            3    0     False          32  0.040000       4   0     min_val   \n",
       "7            3    0     False          32  0.040000       4   0  all_epochs   \n",
       "8            3    0     False          64  0.000625       4   0     min_val   \n",
       "9            3    0     False          64  0.000625       4   0  all_epochs   \n",
       "10           3    0     False          64  0.002500       4   0     min_val   \n",
       "11           3    0     False          64  0.002500       4   0  all_epochs   \n",
       "12           3    0     False          64  0.010000       4   0     min_val   \n",
       "13           3    0     False          64  0.010000       4   0  all_epochs   \n",
       "14           3    0     False          64  0.040000       4   0     min_val   \n",
       "15           3    0     False          64  0.040000       4   0  all_epochs   \n",
       "16           3    0     False         128  0.000625       4   0     min_val   \n",
       "17           3    0     False         128  0.000625       4   0  all_epochs   \n",
       "18           3    0     False         128  0.002500       4   0     min_val   \n",
       "19           3    0     False         128  0.002500       4   0  all_epochs   \n",
       "20           3    0     False         128  0.010000       4   0     min_val   \n",
       "21           3    0     False         128  0.010000       4   0  all_epochs   \n",
       "\n",
       "        Tr_l      Te_l       V_l  Tr_l_t_mse  Te_l_t_mse  V_l_t_mse  Tr_l_ret   \n",
       "0   0.000002  0.000002  0.000002    0.000002    0.000002   0.000002  0.000002  \\\n",
       "1   0.000002  0.000002  0.000002    0.000002    0.000002   0.000002  0.000002   \n",
       "2   0.000003  0.000002  0.000002    0.000003    0.000002   0.000002  0.000003   \n",
       "3   0.000003  0.000002  0.000002    0.000003    0.000002   0.000002  0.000003   \n",
       "4   0.000007  0.000005  0.000004    0.000007    0.000005   0.000004  0.000007   \n",
       "5   0.000007  0.000005  0.000004    0.000007    0.000005   0.000004  0.000007   \n",
       "6   0.000010  0.000005  0.000002    0.000010    0.000005   0.000002  0.000010   \n",
       "7   0.000015  0.000006  0.000003    0.000015    0.000006   0.000003  0.000010   \n",
       "8   0.000041  0.000041  0.000040    0.000041    0.000041   0.000040  0.000040   \n",
       "9   0.000121  0.000116  0.000127    0.000121    0.000116   0.000127  0.000040   \n",
       "10  0.000009  0.000006  0.000006    0.000009    0.000006   0.000006  0.000008   \n",
       "11  0.000009  0.000006  0.000006    0.000009    0.000006   0.000006  0.000008   \n",
       "12  0.000035  0.000032  0.000031    0.000035    0.000032   0.000031  0.000035   \n",
       "13  0.000044  0.000039  0.000037    0.000044    0.000039   0.000037  0.000035   \n",
       "14  0.000014  0.000014  0.000030    0.000014    0.000014   0.000030  0.000014   \n",
       "15  0.000014  0.000014  0.000030    0.000014    0.000014   0.000030  0.000014   \n",
       "16  0.000052  0.000052  0.000054    0.000052    0.000052   0.000054  0.000051   \n",
       "17  0.000151  0.000153  0.000153    0.000151    0.000153   0.000153  0.000051   \n",
       "18  0.000299  0.000315  0.000315    0.000299    0.000315   0.000315  0.000294   \n",
       "19  0.000460  0.000513  0.000508    0.000460    0.000513   0.000508  0.000294   \n",
       "20  0.000057  0.000057  0.000059    0.000057    0.000057   0.000059  0.000056   \n",
       "21  0.000114  0.000117  0.000121    0.000114    0.000117   0.000121  0.000056   \n",
       "\n",
       "    Train_time  Eval_time  \n",
       "0     4.472839   0.077540  \n",
       "1     4.472839   0.094477  \n",
       "2     4.685800   0.083754  \n",
       "3     4.685800   0.093732  \n",
       "4     7.437337   0.095970  \n",
       "5     7.437337   0.131914  \n",
       "6     5.491558   0.059747  \n",
       "7     5.491558   0.091356  \n",
       "8     3.274385   0.081493  \n",
       "9     3.274385   0.089255  \n",
       "10    3.111628   0.091645  \n",
       "11    3.111628   0.084888  \n",
       "12    3.809986   0.074580  \n",
       "13    3.809986   0.084149  \n",
       "14    4.200448   0.119675  \n",
       "15    4.200448   0.132856  \n",
       "16    2.525011   0.107901  \n",
       "17    2.525011   0.136302  \n",
       "18    2.818310   0.077888  \n",
       "19    2.818310   0.099340  \n",
       "20    2.562706   0.091024  \n",
       "21    2.562706   0.138826  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe247f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/All_Exec_split_by_exec_penalize_neg_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0f94c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m f \u001b[38;5;241m=\u001b[39m (\u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mMin_val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (results\u001b[38;5;241m.\u001b[39mRelu_out \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m results[f]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "f = (results.Min_val == True) & (results.Relu_out == False)\n",
    "results[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (results.Epochs == 12)  & (results.Model_type != 0) \n",
    "sns.boxplot(y = \"Te_l\",x=\"Dor\",data = results[f],hue = \"Min_val\")\n",
    "plt.savefig(\"Figures/Split_by_exec/Min_val_effect_Testloss_fDor.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a759efd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f =  (results.Model_type == 0) & (results.Min_val == True)\n",
    "sns.boxplot(y = \"V_l\",x=\"Epochs\",data = results[f],hue = \"Lr\")\n",
    "plt.savefig(\"Figures/Split_by_exec/Lr_effect_Testloss_fEpochs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859941d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00034164607524871826\n",
      "  batch 101 loss: 0.0002987160199381833\n",
      "LOSS train 0.00046863495772977727 valid 6.615156962652691e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1388824532332364e-07\n",
      "  batch 101 loss: 4.279915004374857e-06\n",
      "LOSS train 3.882639331988874e-06 valid 4.274984803487314e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.7142380026343745e-08\n",
      "  batch 101 loss: 4.714095463782542e-06\n",
      "LOSS train 4.845918405521013e-06 valid 8.368496310140472e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.067415334749967e-07\n",
      "  batch 101 loss: 9.704118892841507e-06\n",
      "LOSS train 9.098841420518179e-06 valid 1.0499067684577312e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2910639017936775e-07\n",
      "  batch 101 loss: 8.047440166905062e-06\n",
      "LOSS train 7.632603447944408e-06 valid 6.453345577028813e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4348146578413435e-07\n",
      "  batch 101 loss: 6.558642437823892e-06\n",
      "LOSS train 6.467442821458197e-06 valid 4.256539796188008e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.248340236605146e-08\n",
      "  batch 101 loss: 6.1425094669687045e-06\n",
      "LOSS train 6.336900537815176e-06 valid 4.042789441882633e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.169131549744634e-08\n",
      "  batch 101 loss: 5.983339131745424e-06\n",
      "LOSS train 6.172924947311109e-06 valid 4.524460564425681e-06\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.05 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005922958254814148\n",
      "  batch 101 loss: 0.0017523869141587056\n",
      "LOSS train 0.0020242822022996437 valid 0.00013990193838253617\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1807685950770974e-05\n",
      "  batch 101 loss: 0.0007025678611535113\n",
      "LOSS train 0.0006539419920729256 valid 3.089294477831572e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.049980780109763e-06\n",
      "  batch 101 loss: 0.0002934100830316311\n",
      "LOSS train 0.00027420853407387314 valid 8.263343625003472e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.201225142925978e-06\n",
      "  batch 101 loss: 0.0001229988248087466\n",
      "LOSS train 0.00011415099240771502 valid 2.0785542801604606e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.711556175607257e-07\n",
      "  batch 101 loss: 6.961023875192041e-05\n",
      "LOSS train 6.641250860889656e-05 valid 1.2547947335406207e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.643611802952365e-07\n",
      "  batch 101 loss: 4.652859639463713e-05\n",
      "LOSS train 4.482836706912652e-05 valid 1.7496835425845347e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.848715954925865e-07\n",
      "  batch 101 loss: 3.48935174770304e-05\n",
      "LOSS train 3.2981187260257635e-05 valid 9.343787496618461e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.013871875940822e-07\n",
      "  batch 101 loss: 2.6938014907500475e-05\n",
      "LOSS train 2.6516977179836185e-05 valid 1.499108475400135e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003759964555501938\n",
      "  batch 101 loss: 0.0002994551378594679\n",
      "LOSS train 0.0004980125270533485 valid 3.066094723180868e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.233185114164371e-07\n",
      "  batch 101 loss: 2.3100672784437393e-05\n",
      "LOSS train 1.828971353175092e-05 valid 5.12090355186956e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.323178927734262e-08\n",
      "  batch 101 loss: 1.5219171934859333e-05\n",
      "LOSS train 1.2941402010222649e-05 valid 4.169864951109048e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.2489332271507e-08\n",
      "  batch 101 loss: 1.201928494879212e-05\n",
      "LOSS train 1.0501387999067735e-05 valid 5.3815638239029795e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3004389074922073e-07\n",
      "  batch 101 loss: 1.1259929774496413e-05\n",
      "LOSS train 1.0635669727176109e-05 valid 5.129483724886086e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2970699572179001e-07\n",
      "  batch 101 loss: 9.978570278548205e-06\n",
      "LOSS train 1.0972103528247216e-05 valid 2.3275651983567514e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.492273991578258e-08\n",
      "  batch 101 loss: 6.416301563518801e-06\n",
      "LOSS train 7.143410925165556e-06 valid 1.5495916159125045e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.3757895582530184e-08\n",
      "  batch 101 loss: 5.631845216953479e-06\n",
      "LOSS train 6.252941732593145e-06 valid 1.602147676749155e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.05 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.1198659922229126e-06\n",
      "  batch 101 loss: 0.0006333536207603174\n",
      "LOSS train 0.0004843319122805719 valid 7.497398473788053e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.627797559602186e-07\n",
      "  batch 101 loss: 8.850092912325636e-05\n",
      "LOSS train 9.681447429673187e-05 valid 5.2603452786570415e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0451241400442086e-07\n",
      "  batch 101 loss: 0.00013104593766911422\n",
      "LOSS train 0.00013360057275920315 valid 6.870205834275112e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.950898336479441e-07\n",
      "  batch 101 loss: 0.00013878705189654283\n",
      "LOSS train 0.00013746411305362886 valid 7.302850281121209e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.970096678240225e-07\n",
      "  batch 101 loss: 0.00013051096957042318\n",
      "LOSS train 0.00013020883218409603 valid 7.943673699628562e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.125953349401243e-06\n",
      "  batch 101 loss: 0.00012879935582532198\n",
      "LOSS train 0.00012868645641875268 valid 7.725907926214859e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.451952064409852e-07\n",
      "  batch 101 loss: 0.00012520438244791876\n",
      "LOSS train 0.0001248416180423039 valid 7.843563798815012e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.82089768513106e-07\n",
      "  batch 101 loss: 0.0001242020824338397\n",
      "LOSS train 0.0001238926775927112 valid 7.970364822540432e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.001558859497308731\n",
      "  batch 101 loss: 0.005186513712633314\n",
      "LOSS train 0.004936050064179985 valid 8.999645797302946e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.814881973899901e-07\n",
      "  batch 101 loss: 5.0159332658950007e-05\n",
      "LOSS train 4.703553167347594e-05 valid 4.639695180230774e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.430719712516293e-06\n",
      "  batch 101 loss: 2.501594723753442e-05\n",
      "LOSS train 2.19400943598327e-05 valid 2.1612640921375714e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.538089367793873e-07\n",
      "  batch 101 loss: 1.0706216955327363e-05\n",
      "LOSS train 8.868750437754998e-06 valid 1.7195869077113457e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.948681402718648e-07\n",
      "  batch 101 loss: 1.2491017996438813e-05\n",
      "LOSS train 1.0224801394366399e-05 valid 2.0096551452297717e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.271369395311922e-07\n",
      "  batch 101 loss: 9.39893874971176e-06\n",
      "LOSS train 7.933744244875899e-06 valid 1.791460636013653e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.35157193755731e-07\n",
      "  batch 101 loss: 1.2398367850323666e-05\n",
      "LOSS train 1.0327998213813067e-05 valid 2.206591670983471e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.978040230227635e-07\n",
      "  batch 101 loss: 9.679812596345982e-06\n",
      "LOSS train 8.259995970306956e-06 valid 2.086797212541569e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.05 2 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.350731123238802e-05\n",
      "  batch 101 loss: 7.162675751317238e-05\n",
      "LOSS train 8.467759913230151e-05 valid 0.0002419580123387277\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.040080476552248e-07\n",
      "  batch 101 loss: 8.283613414732826e-05\n",
      "LOSS train 8.413587737068886e-05 valid 0.0002600552688818425\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.028914557537064e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 8.349045927786846e-05\n",
      "LOSS train 8.473249215222182e-05 valid 0.00026572568458504975\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0685541201382875e-06\n",
      "  batch 101 loss: 8.358249860577871e-05\n",
      "LOSS train 8.481747875356945e-05 valid 0.00026821994106285274\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.0860618931474164e-06\n",
      "  batch 101 loss: 8.359412293373224e-05\n",
      "LOSS train 8.482635637699415e-05 valid 0.00026957711088471115\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0956062033073977e-06\n",
      "  batch 101 loss: 8.419897172757374e-05\n",
      "LOSS train 8.527024099659193e-05 valid 0.00027045802562497556\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1018083023373038e-06\n",
      "  batch 101 loss: 8.358560233659773e-05\n",
      "LOSS train 8.481361840040336e-05 valid 0.00027101245359517634\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1057133815484121e-06\n",
      "  batch 101 loss: 8.357661963202645e-05\n",
      "LOSS train 8.480413396891824e-05 valid 0.00027139612939208746\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.2990431170910595e-05\n",
      "  batch 101 loss: 0.007805943408557141\n",
      "LOSS train 0.005747665538861493 valid 4.2484050936764106e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8268736059544606e-07\n",
      "  batch 101 loss: 8.301740681417868e-05\n",
      "LOSS train 8.033088703372295e-05 valid 0.00010289065539836884\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.17837678064825e-07\n",
      "  batch 101 loss: 0.00022230391290520402\n",
      "LOSS train 0.00017162923652939136 valid 7.296604599105194e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.987531716935336e-08\n",
      "  batch 101 loss: 3.4348710817084796e-05\n",
      "LOSS train 3.292066638919195e-05 valid 6.374463555403054e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.022118552413303e-08\n",
      "  batch 101 loss: 2.536394498520167e-05\n",
      "LOSS train 2.3394936250832518e-05 valid 6.77164935041219e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4851939340587704e-07\n",
      "  batch 101 loss: 1.840903212610101e-05\n",
      "LOSS train 1.679027712745572e-05 valid 2.2540629288414493e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.177058948087506e-08\n",
      "  batch 101 loss: 1.056158576574262e-05\n",
      "LOSS train 9.818336009532083e-06 valid 9.886070984066464e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.5841300056490584e-09\n",
      "  batch 101 loss: 5.26026486312503e-06\n",
      "LOSS train 4.806640704896215e-06 valid 6.970905815251172e-06\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.05, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.05 3 False\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005871903896331787\n",
      "  batch 101 loss: 0.0037287083886621986\n",
      "LOSS train 0.003240747868323896 valid 5.29523313161917e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.856662951875478e-06\n",
      "  batch 101 loss: 0.00016238674721535062\n",
      "LOSS train 0.00014129626918866649 valid 4.4139633246231824e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.177985389716923e-07\n",
      "  batch 101 loss: 7.568057643766225e-05\n",
      "LOSS train 7.288980431652433e-05 valid 5.804567990708165e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.2960932660207616e-08\n",
      "  batch 101 loss: 0.00010113231340483253\n",
      "LOSS train 9.823808144438194e-05 valid 7.378833106486127e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3905653304391308e-08\n",
      "  batch 101 loss: 8.790069935685096e-05\n",
      "LOSS train 8.388547479025343e-05 valid 0.000104082930192817\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0352472600061446e-07\n",
      "  batch 101 loss: 7.341942575294524e-05\n",
      "LOSS train 7.061245153895172e-05 valid 0.0001280449505429715\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.072174356726464e-07\n",
      "  batch 101 loss: 6.473507615510243e-05\n",
      "LOSS train 6.273131256284946e-05 valid 0.00014535507943946868\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9930180971859955e-07\n",
      "  batch 101 loss: 5.992781000713876e-05\n",
      "LOSS train 5.8497523609830284e-05 valid 0.00016045761003624648\n"
     ]
    }
   ],
   "source": [
    "##Old loop\n",
    "\n",
    "learning_rates = [0.0025*4**i for i in range(2)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [8]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [2,3]\n",
    "dors = [0,0.05]#,0.1,0.2,0.4]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w_split_by_exec_nl\"\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            for dor in dors:\n",
    "                m = NN_classes.create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor)\n",
    "                m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor\"\n",
    "                optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                saved_models = dict()\n",
    "\n",
    "                for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                    path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                    model = m\n",
    "                    m.load_state_dict(torch.load(path))\n",
    "                    m.eval()\n",
    "\n",
    "                    test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                    test_loss = loss_fn(test_predictions.squeeze(),d_ft_out[\"test\"])\n",
    "\n",
    "                    train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                    train_loss = loss_fn(train_predictions.squeeze(),d_ft_out[\"train\"])\n",
    "\n",
    "                    validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                    validation_loss = loss_fn(validation_prediction.squeeze(),d_ft_out[\"val\"])\n",
    "\n",
    "                    if mt == \"min_val\": \n",
    "                        min_val = True\n",
    "                    else: \n",
    "                        min_val = False\n",
    "\n",
    "                    r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                      \"Min_val\":min_val,\n",
    "                                      \"Epochs\": nb_e,\n",
    "                                      \"Lr\":lr,\n",
    "                                      \"Dor\": dor,\n",
    "                                      \"Tr_l\":train_loss.item(),\n",
    "                                      \"Te_l\":test_loss.item(),\n",
    "                                      \"V_l\": validation_loss.item()}\n",
    "                                     ,index = [i]\n",
    "                    )\n",
    "                    i+=1\n",
    "                    results = pd.concat([results,r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
