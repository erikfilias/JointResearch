{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c19b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_evaluation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a947c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[350, 360, 370, 380, 390]\n",
      "input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "14\n",
      "input_f_sc01_Network_Full_Generation_Full_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_2_cac1_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_2_cac2_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_2_cac3_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_3_cac1_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_3_cac2_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_3_cac3_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_2_Node_3_cac1_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_2_Node_3_cac2_2030.csv\n",
      "14\n",
      "input_f_sc01_PINT_Network_Line_In_Node_2_Node_3_cac3_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_1_Node_2_cac1_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_1_Node_2_cac2_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_1_Node_2_cac3_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_1_Node_3_cac1_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_1_Node_3_cac2_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_1_Node_3_cac3_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_2_Node_3_cac1_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_2_Node_3_cac2_2030.csv\n",
      "14\n",
      "input_f_sc01_TOOT_Network_Line_In_Node_2_Node_3_cac3_2030.csv\n",
      "14\n",
      "8736 350 1\n",
      "[8, 3, 2]\n",
      "8736 360 1\n",
      "[8, 3, 2]\n",
      "8736 370 1\n",
      "[8, 3, 2]\n",
      "8736 380 1\n",
      "[8, 3, 2]\n",
      "8736 390 1\n",
      "[8, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "import DataLoading\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import NN_classes\n",
    "import training_methods\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "case = \"3-bus\"\n",
    "\n",
    "folder = f\"../Data/{case}_DC_fy\"\n",
    "\n",
    "all_executions = DataLoading.list_executions(folder=folder, per=period, sc=sc)\n",
    "executions_start = 0\n",
    "executions_end = len(all_executions)\n",
    "executions = all_executions[executions_start:executions_end]\n",
    "te_s = 0.3\n",
    "val_s = 0.3\n",
    "outp = \"SystemCosts\"\n",
    "#nb_hours_used = 24 * 7 * 12\n",
    "nb_hours_list = [10 * i for i in range(35,40,1)]\n",
    "print(nb_hours_list)\n",
    "exec_name = f\"test_loss_rand_hours_low_{case}_DC_{te_s}_v{val_s}_PF_{executions_start}_{executions_end}\"\n",
    "folder_to_save = f\"{exec_name}\"\n",
    "\n",
    "dfs_in_full, dfs_out_full, dfs_inter_full = DataLoading.load_data_ext_out(folder, executions, period, sc, [\"PowerFlow\"], outp)\n",
    "dfs_inter_j_full = DataLoading.join_frames_inter_layer(dfs_inter_full,all_executions)\n",
    "dfs_inter_j_full = DataLoading.trim_columns_to_common(dfs_inter_j_full)\n",
    "\n",
    "#Save the full datasets as pytorch tensors for informative loss calculation afterwards\n",
    "t_in_fy, t_out_fy, t_inter_fy, maxs = DataLoading.concat_all_exec_fy(dfs_in_full, dfs_out_full, dfs_inter_j_full,all_executions)\n",
    "results = pd.DataFrame()\n",
    "i = 0\n",
    "for nb_hours_used in nb_hours_list:\n",
    "\n",
    "    # Select subset for the training process\n",
    "    # Method = random hours\n",
    "    indices = DataLoading.get_random_hours_indices(nb_available=len(dfs_in_full[executions[0]]), nb_selected=nb_hours_used)\n",
    "\n",
    "    dfs_in, dfs_out, dfs_inter_j = DataLoading.return_selection(dfs_dict_list=[dfs_in_full, dfs_out_full, dfs_inter_j_full],\n",
    "                                                                indices=indices)\n",
    "    # Convert to pytorch tensors\n",
    "    ts_in, ts_out, ts_inter = DataLoading.split_tr_val_te_ext_out(dfs_in, dfs_out, dfs_inter_j, executions, te_s, val_s)\n",
    "\n",
    "    # Concat and normalize\n",
    "    d_ft_in, d_ft_out, d_ft_inter, maxs_1 = DataLoading.concat_and_normalize_ext_out(ts_in, ts_out, ts_inter, executions)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float(), d_ft_inter['train'])\n",
    "    validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float(), d_ft_inter['val'].float())\n",
    "\n",
    "    # Perform the actual loop that checks multiple hyperparams\n",
    "\n",
    "    nbs_hidden = [(1, 1)]  #\n",
    "    dors = [0]\n",
    "    relu_outs = [False]\n",
    "    batch_sizes = [64]\n",
    "    learning_rates = [0.0025 * 4 ** i for i in range(0, 1, 1)]\n",
    "    nbs_e = [64]\n",
    "    alphas = [0]\n",
    "    beta = 1\n",
    "    MAEs = [False]\n",
    "\n",
    "\n",
    "    hp_sets = [(nb_h, dor, relu_out, bs, lr, nb_e, alpha, MAE) for nb_h in nbs_hidden for dor in dors for relu_out in\n",
    "               relu_outs for bs in batch_sizes for lr in learning_rates for nb_e in nbs_e\n",
    "               for alpha in alphas for MAE in MAEs]\n",
    "    for hp_set in hp_sets:\n",
    "        # Initialize hyperparameter from hp_set\n",
    "        nb_hidden, dor, relu_out, bs, lr, nb_e, alpha, MAE = hp_set\n",
    "\n",
    "        # Create training and validation loaders based on batch size\n",
    "        training_loader = DataLoader(train, batch_size=bs)\n",
    "        validation_loader = DataLoader(validation, batch_size=bs)\n",
    "\n",
    "        # Initialize loss functions\n",
    "        loss_fn = NN_classes.create_custom_loss(alpha=alpha, beta=beta, MAE=MAE)\n",
    "        loss_t_mse = torch.nn.MSELoss()\n",
    "        loss_mae = torch.nn.L1Loss()\n",
    "        # Create model based on hyperparameter set\n",
    "        m = NN_classes.create_model(nb_hidden, d_ft_in['train'].shape[1], dropout_ratio=dor, relu_out=relu_out, inter=True,\n",
    "                                    inter_size=dfs_inter_j[\"Network_Existing_Generation_Full\"].shape[1])\n",
    "\n",
    "        # Create model name for saving and loading\n",
    "        m_name = f\"OE_{nb_hours_used}hours_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{relu_out}ro_{bs}bs_{alpha}ill_{MAE}MAE\"\n",
    "        # Create optimizer based on learning rate\n",
    "        optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "        # Train the actual model\n",
    "        t_start_train = time.perf_counter()\n",
    "        train_loss_1 = training_methods.train_multiple_epochs(\n",
    "            nb_e, m, training_loader, validation_loader, loss_fn, optimizer, m_name,\n",
    "            folder_to_save, True)[0]\n",
    "        t_stop_train = time.perf_counter()\n",
    "\n",
    "        for mt in [\"min_val\", \"all_epochs\"]:\n",
    "            t_start_eval = time.perf_counter()\n",
    "            path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "            # Retreive model state and set to evaluation mode\n",
    "            m.load_state_dict(torch.load(path))\n",
    "            m.eval()\n",
    "\n",
    "            # Calculate losses\n",
    "            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "            test_loss = loss_fn(test_predictions[0].squeeze(), d_ft_out[\"test\"], test_predictions[1].squeeze(),\n",
    "                                d_ft_inter[\"test\"])\n",
    "            test_loss_t_mse = loss_t_mse(test_predictions[0].squeeze(), d_ft_out[\"test\"])\n",
    "            test_loss_mae = loss_mae(test_predictions[0].squeeze(), d_ft_out[\"test\"])\n",
    "            manual_diff = test_predictions[0].detach().numpy().transpose() - d_ft_out[\"test\"].numpy()\n",
    "            Te_l_mae_man = np.mean(np.abs(manual_diff))\n",
    "\n",
    "            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "            train_loss = loss_fn(train_predictions[0].squeeze(), d_ft_out[\"train\"], train_predictions[1].squeeze(),\n",
    "                                 d_ft_inter[\"train\"])\n",
    "            train_loss_t_mse = loss_t_mse(train_predictions[0].squeeze(), d_ft_out[\"train\"])\n",
    "            train_loss_mae = loss_mae(train_predictions[0].squeeze(), d_ft_out[\"train\"])\n",
    "\n",
    "            validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "            validation_loss = loss_fn(validation_prediction[0].squeeze(), d_ft_out[\"val\"],\n",
    "                                      validation_prediction[1].squeeze(), d_ft_inter[\"val\"])\n",
    "            validation_loss_t_mse = loss_t_mse(validation_prediction[0].squeeze(), d_ft_out[\"val\"])\n",
    "            validation_loss_mae = loss_mae(validation_prediction[0].squeeze(), d_ft_out[\"val\"])\n",
    "\n",
    "\n",
    "            t_stop_eval = time.perf_counter()\n",
    "\n",
    "            fy_prediction = m(t_in_fy.float())\n",
    "\n",
    "            fy_l = loss_fn(fy_prediction[0].squeeze(), t_out_fy,fy_prediction[1].squeeze(),t_inter_fy)\n",
    "            fy_l_mse = loss_t_mse(fy_prediction[0].squeeze(), t_out_fy)\n",
    "            fy_l_mae = loss_mae(fy_prediction[0].squeeze(), t_out_fy)\n",
    "\n",
    "            manual_diff = fy_prediction[0].detach().numpy().transpose() - t_out_fy.numpy()\n",
    "            fy_l_mae_man = np.mean(np.abs(manual_diff))\n",
    "\n",
    "            # Calculate some calculation times\n",
    "            t_train = t_stop_train - t_start_train\n",
    "            t_eval = t_stop_eval - t_start_eval\n",
    "\n",
    "            #Calculate some losses manually\n",
    "\n",
    "\n",
    "            # Finally, save all desired values in a dataframe\n",
    "            r = pd.DataFrame({\"Model_type\": [nb_hidden],\n",
    "                              \"Dor\": dor,\n",
    "                              \"Relu_out\": relu_out,\n",
    "                              \"Batch_size\": bs,\n",
    "                              \"Lr\": lr,\n",
    "                              \"Epochs\": nb_e,\n",
    "                              \"Min_val\": mt,\n",
    "                              \"Nb_hours_used\":nb_hours_used,\n",
    "                              \"Tr_l\": train_loss.item(),\n",
    "                              \"Te_l\": test_loss.item(),\n",
    "                              \"V_l\": validation_loss.item(),\n",
    "                              \"Tr_l_mse\": train_loss_t_mse.item(),\n",
    "                              \"Te_l_mse\": test_loss_t_mse.item(),\n",
    "                              \"V_l_mse\": validation_loss_t_mse.item(),\n",
    "                              \"Tr_l_mae\": train_loss_mae.item(),\n",
    "                              \"Te_l_mae\": test_loss_mae.item(),\n",
    "                              \"Te_l_mae_man\": Te_l_mae_man ,\n",
    "                              \"V_l_mae\": validation_loss_mae.item(),\n",
    "                              \"fy_l\":fy_l.item(),\n",
    "                              \"fy_l_mse\":fy_l_mse.item(),\n",
    "                              \"fy_l_mae\":fy_l_mae.item(),\n",
    "                              \"fy_l_mae_man\":fy_l_mae_man,\n",
    "                              \"Train_time\": t_train,\n",
    "                              \"Eval_time\": t_eval,\n",
    "                              \"alpha\": alpha,\n",
    "                              \"beta\": beta,\n",
    "                              \"MAE\": MAE,\n",
    "                              \"Test size\": te_s,\n",
    "                              \"Val size\": val_s\n",
    "                              }\n",
    "                             , index=[i])\n",
    "            i += 1\n",
    "            results = pd.concat([results, r])\n",
    "        results.to_csv(f\"Loss_results_csv/{exec_name}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5774d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 3, 2]\n",
      "OE_390hours_(1, 1)h_64e_0.0025lr_0dor_Falsero_64bs_0ill_FalseMAE min_val\n",
      "trained_models/test_loss_rand_hours_low_3-bus_DC_0.3_v0.3_PF_0_20/min_val/model_OE_390hours_(1, 1)h_64e_0.0025lr_0dor_Falsero_64bs_0ill_FalseMAE.pth\n"
     ]
    }
   ],
   "source": [
    "input_size = dfs_in[\"Network_Existing_Generation_Full\"].shape[1]\n",
    "inter_size = dfs_inter_j[\"Network_Existing_Generation_Full\"].shape[1]\n",
    "\n",
    "\n",
    "hyperloop_name = \"test_loss_rand_hours_low_3-bus_DC_0.3_v0.3_PF_0_20\"\n",
    "df_losses = pd.read_csv(f\"Loss_results_csv/{hyperloop_name}.csv\",index_col=0)\n",
    "\n",
    "f = df_losses.Nb_hours_used >=1\n",
    "loss_to_sort = \"fy_l_mae\"\n",
    "xth_best = 2\n",
    "row = NN_evaluation.find_xthbest_model_params_from_df(df_losses[f],loss_to_sort,xth_best)\n",
    "m = NN_evaluation.create_model_and_load_state_from_row(row,input_size,inter_size,hyperloop_name,cluster_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4e9644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'min_val'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.Min_val.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d09741",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = m(t_in_fy.float())\n",
    "manual_diff = prediction[0].detach().numpy().transpose() - t_out_fy.numpy()\n",
    "np.mean(np.abs(manual_diff)),np.mean(np.square(manual_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80d80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_t_mse = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321b1b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0557e-06, dtype=torch.float64, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_t_mse(prediction[0].squeeze(),t_out_fy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7f02ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0556548932717828e-06"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.fy_l_mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa8d6b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3159,  0.5976,  0.7358],\n",
       "        [-0.3508,  0.5964,  0.7499],\n",
       "        [-0.3881,  0.6003,  0.7701],\n",
       "        ...,\n",
       "        [-1.0000,  0.5625,  1.0000],\n",
       "        [-1.0000,  0.5625,  1.0000],\n",
       "        [-1.0000,  0.5625,  1.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_inter_fy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
