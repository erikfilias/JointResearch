{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c19b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import NN_evaluation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29d4128d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "23\n",
      "input_f_sc01_Network_Full_Generation_Full_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_2_cac1_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_2_cac2_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_2_cac3_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_3_cac1_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_3_cac2_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_1_Node_3_cac3_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_2_Node_3_cac1_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_2_Node_3_cac2_2030.csv\n",
      "23\n",
      "input_f_sc01_PINT_Network_Line_In_Node_2_Node_3_cac3_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_1_Node_2_cac1_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_1_Node_2_cac2_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_1_Node_2_cac3_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_1_Node_3_cac1_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_1_Node_3_cac2_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_1_Node_3_cac3_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_2_Node_3_cac1_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_2_Node_3_cac2_2030.csv\n",
      "23\n",
      "input_f_sc01_TOOT_Network_Line_Out_Node_2_Node_3_cac3_2030.csv\n",
      "23\n",
      "Amount of nb_hours:  9 [24, 72, 120, 168, 240, 960, 1680, 2400, 3120]\n",
      "2016 2016 Days\n",
      "Normalized output tensor validation set has mean:  tensor(0.2715, dtype=torch.float64)\n",
      "Number of hyperparameters:  16\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "Amount of nb_hours:  9 [24, 72, 120, 168, 240, 960, 1680, 2400, 3120]\n",
      "2016 2016 Days\n",
      "Normalized output tensor validation set has mean:  tensor(0.0240, dtype=torch.float64)\n",
      "Number of hyperparameters:  16\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[3]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n",
      "[46, 69, 13, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "import DataLoading\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import NN_classes\n",
    "import training_methods\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "case = \"3-bus\"\n",
    "\n",
    "folder = f\"../Data/{case}_DC_fy\"\n",
    "\n",
    "all_executions = DataLoading.list_executions(folder=folder, per=period, sc=sc)\n",
    "executions_start = 0\n",
    "executions_end = len(all_executions)\n",
    "executions = all_executions[executions_start:executions_end]\n",
    "te_s = 1/4\n",
    "val_s = te_s/(1-te_s)\n",
    "outp = \"SystemCosts\"\n",
    "val_s_name = round(val_s,2)\n",
    "\n",
    "nb_hours_list = [24 * i for i in range(1,9,2)] + [24 * i for i in range(10,150,30)]\n",
    "# nb_hours_list = [24 * i for i in range(1,2,2)]\n",
    "#nb_hours_list = [24 * i for i in range(1,3,2)]\n",
    "#exec_name = f\"rand_days_and_hours_{case}_DC_{te_s}_v{val_s_name}_PF_{executions_start}_{executions_end}\"\n",
    "\n",
    "\n",
    "dfs_in_full, dfs_out_full, dfs_inter_full = DataLoading.load_data_ext_out(folder, executions, period, sc, [\"PowerFlow\"], outp)\n",
    "dfs_inter_j_full = DataLoading.join_frames_inter_layer(dfs_inter_full,all_executions)\n",
    "dfs_inter_j_full = DataLoading.trim_columns_to_common(dfs_inter_j_full)\n",
    "results = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "for normalization in [\"min-max\",\"z-score\"]:\n",
    "    #Save the full datasets as pytorch tensors for informative loss calculation afterwards\n",
    "    t_in_fy, t_out_fy, t_inter_fy, maxs = DataLoading.concat_all_exec_fy(dfs_in_full, dfs_out_full, dfs_inter_j_full,all_executions,normalization=normalization)\n",
    "\n",
    "\n",
    "    selection_methods = [\"Days\",\"Hours\"]\n",
    "    selection_sets = [(selection_method,nb_hours) for nb_hours in nb_hours_list for selection_method in selection_methods]\n",
    "    selection_sets.append((\"Weeks\",24*7*12))\n",
    "    selection_sets = [(\"Days\",24*7*12)]\n",
    "\n",
    "    print(\"Amount of nb_hours: \", len(nb_hours_list), nb_hours_list)\n",
    "    for selection_set in selection_sets:\n",
    "        selection_method,nb_hours = selection_set[0],selection_set[1]\n",
    "        exec_name = f\"test_norm_meth_rand_{selection_method}_{case}_DC_{te_s}_v{val_s_name}_PF_{executions_start}_{executions_end}_extra\"\n",
    "        folder_to_save = f\"{exec_name}\"\n",
    "\n",
    "        # Select subset for the training process\n",
    "        if selection_method == \"Hours\":\n",
    "            # Method = random hours\n",
    "            indices = DataLoading.get_random_hours_indices(nb_available=len(dfs_in_full[executions[0]]), nb_selected=nb_hours)\n",
    "            nb_hours_used = nb_hours\n",
    "        elif selection_method == \"Days\":\n",
    "            nb_days = int(nb_hours/24)\n",
    "            indices = DataLoading.get_random_days_indices(hours_available = len(dfs_in_full[executions[0]]),nb_selected = nb_days,hours_in_day=24,sorted = True)\n",
    "            nb_hours_used= nb_days*24\n",
    "\n",
    "            assert(nb_hours_used == len(indices))\n",
    "        elif selection_method == \"Weeks\":\n",
    "            indices = DataLoading.get_random_week_per_month_indices(df= dfs_out_full[executions[0]],hours_in_day=24, days_in_week=7)\n",
    "            nb_hours_used = len(indices)\n",
    "        else:\n",
    "            raise Exception(f\"Selection method {selection_method} not implemented\")\n",
    "        # print(indices)\n",
    "        print(nb_hours_used, len(indices), selection_method)\n",
    "        dfs_in, dfs_out, dfs_inter_j = DataLoading.return_selection(dfs_dict_list=[dfs_in_full, dfs_out_full, dfs_inter_j_full],\n",
    "                                                                    indices=indices)\n",
    "        # Convert to pytorch tensors\n",
    "        ts_in, ts_out, ts_inter = DataLoading.split_tr_val_te_ext_out(dfs_in, dfs_out, dfs_inter_j, executions, te_s, val_s)\n",
    "\n",
    "        # Concat and normalize\n",
    "        d_ft_in, d_ft_out, d_ft_inter, maxs_1 = DataLoading.concat_and_normalize_ext_out(ts_in, ts_out, ts_inter, executions,normalize= False)\n",
    "        for setname in [\"train\",\"test\",\"val\"]:\n",
    "            d_ft_in[setname] = torch.nan_to_num((d_ft_in[setname]-maxs[\"in_shift\"])/maxs[\"in_scalar\"])\n",
    "            d_ft_inter[setname] = torch.nan_to_num((d_ft_inter[setname]-maxs[\"inter_shift\"])/maxs[\"inter_scalar\"])\n",
    "            d_ft_out[setname] = torch.nan_to_num((d_ft_out[setname]-maxs[\"out_shift\"])/maxs[\"out_scalar\"])\n",
    "        # print(d_ft_in[\"train\"].shape)\n",
    "        # print(d_ft_in[\"test\"].shape)\n",
    "        # print(d_ft_in[\"val\"].shape)\n",
    "        print(\"Normalized output tensor validation set has mean: \", d_ft_out[setname].mean())\n",
    "\n",
    "\n",
    "        # Create TensorDatasets\n",
    "        train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float(), d_ft_inter['train'])\n",
    "        validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float(), d_ft_inter['val'].float())\n",
    "\n",
    "        # Perform the actual loop that checks multiple hyperparams\n",
    "\n",
    "        nbs_hidden = [(0,0),(3,1)]  #\n",
    "        # nbs_hidden = [(0,0)]\n",
    "\n",
    "        dors = [0]\n",
    "        relu_outs = [False]\n",
    "\n",
    "        batch_sizes = [64]\n",
    "        # batch_sizes = [128]\n",
    "\n",
    "        learning_rates = [0.0025 * 2 ** i for i in range(-1, 1, 1)]\n",
    "        #learning_rates = [0.0025*2**i for i in range(0,1,1)]\n",
    "\n",
    "        nbs_e = [128,256]\n",
    "        #nbs_e = [64,128]\n",
    "\n",
    "        #alphas = [0,1]\n",
    "        alphas = [0,1]\n",
    "        beta = 1\n",
    "\n",
    "        MAEs = [False]\n",
    "\n",
    "\n",
    "        hp_sets = [(nb_h, dor, relu_out, bs, lr, nb_e, alpha, MAE) for nb_h in nbs_hidden for dor in dors for relu_out in\n",
    "                   relu_outs for bs in batch_sizes for lr in learning_rates for nb_e in nbs_e\n",
    "                   for alpha in alphas for MAE in MAEs]\n",
    "        print(\"Number of hyperparameters: \" ,len(hp_sets))\n",
    "        for hp_set in hp_sets:\n",
    "            # Initialize hyperparameter from hp_set\n",
    "            nb_hidden, dor, relu_out, bs, lr, nb_e, alpha, MAE = hp_set\n",
    "\n",
    "            # Create training and validation loaders based on batch size\n",
    "            training_loader = DataLoader(train, batch_size=bs,shuffle = True)\n",
    "            validation_loader = DataLoader(validation, batch_size=bs,shuffle = True)\n",
    "\n",
    "            # Initialize loss functions\n",
    "            loss_fn = NN_classes.create_custom_loss(alpha=alpha, beta=beta, MAE=MAE)\n",
    "            loss_t_mse = torch.nn.MSELoss()\n",
    "            loss_mae = torch.nn.L1Loss()\n",
    "\n",
    "            #Create hidden sizes vector\n",
    "            if case == \"RTS24\" and nb_hidden == (3,1):\n",
    "                hs = [60,60,60,38,19]\n",
    "            else:\n",
    "                hs = None\n",
    "\n",
    "            # Create model based on hyperparameter set\n",
    "            m = NN_classes.create_model(nb_hidden, d_ft_in['train'].shape[1], dropout_ratio=dor, relu_out=relu_out, inter=True,\n",
    "                                        inter_size=dfs_inter_j[\"Network_Existing_Generation_Full\"].shape[1],hidden_sizes = hs)\n",
    "\n",
    "            # Create model name for saving and loading\n",
    "            m_name = f\"OE_{nb_hours_used}hours_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor_{relu_out}ro_{bs}bs_{alpha}ill_{MAE}MAE\"\n",
    "            # Create optimizer based on learning rate\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            # Train the actual model\n",
    "#             t_start_train = time.perf_counter()\n",
    "#             train_loss_1 = training_methods.train_multiple_epochs(\n",
    "#                 nb_e, m, training_loader, validation_loader, loss_fn, optimizer, m_name,\n",
    "#                 folder_to_save, True)[0]\n",
    "#             t_stop_train = time.perf_counter()\n",
    "\n",
    "#             for mt in [\"min_val\", \"all_epochs\"]:\n",
    "#                 t_start_eval = time.perf_counter()\n",
    "#                 path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "#                 # Retreive model state and set to evaluation mode\n",
    "#                 m.load_state_dict(torch.load(path))\n",
    "#                 m.eval()\n",
    "\n",
    "#                 # Calculate losses\n",
    "#                 test_predictions = m(d_ft_in[\"test\"].float())\n",
    "#                 test_loss = loss_fn(test_predictions[0].squeeze(), d_ft_out[\"test\"], test_predictions[1].squeeze(),\n",
    "#                                     d_ft_inter[\"test\"])\n",
    "#                 test_loss_t_mse = loss_t_mse(test_predictions[0].squeeze(), d_ft_out[\"test\"])\n",
    "#                 test_loss_mae = loss_mae(test_predictions[0].squeeze(), d_ft_out[\"test\"])\n",
    "#                 manual_diff = test_predictions[0].detach().numpy().transpose() - d_ft_out[\"test\"].numpy()\n",
    "#                 Te_l_mae_man = np.mean(np.abs(manual_diff))\n",
    "\n",
    "#                 train_predictions = m(d_ft_in[\"train\"].float())\n",
    "#                 train_loss = loss_fn(train_predictions[0].squeeze(), d_ft_out[\"train\"], train_predictions[1].squeeze(),\n",
    "#                                      d_ft_inter[\"train\"])\n",
    "#                 train_loss_t_mse = loss_t_mse(train_predictions[0].squeeze(), d_ft_out[\"train\"])\n",
    "#                 train_loss_mae = loss_mae(train_predictions[0].squeeze(), d_ft_out[\"train\"])\n",
    "\n",
    "#                 validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "#                 validation_loss = loss_fn(validation_prediction[0].squeeze(), d_ft_out[\"val\"],\n",
    "#                                           validation_prediction[1].squeeze(), d_ft_inter[\"val\"])\n",
    "#                 validation_loss_t_mse = loss_t_mse(validation_prediction[0].squeeze(), d_ft_out[\"val\"])\n",
    "#                 validation_loss_mae = loss_mae(validation_prediction[0].squeeze(), d_ft_out[\"val\"])\n",
    "\n",
    "\n",
    "#                 t_stop_eval = time.perf_counter()\n",
    "\n",
    "#                 fy_prediction = m(t_in_fy.float())\n",
    "\n",
    "#                 fy_l = loss_fn(fy_prediction[0].squeeze(), t_out_fy,fy_prediction[1].squeeze(),t_inter_fy)\n",
    "#                 fy_l_mse = loss_t_mse(fy_prediction[0].squeeze(), t_out_fy)\n",
    "#                 fy_l_mae = loss_mae(fy_prediction[0].squeeze(), t_out_fy)\n",
    "\n",
    "#                 manual_diff = fy_prediction[0].detach().numpy().transpose() - t_out_fy.numpy()\n",
    "#                 fy_l_mae_man = np.mean(np.abs(manual_diff))\n",
    "\n",
    "#                 # Calculate some calculation times\n",
    "#                 t_train = t_stop_train - t_start_train\n",
    "#                 t_eval = t_stop_eval - t_start_eval\n",
    "\n",
    "#                 #Calculate some losses manually\n",
    "\n",
    "\n",
    "#                 # Finally, save all desired values in a dataframe\n",
    "#                 r = pd.DataFrame({\"Model_type\": [nb_hidden],\n",
    "#                                   \"Dor\": dor,\n",
    "#                                   \"Relu_out\": relu_out,\n",
    "#                                   \"Batch_size\": bs,\n",
    "#                                   \"Lr\": lr,\n",
    "#                                   \"Epochs\": nb_e,\n",
    "#                                   \"Min_val\": mt,\n",
    "#                                   \"Nb_hours_used\":nb_hours_used,\n",
    "#                                   \"Sel_method\" : selection_method,\n",
    "#                                   \"Norm_method\": normalization,\n",
    "#                                   \"Tr_l\": train_loss.item(),\n",
    "#                                   \"Te_l\": test_loss.item(),\n",
    "#                                   \"V_l\": validation_loss.item(),\n",
    "#                                   \"Tr_l_mse\": train_loss_t_mse.item(),\n",
    "#                                   \"Te_l_mse\": test_loss_t_mse.item(),\n",
    "#                                   \"V_l_mse\": validation_loss_t_mse.item(),\n",
    "#                                   \"Tr_l_mae\": train_loss_mae.item(),\n",
    "#                                   \"Te_l_mae\": test_loss_mae.item(),\n",
    "#                                   \"Te_l_mae_man\": Te_l_mae_man ,\n",
    "#                                   \"V_l_mae\": validation_loss_mae.item(),\n",
    "#                                   \"fy_l\":fy_l.item(),\n",
    "#                                   \"fy_l_mse\":fy_l_mse.item(),\n",
    "#                                   \"fy_l_mae\":fy_l_mae.item(),\n",
    "#                                   \"fy_l_mae_man\":fy_l_mae_man,\n",
    "#                                   \"Train_time\": t_train,\n",
    "#                                   \"Eval_time\": t_eval,\n",
    "#                                   \"alpha\": alpha,\n",
    "#                                   \"beta\": beta,\n",
    "#                                   \"MAE\": MAE,\n",
    "#                                   \"Test size\": te_s,\n",
    "#                                   \"Val size\": val_s\n",
    "#                                   }\n",
    "#                                  , index=[i])\n",
    "#                 i += 1\n",
    "#                 results = pd.concat([results, r])\n",
    "#             results.to_csv(f\"Loss_results_csv/{exec_name}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55715f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0511, dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ft_out[\"train\"].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e059dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_in_fy, t_out_fy, t_inter_fy, maxs = DataLoading.concat_all_exec_fy(dfs_in_full, dfs_out_full, dfs_inter_j_full,all_executions,normalization=\"min-max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fba0a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6144, 0.6858, 0.5616, 0.2394, 0.3405, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000], dtype=torch.float64),\n",
       " tensor([-0.2224,  0.6647,  0.7620], dtype=torch.float64),\n",
       " tensor(0.2733, dtype=torch.float64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_in_fy.mean(dim=0),t_inter_fy.mean(dim=0),t_out_fy.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f62b358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1405, 0.1122, 0.1228, 0.3096, 0.2013, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000], dtype=torch.float64),\n",
       " tensor([0.4689, 0.2630, 0.2525], dtype=torch.float64),\n",
       " tensor(0.0932, dtype=torch.float64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_in_fy.std(dim=0),t_inter_fy.std(dim=0),t_out_fy.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a256ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0229, 0.0415, 0.1034, 0.0112, 0.0340, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000], dtype=torch.float64),\n",
       " tensor([-0.0117,  0.0399,  0.0457], dtype=torch.float64),\n",
       " tensor(7.7391, dtype=torch.float64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t_in_fy*maxs[\"in_scalar\"] + maxs['in_shift']).mean(dim=0),(t_inter_fy*maxs[\"inter_scalar\"] + maxs['inter_shift']).mean(dim=0),(t_out_fy*maxs[\"out_scalar\"] + maxs['out_shift']).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29e62248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0229, 0.0415, 0.1034, 0.0112, 0.0340, 0.5000, 0.5000, 0.5000, 0.5000,\n",
       "         0.5000, 0.5000, 0.5000, 0.5000, 0.5000], dtype=torch.float64),\n",
       " tensor([-0.0117,  0.0399,  0.0457], dtype=torch.float64),\n",
       " tensor(7.7391, dtype=torch.float64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t_in_fy*maxs[\"in_scalar\"] + maxs['in_shift']).mean(dim=0),(t_inter_fy*maxs[\"inter_scalar\"] + maxs['inter_shift']).mean(dim=0),(t_out_fy*maxs[\"out_scalar\"] + maxs['out_shift']).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59c9d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_pred = (fy_prediction[0]*maxs[\"out\"]).detach().numpy()\n",
    "np_out = (t_out_fy*maxs[\"out\"]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1798f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m offs \u001b[38;5;241m=\u001b[39m ly\u001b[38;5;241m*\u001b[39mnby\n\u001b[0;32m      7\u001b[0m offs2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m----> 8\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mnp_pred\u001b[49m[s:e],label\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEx1 est\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np_out[s:e],label\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEx1 act\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np_pred[s\u001b[38;5;241m+\u001b[39moffs\u001b[38;5;241m+\u001b[39moffs2:e\u001b[38;5;241m+\u001b[39moffs\u001b[38;5;241m+\u001b[39moffs2],label\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEx2 est\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_pred' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "s=0\n",
    "e= 120\n",
    "ly = 8736\n",
    "nby = 3\n",
    "offs = ly*nby\n",
    "offs2 = 2\n",
    "plt.plot(np_pred[s:e],label= \"Ex1 est\")\n",
    "plt.plot(np_out[s:e],label= \"Ex1 act\")\n",
    "plt.plot(np_pred[s+offs+offs2:e+offs+offs2],label= \"Ex2 est\")\n",
    "plt.plot(np_out[s+offs+offs2:e+offs+offs2],label= \"Ex2 act\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "(t_out_fy[0:ly] - t_out_fy[ly*3:ly*4]).sum()*maxs[\"out\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
