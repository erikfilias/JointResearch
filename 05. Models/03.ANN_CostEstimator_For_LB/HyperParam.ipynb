{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b42ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NN_classes\n",
    "from torchvision import datasets, transforms\n",
    "import training_methods\n",
    "import DataLoading\n",
    "import pivottablejs\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cca8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_101_N_105_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_102_N_106_cac1\",\"Network_Line_Out_N_103_N_109_cac1\"]\n",
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\"]\n",
    "#executions = [\"Network_Full_Generation_Full\",\"Network_Line_In_N_101_N_102_cac1\",\"Network_Line_In_N_101_N_103_cac1\",\"Network_Line_In_N_101_N_105_cac1\"]\n",
    "\n",
    "executions = [\"Network_Existing_Generation_Full\"]\n",
    "\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"../Data/RTS24_AC_12w\"\n",
    "all_executions = DataLoading.list_executions(folder=\"../Data/RTS24_AC_12w\",per = period,sc=sc)\n",
    "executions = all_executions[1:10]\n",
    "te_s = 0.1\n",
    "val_s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f16d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Line_In_N_101_N_102_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_103_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_105_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_104_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_106_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_124_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_104_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_105_N_110_cac1_2030.csv\n",
      "1227\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te(dfs_in,dfs_out,executions,te_s,val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4032fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out = DataLoading.concat_and_normalize(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da40d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "training_loader = DataLoader(train,batch_size=32)\n",
    "validation_loader = DataLoader(train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4489ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nb_hidden, input_size, dropout_ratio):\n",
    "    hidden_sizes = []\n",
    "    if nb_hidden == 0:\n",
    "        hidden_sizes.append(input_size)\n",
    "    elif nb_hidden == 1:\n",
    "        hidden_sizes.extend([int(math.sqrt(input_size))])\n",
    "    elif nb_hidden == 2:\n",
    "        hidden_sizes.extend([int(math.sqrt(input_size)), int(math.sqrt(math.sqrt(input_size)))])\n",
    "    elif nb_hidden == 3:\n",
    "        hidden_sizes.extend([int(input_size / 4), int(input_size / 16), int(input_size / 64)])\n",
    "\n",
    "\n",
    "\n",
    "    if nb_hidden == 0:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_Single_layer\n",
    "    elif nb_hidden == 1:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_1hidden_layer\n",
    "    elif nb_hidden == 2:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_2hidden_layer\n",
    "    elif nb_hidden == 3:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_3hidden_layer\n",
    "    model = model_class(input_size=input_size, hidden_sizes=hidden_sizes, output_size=1, dropout_ratio=dor)\n",
    "    print(model,dor,nb_hidden)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c9493ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002182842195034027\n",
      "  batch 101 loss: 0.0944562729226891\n",
      "  batch 201 loss: 0.0002686175112830824\n",
      "  batch 301 loss: 0.00014778208041207107\n",
      "  batch 401 loss: 0.0005557895180027117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.02209590472265729 valid 0.0030495452228933573\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00010719413869082928\n",
      "  batch 101 loss: 0.0011753741141706086\n",
      "  batch 201 loss: 7.706170817982639e-05\n",
      "  batch 301 loss: 0.00016723255129818426\n",
      "  batch 401 loss: 0.0009362949762362405\n",
      "LOSS train 0.0017995639664134678 valid 0.125700443983078\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.001716688424348831\n",
      "  batch 101 loss: 0.021757008366912488\n",
      "  batch 201 loss: 0.0008550262528660824\n",
      "  batch 301 loss: 0.00036528385641531713\n",
      "  batch 401 loss: 0.0007340386781947928\n",
      "LOSS train 0.005778163674911834 valid 0.0006494916160590947\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.1327551803551615e-06\n",
      "  batch 101 loss: 0.00020292409256626344\n",
      "  batch 201 loss: 0.00013817979303212268\n",
      "  batch 301 loss: 5.108832147925568e-05\n",
      "  batch 401 loss: 0.0002910484134372382\n",
      "LOSS train 0.00016061930331099787 valid 0.000310040166368708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1971])) that is different to the input size (torch.Size([1971, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([14148])) that is different to the input size (torch.Size([14148, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([3537])) that is different to the input size (torch.Size([3537, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0019286046922206879\n",
      "  batch 101 loss: 0.10165136162191629\n",
      "  batch 201 loss: 0.002761654840433039\n",
      "  batch 301 loss: 0.0011659573629731312\n",
      "  batch 401 loss: 0.000354746902739862\n",
      "LOSS train 0.024364445742974417 valid 0.00032597052631899714\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.273680577985943e-07\n",
      "  batch 101 loss: 8.759126347285929e-05\n",
      "  batch 201 loss: 3.798780745910335e-05\n",
      "  batch 301 loss: 6.65052009799183e-05\n",
      "  batch 401 loss: 0.00020504814263404115\n",
      "LOSS train 0.00011804980138746673 valid 7.29520179447718e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0064509842777625e-06\n",
      "  batch 101 loss: 0.00012569389261443575\n",
      "  batch 201 loss: 7.460362454366986e-05\n",
      "  batch 301 loss: 6.127745436970144e-05\n",
      "  batch 401 loss: 5.0284304988963414e-05\n",
      "LOSS train 9.134343034730626e-05 valid 0.0007858521421439946\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.750483346171678e-06\n",
      "  batch 101 loss: 0.0012197925100917927\n",
      "  batch 201 loss: 0.010186624136549654\n",
      "  batch 301 loss: 0.006018297297414392\n",
      "  batch 401 loss: 0.0018698159954510629\n",
      "LOSS train 0.004448264852428735 valid 0.0009911709930747747\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.493540644645691e-05\n",
      "  batch 101 loss: 0.0477991276467219\n",
      "  batch 201 loss: 0.0012828770163469017\n",
      "  batch 301 loss: 0.00017876922327559442\n",
      "  batch 401 loss: 0.00012757305858031033\n",
      "LOSS train 0.011175518816756925 valid 0.0007830672548152506\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.974000182002783e-06\n",
      "  batch 101 loss: 0.0014114343099936378\n",
      "  batch 201 loss: 0.00045745493509457446\n",
      "  batch 301 loss: 7.067173816722061e-05\n",
      "  batch 401 loss: 0.0012877404408754955\n",
      "LOSS train 0.0021744751477466126 valid 0.0023261425085365772\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.1561541836708786e-05\n",
      "  batch 101 loss: 0.00725766319403192\n",
      "  batch 201 loss: 0.0003779877469787607\n",
      "  batch 301 loss: 0.0006077752454802976\n",
      "  batch 401 loss: 0.011542033084333525\n",
      "LOSS train 0.007299232393064398 valid 0.05924960598349571\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.0007700829952955245\n",
      "  batch 101 loss: 0.02906528500141576\n",
      "  batch 201 loss: 0.004945807037875056\n",
      "  batch 301 loss: 0.0026435454045713415\n",
      "  batch 401 loss: 0.001862495968089206\n",
      "LOSS train 0.009070986212398352 valid 0.0044899131171405315\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003954368829727173\n",
      "  batch 101 loss: 0.1854044985724613\n",
      "  batch 201 loss: 0.001899839755060384\n",
      "  batch 301 loss: 9.404710919625359e-05\n",
      "  batch 401 loss: 6.035110188349791e-05\n",
      "LOSS train 0.042412070986376775 valid 0.00018395727965980768\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.028347953455523e-06\n",
      "  batch 101 loss: 0.00010295972184394486\n",
      "  batch 201 loss: 6.694958757634595e-05\n",
      "  batch 301 loss: 4.951788869675511e-05\n",
      "  batch 401 loss: 5.2242693436710395e-05\n",
      "LOSS train 6.571446446909224e-05 valid 6.575811130460352e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.2988489440176636e-07\n",
      "  batch 101 loss: 9.057374747499125e-05\n",
      "  batch 201 loss: 8.685580982273678e-05\n",
      "  batch 301 loss: 0.00010698954238250736\n",
      "  batch 401 loss: 0.0003020742887019878\n",
      "LOSS train 0.0003502904425123084 valid 0.002446809783577919\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.3248803336173297e-05\n",
      "  batch 101 loss: 0.017446893753949554\n",
      "  batch 201 loss: 0.016302285883575678\n",
      "  batch 301 loss: 0.0019156090068281628\n",
      "  batch 401 loss: 0.002410036460496485\n",
      "LOSS train 0.009061879628644331 valid 0.03441052883863449\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00025784607976675033\n",
      "  batch 101 loss: 0.032388269515349746\n",
      "  batch 201 loss: 7.781993999742554e-05\n",
      "  batch 301 loss: 9.097183941094045e-05\n",
      "  batch 401 loss: 9.937544891499783e-05\n",
      "LOSS train 0.007438276356501199 valid 0.0001280133001273498\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7670010856818408e-06\n",
      "  batch 101 loss: 0.00010640024241411084\n",
      "  batch 201 loss: 0.0001098362577826606\n",
      "  batch 301 loss: 0.00010910572553626707\n",
      "  batch 401 loss: 0.00010605080737605021\n",
      "LOSS train 0.00010499325986898105 valid 0.0002220680471509695\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.16647463478148e-06\n",
      "  batch 101 loss: 0.00010036840827297055\n",
      "  batch 201 loss: 9.5340201623344e-05\n",
      "  batch 301 loss: 8.692701368659072e-05\n",
      "  batch 401 loss: 8.012413184587785e-05\n",
      "LOSS train 8.817852117384394e-05 valid 0.00019784098549280316\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.8230936732143162e-06\n",
      "  batch 101 loss: 7.515940059477089e-05\n",
      "  batch 201 loss: 7.1069358477871e-05\n",
      "  batch 301 loss: 6.560179011785294e-05\n",
      "  batch 401 loss: 6.241272784677676e-05\n",
      "LOSS train 6.732234259030647e-05 valid 0.00016452714044135064\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002453665807843208\n",
      "  batch 101 loss: 0.06626098313234252\n",
      "  batch 201 loss: 0.00011907184377378143\n",
      "  batch 301 loss: 0.00027582456922573326\n",
      "  batch 401 loss: 0.00010025941378444259\n",
      "LOSS train 0.015132666997131117 valid 8.532522042514756e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0072258010040968e-06\n",
      "  batch 101 loss: 9.409256032768099e-05\n",
      "  batch 201 loss: 9.874564667597952e-05\n",
      "  batch 301 loss: 0.00010266173939271539\n",
      "  batch 401 loss: 0.00010644053274745602\n",
      "LOSS train 9.931886805292724e-05 valid 0.00015479265130124986\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1875687525607644e-06\n",
      "  batch 101 loss: 0.00010913609359505471\n",
      "  batch 201 loss: 0.00011044762581093437\n",
      "  batch 301 loss: 0.00010851352412089455\n",
      "  batch 401 loss: 0.00010569569077347296\n",
      "LOSS train 0.0001056603796135449 valid 0.00022071115381550044\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1474512070417406e-06\n",
      "  batch 101 loss: 0.00010150138214612525\n",
      "  batch 201 loss: 9.789738036261041e-05\n",
      "  batch 301 loss: 9.078523607115585e-05\n",
      "  batch 401 loss: 8.478769417365584e-05\n",
      "LOSS train 9.119927383840221e-05 valid 0.00020707993826363236\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.171961521729827e-05\n",
      "  batch 101 loss: 0.012483225501132438\n",
      "  batch 201 loss: 9.778185762570501e-05\n",
      "  batch 301 loss: 0.00010804979519889457\n",
      "  batch 401 loss: 0.00010727026087465674\n",
      "LOSS train 0.0029028591190264786 valid 0.0002255644794786349\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.215380129404366e-06\n",
      "  batch 101 loss: 9.66088350205041e-05\n",
      "  batch 201 loss: 8.709560551551477e-05\n",
      "  batch 301 loss: 7.603949314557213e-05\n",
      "  batch 401 loss: 6.874979771282597e-05\n",
      "LOSS train 7.987737898453946e-05 valid 0.00017400539945811033\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.476039808243513e-06\n",
      "  batch 101 loss: 6.517715505879096e-05\n",
      "  batch 201 loss: 6.213890952153634e-05\n",
      "  batch 301 loss: 5.865516101493995e-05\n",
      "  batch 401 loss: 5.75624576458722e-05\n",
      "LOSS train 6.026707926306674e-05 valid 0.00015679073112551123\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2180064115673302e-06\n",
      "  batch 101 loss: 6.0144733131437535e-05\n",
      "  batch 201 loss: 5.9699683596363684e-05\n",
      "  batch 301 loss: 5.85331421234514e-05\n",
      "  batch 401 loss: 5.926468593486334e-05\n",
      "LOSS train 5.943324604593595e-05 valid 0.00014781691425014287\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00018508927896618844\n",
      "  batch 101 loss: 0.028067830085428794\n",
      "  batch 201 loss: 8.57293880017096e-05\n",
      "  batch 301 loss: 9.513141248817192e-05\n",
      "  batch 401 loss: 0.00010363194485762505\n",
      "LOSS train 0.006450202177369247 valid 0.00015176381566561759\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.1412150817923247e-06\n",
      "  batch 101 loss: 0.00010887509663319861\n",
      "  batch 201 loss: 0.00011008046934279037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 0.00010576161986080024\n",
      "  batch 401 loss: 9.943925805828258e-05\n",
      "LOSS train 0.00010286914432304202 valid 0.00022368664212990552\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.189128474332392e-06\n",
      "  batch 101 loss: 9.171075853430466e-05\n",
      "  batch 201 loss: 8.57118379599342e-05\n",
      "  batch 301 loss: 7.742300143434022e-05\n",
      "  batch 401 loss: 7.153913338811435e-05\n",
      "LOSS train 7.955470673951078e-05 valid 0.00018139780149795115\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.5848124641925095e-06\n",
      "  batch 101 loss: 6.827742295286044e-05\n",
      "  batch 201 loss: 0.012258925817292265\n",
      "  batch 301 loss: 0.00041256655990423496\n",
      "  batch 401 loss: 8.420100535602159e-05\n",
      "LOSS train 0.002901175626292448 valid 0.00020535623480100185\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004360442981123924\n",
      "  batch 101 loss: 0.009743818741628728\n",
      "  batch 201 loss: 3.85178352416915e-05\n",
      "  batch 301 loss: 3.29692337089682e-05\n",
      "  batch 401 loss: 2.522982467894508e-05\n",
      "LOSS train 0.002321889183752706 valid 4.338960934546776e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.494116627844051e-08\n",
      "  batch 101 loss: 2.7076208217522434e-05\n",
      "  batch 201 loss: 8.120282448516036e-06\n",
      "  batch 301 loss: 3.907887576133362e-06\n",
      "  batch 401 loss: 4.064408816759624e-06\n",
      "LOSS train 1.0133730799867904e-05 valid 4.798834561370313e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.11399913294008e-08\n",
      "  batch 101 loss: 3.7069808207235153e-06\n",
      "  batch 201 loss: 3.637132827378764e-06\n",
      "  batch 301 loss: 4.040331766219651e-06\n",
      "  batch 401 loss: 2.4648583814013136e-06\n",
      "LOSS train 3.298322116385695e-06 valid 5.2288502047304064e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.707922693749424e-08\n",
      "  batch 101 loss: 1.7135289210301607e-06\n",
      "  batch 201 loss: 3.0024876888035123e-06\n",
      "  batch 301 loss: 2.283291635905016e-06\n",
      "  batch 401 loss: 4.819033441165743e-06\n",
      "LOSS train 3.009796931254514e-06 valid 5.226091525400989e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00012801649048924446\n",
      "  batch 101 loss: 0.000313796211448647\n",
      "  batch 201 loss: 5.573326786702637e-05\n",
      "  batch 301 loss: 4.828521989281853e-05\n",
      "  batch 401 loss: 4.5799705833360346e-05\n",
      "LOSS train 0.00013764045917738895 valid 6.983225466683507e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.492744432762266e-07\n",
      "  batch 101 loss: 4.5725311765352216e-05\n",
      "  batch 201 loss: 4.7711568057025034e-05\n",
      "  batch 301 loss: 4.918006752802739e-05\n",
      "  batch 401 loss: 4.86538253413471e-05\n",
      "LOSS train 4.790047787595716e-05 valid 6.175455928314477e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.305300222360529e-07\n",
      "  batch 101 loss: 4.344725055290155e-05\n",
      "  batch 201 loss: 4.3851978168731876e-05\n",
      "  batch 301 loss: 4.293470782727127e-05\n",
      "  batch 401 loss: 4.0797701427663927e-05\n",
      "LOSS train 4.279572817844409e-05 valid 6.606429815292358e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.034667275234824e-07\n",
      "  batch 101 loss: 3.7702100658520973e-05\n",
      "  batch 201 loss: 3.7914111397299164e-05\n",
      "  batch 301 loss: 3.751907419996314e-05\n",
      "  batch 401 loss: 3.669513800389268e-05\n",
      "LOSS train 3.777188644786466e-05 valid 6.468757055699825e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.0238871220499276e-05\n",
      "  batch 101 loss: 0.0005189665915594332\n",
      "  batch 201 loss: 5.489473318789351e-05\n",
      "  batch 301 loss: 4.8414566410883705e-05\n",
      "  batch 401 loss: 4.640545055735856e-05\n",
      "LOSS train 0.00015988576425730636 valid 7.116196502465755e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.851732905488461e-07\n",
      "  batch 101 loss: 4.69093894668049e-05\n",
      "  batch 201 loss: 4.803310986375209e-05\n",
      "  batch 301 loss: 4.9630293606810486e-05\n",
      "  batch 401 loss: 4.840173370979528e-05\n",
      "LOSS train 4.823704353064511e-05 valid 6.220871728146449e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.984226400963962e-07\n",
      "  batch 101 loss: 4.283286751046944e-05\n",
      "  batch 201 loss: 4.387318799899731e-05\n",
      "  batch 301 loss: 4.18957585395674e-05\n",
      "  batch 401 loss: 4.0456195950469006e-05\n",
      "LOSS train 4.231278290213435e-05 valid 6.613667937926948e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0246651072520762e-07\n",
      "  batch 101 loss: 3.7390719770655776e-05\n",
      "  batch 201 loss: 3.747313085170845e-05\n",
      "  batch 301 loss: 3.718159390246001e-05\n",
      "  batch 401 loss: 3.66211911708092e-05\n",
      "LOSS train 3.750010841209427e-05 valid 6.435939576476812e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 9.279235266149044e-05\n",
      "  batch 101 loss: 0.0021234974658217707\n",
      "  batch 201 loss: 5.954135543163375e-05\n",
      "  batch 301 loss: 5.911240863838429e-05\n",
      "  batch 401 loss: 5.768575293870981e-05\n",
      "LOSS train 0.0005452205918409464 valid 9.012301597977057e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1028941662516445e-06\n",
      "  batch 101 loss: 4.9523756952112305e-05\n",
      "  batch 201 loss: 4.643716879400017e-05\n",
      "  batch 301 loss: 4.531691063107246e-05\n",
      "  batch 401 loss: 4.597662134699476e-05\n",
      "LOSS train 4.683347440156814e-05 valid 7.148300937842578e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.936024146853015e-07\n",
      "  batch 101 loss: 4.529966681843689e-05\n",
      "  batch 201 loss: 4.671897315517981e-05\n",
      "  batch 301 loss: 4.845191699189399e-05\n",
      "  batch 401 loss: 4.9560107425463686e-05\n",
      "LOSS train 4.7676559917923674e-05 valid 6.307817238848656e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.148793232161552e-07\n",
      "  batch 101 loss: 4.626253193634966e-05\n",
      "  batch 201 loss: 4.749944996149225e-05\n",
      "  batch 301 loss: 4.730555562559857e-05\n",
      "  batch 401 loss: 4.5819268070772526e-05\n",
      "LOSS train 4.670432305691242e-05 valid 6.334663339657709e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001250611338764429\n",
      "  batch 101 loss: 0.014668766721214296\n",
      "  batch 201 loss: 3.59270827061664e-05\n",
      "  batch 301 loss: 4.426364622190704e-05\n",
      "  batch 401 loss: 4.1452853241707996e-05\n",
      "LOSS train 0.0033704704920218265 valid 6.508546357508749e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.266835407586768e-07\n",
      "  batch 101 loss: 3.3709239329482446e-05\n",
      "  batch 201 loss: 2.5827786715808542e-05\n",
      "  batch 301 loss: 2.1938957950169424e-05\n",
      "  batch 401 loss: 1.7418058101839053e-05\n",
      "LOSS train 2.3813390495537528e-05 valid 4.5246055378811434e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.661565667949617e-07\n",
      "  batch 101 loss: 1.026035850372864e-05\n",
      "  batch 201 loss: 5.212522185615853e-06\n",
      "  batch 301 loss: 2.068421829477529e-06\n",
      "  batch 401 loss: 1.4046801696565581e-06\n",
      "LOSS train 4.4647267252734985e-06 valid 4.631564297596924e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.367243467915614e-09\n",
      "  batch 101 loss: 1.2824103025366186e-06\n",
      "  batch 201 loss: 1.3001990889449643e-06\n",
      "  batch 301 loss: 1.1892139696101366e-06\n",
      "  batch 401 loss: 1.276344057004053e-06\n",
      "LOSS train 1.254040042786186e-06 valid 4.7595032810932025e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003579498827457428\n",
      "  batch 101 loss: 0.03836251385509968\n",
      "  batch 201 loss: 0.0010479211757774465\n",
      "  batch 301 loss: 0.00048764757346361873\n",
      "  batch 401 loss: 0.000305646386477747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.009175207235010916 valid 7.783670298522338e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.6198552222922442e-06\n",
      "  batch 101 loss: 0.00014253932793508283\n",
      "  batch 201 loss: 9.276791335651069e-05\n",
      "  batch 301 loss: 8.486363916745177e-05\n",
      "  batch 401 loss: 8.112609168165364e-05\n",
      "LOSS train 9.812878871132393e-05 valid 0.00017225848569069058\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0652398709207772e-06\n",
      "  batch 101 loss: 8.679354496962332e-05\n",
      "  batch 201 loss: 8.161289058080001e-05\n",
      "  batch 301 loss: 7.401689040307247e-05\n",
      "  batch 401 loss: 6.921253398104455e-05\n",
      "LOSS train 7.645998402641816e-05 valid 0.0001660757843637839\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.343585656490177e-06\n",
      "  batch 101 loss: 6.656155017935817e-05\n",
      "  batch 201 loss: 6.341146097838646e-05\n",
      "  batch 301 loss: 5.991584435150799e-05\n",
      "  batch 401 loss: 5.9436834852704124e-05\n",
      "LOSS train 6.169499894415741e-05 valid 0.0001519691286375746\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00040864840149879456\n",
      "  batch 101 loss: 0.015862521202507196\n",
      "  batch 201 loss: 0.00030432689760345966\n",
      "  batch 301 loss: 0.00012545267069071997\n",
      "  batch 401 loss: 9.511891788861248e-05\n",
      "LOSS train 0.0037981614293272066 valid 0.00021610881958622485\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8999056667089462e-06\n",
      "  batch 101 loss: 9.17518117421423e-05\n",
      "  batch 201 loss: 8.234286758579401e-05\n",
      "  batch 301 loss: 7.351303790528618e-05\n",
      "  batch 401 loss: 6.739749343978474e-05\n",
      "LOSS train 7.709954491969273e-05 valid 9.957763541024178e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.4463148545473814e-06\n",
      "  batch 101 loss: 6.046425019576418e-05\n",
      "  batch 201 loss: 6.259507855702396e-05\n",
      "  batch 301 loss: 5.915143475931473e-05\n",
      "  batch 401 loss: 5.766172325706975e-05\n",
      "LOSS train 5.917175172170929e-05 valid 0.00015616528980899602\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.234064741060138e-06\n",
      "  batch 101 loss: 6.0387046239611664e-05\n",
      "  batch 201 loss: 5.96659625182383e-05\n",
      "  batch 301 loss: 5.799925282417462e-05\n",
      "  batch 401 loss: 5.871058290608744e-05\n",
      "LOSS train 5.912203262993712e-05 valid 0.00015224081289488822\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002058337442576885\n",
      "  batch 101 loss: 0.036315162699611395\n",
      "  batch 201 loss: 0.00010483394539505753\n",
      "  batch 301 loss: 0.00010323172029075067\n",
      "  batch 401 loss: 0.0001062344420847694\n",
      "LOSS train 0.00832338073758064 valid 0.00014828519488219172\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.079259720630944e-06\n",
      "  batch 101 loss: 0.00010915695497885736\n",
      "  batch 201 loss: 0.00011151336756057617\n",
      "  batch 301 loss: 0.00010782716555695514\n",
      "  batch 401 loss: 0.00010065558334190427\n",
      "LOSS train 0.0001041449032179335 valid 0.00022365311451721936\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.153239085804671e-06\n",
      "  batch 101 loss: 9.404936185774205e-05\n",
      "  batch 201 loss: 8.739811056102554e-05\n",
      "  batch 301 loss: 7.895308431557168e-05\n",
      "  batch 401 loss: 7.31584586696954e-05\n",
      "LOSS train 8.123648485917671e-05 valid 0.00018407613970339298\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.6300299214199185e-06\n",
      "  batch 101 loss: 6.939459165096195e-05\n",
      "  batch 201 loss: 6.60517482481282e-05\n",
      "  batch 301 loss: 6.16852709106297e-05\n",
      "  batch 401 loss: 5.962636327353721e-05\n",
      "LOSS train 6.324786832861158e-05 valid 0.00015930492372717708\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00010131999850273133\n",
      "  batch 101 loss: 2.8102652511012276\n",
      "  batch 201 loss: 0.001407574602344539\n",
      "  batch 301 loss: 0.0008313253630331019\n",
      "  batch 401 loss: 0.0003385460806930496\n",
      "LOSS train 0.6349966546812976 valid 0.0008797385962679982\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.009347220649943e-07\n",
      "  batch 101 loss: 0.00011729313079058556\n",
      "  batch 201 loss: 0.00014357701490780528\n",
      "  batch 301 loss: 0.0002872999378359964\n",
      "  batch 401 loss: 7.955655092473535e-05\n",
      "LOSS train 0.00014669046193593457 valid 0.000708047125954181\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.960927253705449e-07\n",
      "  batch 101 loss: 6.7539138678967e-05\n",
      "  batch 201 loss: 0.0001877866460807809\n",
      "  batch 301 loss: 0.000649126799107762\n",
      "  batch 401 loss: 0.00029155469218494545\n",
      "LOSS train 0.0002885944470228216 valid 0.00328096398152411\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.923043979331851e-05\n",
      "  batch 101 loss: 0.0006601323227187095\n",
      "  batch 201 loss: 0.0004445740676692367\n",
      "  batch 301 loss: 0.0005621273794258741\n",
      "  batch 401 loss: 0.0005982292745466112\n",
      "LOSS train 0.0005636965494475304 valid 0.0008863533148542047\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00043156031519174575\n",
      "  batch 101 loss: 2.7344958113087343\n",
      "  batch 201 loss: 0.007692865722347051\n",
      "  batch 301 loss: 0.0036506969423498957\n",
      "  batch 401 loss: 0.0011286138076684437\n",
      "LOSS train 0.6202382956765241 valid 0.0003515660355333239\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.583669016137719e-06\n",
      "  batch 101 loss: 0.00042241834933520294\n",
      "  batch 201 loss: 0.000127265712726512\n",
      "  batch 301 loss: 4.906947059680533e-05\n",
      "  batch 401 loss: 4.0998411195687366e-05\n",
      "LOSS train 0.00014849599721104655 valid 0.0001207551613333635\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.83908287808299e-06\n",
      "  batch 101 loss: 9.075963751456584e-05\n",
      "  batch 201 loss: 4.601569880833267e-05\n",
      "  batch 301 loss: 2.8786965135623177e-05\n",
      "  batch 401 loss: 0.00011230096145936841\n",
      "LOSS train 8.594369478025419e-05 valid 0.0003347884921822697\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.968975019641221e-06\n",
      "  batch 101 loss: 0.0006168691651328118\n",
      "  batch 201 loss: 0.00019422580939135515\n",
      "  batch 301 loss: 0.00013570576925303612\n",
      "  batch 401 loss: 3.164478609960497e-05\n",
      "LOSS train 0.00022588759442027383 valid 5.68516188650392e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00011521586216986179\n",
      "  batch 101 loss: 2.8224247971177103\n",
      "  batch 201 loss: 0.012286195543128996\n",
      "  batch 301 loss: 0.003115849421010353\n",
      "  batch 401 loss: 0.0006065973294607829\n",
      "LOSS train 0.6407707921580388 valid 0.000195302942302078\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.179473082534969e-06\n",
      "  batch 101 loss: 0.00012889035495390998\n",
      "  batch 201 loss: 3.316159811220132e-05\n",
      "  batch 301 loss: 2.577125781272116e-05\n",
      "  batch 401 loss: 1.9409459105190764e-05\n",
      "LOSS train 5.041585381033048e-05 valid 6.0172103985678405e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.8430722522898575e-07\n",
      "  batch 101 loss: 2.3086554133442405e-05\n",
      "  batch 201 loss: 3.254747505707201e-05\n",
      "  batch 301 loss: 4.9139459842990615e-05\n",
      "  batch 401 loss: 8.851913640683052e-05\n",
      "LOSS train 8.841448741673531e-05 valid 0.00067802396370098\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.694713956676423e-06\n",
      "  batch 101 loss: 0.0003521990439730871\n",
      "  batch 201 loss: 0.00011865899370604893\n",
      "  batch 301 loss: 6.153792538952985e-05\n",
      "  batch 401 loss: 0.00012272574414055271\n",
      "LOSS train 0.00022840104606297056 valid 0.0001267727348022163\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00047407843172550204\n",
      "  batch 101 loss: 2.68127101842314\n",
      "  batch 201 loss: 0.016350710939150304\n",
      "  batch 301 loss: 0.0009062664717203006\n",
      "  batch 401 loss: 0.0003557295860082377\n",
      "LOSS train 0.6093523709865585 valid 0.00012188094842713326\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.638944175094365e-06\n",
      "  batch 101 loss: 0.0003375350312853698\n",
      "  batch 201 loss: 6.0874043620060546e-05\n",
      "  batch 301 loss: 4.806989068129042e-05\n",
      "  batch 401 loss: 4.2964701387973035e-05\n",
      "LOSS train 0.0001167509484295301 valid 7.052627188386396e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.9047892894595863e-07\n",
      "  batch 101 loss: 8.040714730668697e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 9.175960834909347e-05\n",
      "  batch 301 loss: 0.00012311155960560428\n",
      "  batch 401 loss: 0.0005043538645077205\n",
      "LOSS train 0.0004283110032841704 valid 0.008846352808177471\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.7820403277874e-05\n",
      "  batch 101 loss: 0.0026866770829656163\n",
      "  batch 201 loss: 0.0017617930579581298\n",
      "  batch 301 loss: 0.0019980057267821394\n",
      "  batch 401 loss: 0.009894407619722188\n",
      "LOSS train 0.016658539461556918 valid 1.1270445585250854\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00015809055417776108\n",
      "  batch 101 loss: 0.7768675114708095\n",
      "  batch 201 loss: 8.689303097526136e-05\n",
      "  batch 301 loss: 8.48380915067537e-05\n",
      "  batch 401 loss: 9.211899974388871e-05\n",
      "LOSS train 0.17546869476414587 valid 9.930014493875206e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2758185039274395e-06\n",
      "  batch 101 loss: 9.978621774052954e-05\n",
      "  batch 201 loss: 0.00010463165041073808\n",
      "  batch 301 loss: 0.00010763850342414116\n",
      "  batch 401 loss: 0.00010940639845728128\n",
      "LOSS train 0.0001034111857161655 valid 0.00019192829495295882\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.7379518724046647e-06\n",
      "  batch 101 loss: 0.00010872548184750031\n",
      "  batch 201 loss: 0.00010710806816405238\n",
      "  batch 301 loss: 0.00010119893500302623\n",
      "  batch 401 loss: 9.502631947611916e-05\n",
      "LOSS train 0.00010003638474773892 valid 0.00022047001402825117\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.144068468827754e-06\n",
      "  batch 101 loss: 8.873338260457331e-05\n",
      "  batch 201 loss: 8.375604526463576e-05\n",
      "  batch 301 loss: 7.640243142361668e-05\n",
      "  batch 401 loss: 7.118200548688946e-05\n",
      "LOSS train 7.811490024393378e-05 valid 0.00018136098515242338\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003330862894654274\n",
      "  batch 101 loss: 0.6865680335330399\n",
      "  batch 201 loss: 0.0024654588610275143\n",
      "  batch 301 loss: 0.0006136908341227354\n",
      "  batch 401 loss: 0.00010822839908996684\n",
      "LOSS train 0.1557786584030439 valid 3.98806732846424e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.890906241897028e-07\n",
      "  batch 101 loss: 6.302503198625686e-05\n",
      "  batch 201 loss: 0.00010715119127041817\n",
      "  batch 301 loss: 0.00010899238950287326\n",
      "  batch 401 loss: 0.00010922763129940449\n",
      "LOSS train 9.544968634950509e-05 valid 0.00020634154498111457\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.944527950603515e-06\n",
      "  batch 101 loss: 0.00010660075170335404\n",
      "  batch 201 loss: 0.00010347509529538002\n",
      "  batch 301 loss: 9.612590340566385e-05\n",
      "  batch 401 loss: 8.920497254507608e-05\n",
      "LOSS train 9.597559709270518e-05 valid 0.00021294014004524797\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.038060967810452e-06\n",
      "  batch 101 loss: 8.2985085313112e-05\n",
      "  batch 201 loss: 7.811160583855781e-05\n",
      "  batch 301 loss: 7.135363762017733e-05\n",
      "  batch 401 loss: 6.689763569283968e-05\n",
      "LOSS train 7.321741603646877e-05 valid 0.00017290022515226156\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.094327360391616e-05\n",
      "  batch 101 loss: 1.9061094231542666\n",
      "  batch 201 loss: 0.0002047272550589696\n",
      "  batch 301 loss: 7.492573844501749e-05\n",
      "  batch 401 loss: 7.95287527716937e-05\n",
      "LOSS train 0.4303778612072291 valid 7.171782635850832e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.997145828790962e-07\n",
      "  batch 101 loss: 8.475140610698872e-05\n",
      "  batch 201 loss: 8.827132129681559e-05\n",
      "  batch 301 loss: 9.173838320521099e-05\n",
      "  batch 401 loss: 9.598129562959912e-05\n",
      "LOSS train 9.000311742642652e-05 valid 0.00010340462904423475\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.349923841189593e-06\n",
      "  batch 101 loss: 0.00010067576185520011\n",
      "  batch 201 loss: 0.00010378238389648687\n",
      "  batch 301 loss: 0.00010601451353011271\n",
      "  batch 401 loss: 0.00010827689618110981\n",
      "LOSS train 0.00010309792941066325 valid 0.00016724792658351362\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.375592739554122e-06\n",
      "  batch 101 loss: 0.0001096712520373444\n",
      "  batch 201 loss: 0.00011032776284736201\n",
      "  batch 301 loss: 0.00010789956670862467\n",
      "  batch 401 loss: 0.000104999201512328\n",
      "LOSS train 0.00010545384258650535 valid 0.0002211972459917888\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0007517275214195252\n",
      "  batch 101 loss: 1.8087759482982801\n",
      "  batch 201 loss: 8.898513489839388e-05\n",
      "  batch 301 loss: 7.603690503856342e-05\n",
      "  batch 401 loss: 8.060269431098277e-05\n",
      "LOSS train 0.4085345279138206 valid 7.321689190575853e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.377676229225471e-07\n",
      "  batch 101 loss: 8.612232718405722e-05\n",
      "  batch 201 loss: 8.984644122392638e-05\n",
      "  batch 301 loss: 9.347836969482159e-05\n",
      "  batch 401 loss: 9.781414000826771e-05\n",
      "LOSS train 9.151528873202136e-05 valid 0.00010984154505422339\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.4630296209361405e-06\n",
      "  batch 101 loss: 0.00010246236346631577\n",
      "  batch 201 loss: 0.00010551865191359865\n",
      "  batch 301 loss: 0.00010739267602275505\n",
      "  batch 401 loss: 0.00010908433102827075\n",
      "LOSS train 0.00010429455798781423 valid 0.00017868007125798613\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.544951275922358e-06\n",
      "  batch 101 loss: 0.00010964443327267759\n",
      "  batch 201 loss: 0.00010961968789843014\n",
      "  batch 301 loss: 0.00010617022023780009\n",
      "  batch 401 loss: 0.00010233469928380145\n",
      "LOSS train 0.00010409376365624758 valid 0.00022371430532075465\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.3930675815790892e-05\n",
      "  batch 101 loss: 0.07797629800957338\n",
      "  batch 201 loss: 4.4465836551808025e-05\n",
      "  batch 301 loss: 2.798845298229935e-05\n",
      "  batch 401 loss: 2.3480795184696036e-05\n",
      "LOSS train 0.017631005027585005 valid 4.3090065446449444e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.041245832311688e-07\n",
      "  batch 101 loss: 1.8576903381699595e-05\n",
      "  batch 201 loss: 1.7483498946262444e-05\n",
      "  batch 301 loss: 1.2969856300912851e-05\n",
      "  batch 401 loss: 6.676749837311036e-06\n",
      "LOSS train 1.3062717598096297e-05 valid 4.8789206630317494e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0346650469728048e-08\n",
      "  batch 101 loss: 5.536833720896084e-06\n",
      "  batch 201 loss: 6.0937258731996735e-06\n",
      "  batch 301 loss: 7.217389248239669e-06\n",
      "  batch 401 loss: 6.376721434548926e-06\n",
      "LOSS train 6.237999425026292e-06 valid 5.827931090607308e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1450254027731715e-08\n",
      "  batch 101 loss: 5.776501352556807e-06\n",
      "  batch 201 loss: 5.6041067776391175e-06\n",
      "  batch 301 loss: 6.590883133696934e-06\n",
      "  batch 401 loss: 5.244383766864758e-06\n",
      "LOSS train 5.7083817123348575e-06 valid 5.708529715775512e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0009670770168304444\n",
      "  batch 101 loss: 0.20043397124041804\n",
      "  batch 201 loss: 0.0005146501069975784\n",
      "  batch 301 loss: 0.0001742261134495493\n",
      "  batch 401 loss: 0.00013606717904622202\n",
      "LOSS train 0.04566154469709379 valid 5.0445287342881784e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.6183148545678705e-06\n",
      "  batch 101 loss: 0.00010248710714222398\n",
      "  batch 201 loss: 7.85793585691863e-05\n",
      "  batch 301 loss: 6.177724842928001e-05\n",
      "  batch 401 loss: 5.1464315856719625e-05\n",
      "LOSS train 7.075684618100012e-05 valid 4.2848150769714266e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5811539924470708e-07\n",
      "  batch 101 loss: 3.393528458218498e-05\n",
      "  batch 201 loss: 2.4873533202480758e-05\n",
      "  batch 301 loss: 2.3358264270427755e-05\n",
      "  batch 401 loss: 2.1770100284470574e-05\n",
      "LOSS train 2.5027358538236863e-05 valid 4.038856422994286e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.132676490873564e-07\n",
      "  batch 101 loss: 2.0324581535078322e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 2.3900585572391718e-05\n",
      "  batch 301 loss: 2.2493402967711516e-05\n",
      "  batch 401 loss: 2.4202371634487462e-05\n",
      "LOSS train 2.5109387577108065e-05 valid 9.309567394666374e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00026735689491033557\n",
      "  batch 101 loss: 0.9581720612373101\n",
      "  batch 201 loss: 9.095572431760957e-05\n",
      "  batch 301 loss: 8.611850057377524e-05\n",
      "  batch 401 loss: 9.366346937667913e-05\n",
      "LOSS train 0.21642148635401914 valid 0.00010448905959492549\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.3692285574506968e-06\n",
      "  batch 101 loss: 0.00010138277127452966\n",
      "  batch 201 loss: 0.00010612431748768358\n",
      "  batch 301 loss: 0.00010857010805011669\n",
      "  batch 401 loss: 0.00010942873851078616\n",
      "LOSS train 0.00010419340369808623 valid 0.00020124216098338366\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.871812030207366e-06\n",
      "  batch 101 loss: 0.0001075499785338252\n",
      "  batch 201 loss: 0.00010498785030108593\n",
      "  batch 301 loss: 9.81352743701791e-05\n",
      "  batch 401 loss: 9.143424521255383e-05\n",
      "LOSS train 9.760060221352426e-05 valid 0.0002161281881853938\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.0830336618237196e-06\n",
      "  batch 101 loss: 8.512461550367335e-05\n",
      "  batch 201 loss: 8.017637255193222e-05\n",
      "  batch 301 loss: 7.31670969469178e-05\n",
      "  batch 401 loss: 6.841270341283234e-05\n",
      "LOSS train 7.499537522974239e-05 valid 0.00017588176706340164\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002934015989303589\n",
      "  batch 101 loss: 0.013083618875224374\n",
      "  batch 201 loss: 5.721904273656264e-05\n",
      "  batch 301 loss: 5.7165744137819275e-05\n",
      "  batch 401 loss: 5.3700616361993526e-05\n",
      "LOSS train 0.0036582559642179932 valid 7.479510531993583e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.762788300169632e-07\n",
      "  batch 101 loss: 4.4917925905565425e-05\n",
      "  batch 201 loss: 4.477623429465894e-05\n",
      "  batch 301 loss: 4.691660399657849e-05\n",
      "  batch 401 loss: 4.932114351333894e-05\n",
      "LOSS train 4.679092043352182e-05 valid 6.178348849061877e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.2525003916816787e-07\n",
      "  batch 101 loss: 4.468522826883259e-05\n",
      "  batch 201 loss: 4.355869795205081e-05\n",
      "  batch 301 loss: 4.047639380246437e-05\n",
      "  batch 401 loss: 3.770959425196452e-05\n",
      "LOSS train 4.1591326294880216e-05 valid 6.496589776361361e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.2058162610628642e-07\n",
      "  batch 101 loss: 3.578493676997141e-05\n",
      "  batch 201 loss: 3.5665283102019886e-05\n",
      "  batch 301 loss: 3.623921853773027e-05\n",
      "  batch 401 loss: 3.676420003941416e-05\n",
      "LOSS train 3.6621253492075e-05 valid 6.332076009130105e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00012661155313253402\n",
      "  batch 101 loss: 12.002956789242889\n",
      "  batch 201 loss: 0.002718063676256861\n",
      "  batch 301 loss: 6.832956966718485e-05\n",
      "  batch 401 loss: 4.236690897641893e-05\n",
      "LOSS train 2.71014098648664 valid 5.026452345191501e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.79085849999683e-08\n",
      "  batch 101 loss: 2.096080241130949e-05\n",
      "  batch 201 loss: 1.0059430008482196e-05\n",
      "  batch 301 loss: 5.510400206958366e-06\n",
      "  batch 401 loss: 3.432960190821177e-06\n",
      "LOSS train 9.299030377237328e-06 valid 4.733085006591864e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.066304270760156e-07\n",
      "  batch 101 loss: 4.8396062301492296e-06\n",
      "  batch 201 loss: 2.7125929761950827e-06\n",
      "  batch 301 loss: 3.94552085253963e-06\n",
      "  batch 401 loss: 3.6356931961734064e-06\n",
      "LOSS train 3.863218120390184e-06 valid 5.044024146627635e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.2413423468824475e-07\n",
      "  batch 101 loss: 5.915577026200936e-06\n",
      "  batch 201 loss: 6.142262311072955e-06\n",
      "  batch 301 loss: 7.130295629167449e-06\n",
      "  batch 401 loss: 4.9968720637139085e-06\n",
      "LOSS train 5.9687901687284545e-06 valid 5.037601658841595e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.6910787457600236e-06\n",
      "  batch 101 loss: 14.803986173785452\n",
      "  batch 201 loss: 0.0025786402499943504\n",
      "  batch 301 loss: 0.0001303287298500777\n",
      "  batch 401 loss: 6.589839734260749e-05\n",
      "LOSS train 3.3423920320420906 valid 6.211488653207198e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.5593726352090014e-07\n",
      "  batch 101 loss: 6.80147272942122e-05\n",
      "  batch 201 loss: 6.962029920032364e-05\n",
      "  batch 301 loss: 7.035470199298288e-05\n",
      "  batch 401 loss: 7.991322125690203e-05\n",
      "LOSS train 7.224913452069571e-05 valid 6.381326238624752e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.495005850913003e-07\n",
      "  batch 101 loss: 7.315769100387115e-05\n",
      "  batch 201 loss: 7.415006973133132e-05\n",
      "  batch 301 loss: 7.511892981710843e-05\n",
      "  batch 401 loss: 7.687465513754432e-05\n",
      "LOSS train 7.519602292219384e-05 valid 6.701004167553037e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.660568058374338e-07\n",
      "  batch 101 loss: 7.886348384545272e-05\n",
      "  batch 201 loss: 8.006539469533891e-05\n",
      "  batch 301 loss: 8.13207088958734e-05\n",
      "  batch 401 loss: 8.34708753745872e-05\n",
      "LOSS train 8.111822581271857e-05 valid 7.376328721875325e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.4713682252913715e-05\n",
      "  batch 101 loss: 40.02748385173036\n",
      "  batch 201 loss: 0.006130445112939924\n",
      "  batch 301 loss: 0.004991273595951497\n",
      "  batch 401 loss: 0.003664506406057626\n",
      "LOSS train 9.039180633477823 valid 0.00030126862111501396\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.7757768984884025e-05\n",
      "  batch 101 loss: 0.002313567072851583\n",
      "  batch 201 loss: 0.0015605821844656021\n",
      "  batch 301 loss: 0.001026490743388422\n",
      "  batch 401 loss: 0.0006250088370870799\n",
      "LOSS train 0.0013009149123087806 valid 0.00027720455545932055\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.527336616069078e-06\n",
      "  batch 101 loss: 0.0003492527724301908\n",
      "  batch 201 loss: 0.00022295708105957602\n",
      "  batch 301 loss: 0.00019712221284862608\n",
      "  batch 401 loss: 0.00015145989924349125\n",
      "LOSS train 0.000218996077823717 valid 0.00012904121831525117\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.960936981253326e-06\n",
      "  batch 101 loss: 0.00011964468354562996\n",
      "  batch 201 loss: 0.00011012400980689563\n",
      "  batch 301 loss: 8.948501897066308e-05\n",
      "  batch 401 loss: 8.693058412063692e-05\n",
      "LOSS train 0.00010105269397435167 valid 7.275634561665356e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00017406336963176727\n",
      "  batch 101 loss: 5.7454604371031746\n",
      "  batch 201 loss: 0.003060416832449846\n",
      "  batch 301 loss: 0.007587591176213664\n",
      "  batch 401 loss: 8.213886918383651e-05\n",
      "LOSS train 1.2994123962860897 valid 6.457695417338982e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.810657992493361e-07\n",
      "  batch 101 loss: 7.5205182274658e-05\n",
      "  batch 201 loss: 7.717930538092332e-05\n",
      "  batch 301 loss: 7.960566006659064e-05\n",
      "  batch 401 loss: 8.192745136057056e-05\n",
      "LOSS train 7.875915291755206e-05 valid 7.343968900386244e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.432940765284002e-07\n",
      "  batch 101 loss: 8.560264599509537e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 8.752197229114245e-05\n",
      "  batch 301 loss: 8.980792192460285e-05\n",
      "  batch 401 loss: 9.29437370064079e-05\n",
      "LOSS train 8.885552110163267e-05 valid 9.205032256431878e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.1401851224945859e-06\n",
      "  batch 101 loss: 9.656798488322238e-05\n",
      "  batch 201 loss: 9.915428307976981e-05\n",
      "  batch 301 loss: 0.00010123816450686718\n",
      "  batch 401 loss: 0.00010409364027282209\n",
      "LOSS train 9.926433318658087e-05 valid 0.00013328972272574902\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.1546082096174359e-05\n",
      "  batch 101 loss: 46.06924241340719\n",
      "  batch 201 loss: 0.024281597959343343\n",
      "  batch 301 loss: 0.005991633452649694\n",
      "  batch 401 loss: 0.002479370905057294\n",
      "LOSS train 10.406919818470975 valid 0.00046740405377931893\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.939442056231201e-06\n",
      "  batch 101 loss: 0.0004988865173982049\n",
      "  batch 201 loss: 0.00027666286740895884\n",
      "  batch 301 loss: 8.676683654357476e-05\n",
      "  batch 401 loss: 0.00011189410320412207\n",
      "LOSS train 0.00023821700051741726 valid 0.0009850456845015287\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.295522907748818e-06\n",
      "  batch 101 loss: 0.00018659049136431348\n",
      "  batch 201 loss: 0.0003056071391074511\n",
      "  batch 301 loss: 4.571868613879815e-05\n",
      "  batch 401 loss: 0.00032746128233156923\n",
      "LOSS train 0.00029699877484136504 valid 0.0012202159268781543\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 9.58939956035465e-06\n",
      "  batch 101 loss: 0.0005938501871696645\n",
      "  batch 201 loss: 0.00018228911322921705\n",
      "  batch 301 loss: 3.985489809963383e-05\n",
      "  batch 401 loss: 0.00026076025912516346\n",
      "LOSS train 0.0003096750416926711 valid 0.00045214476995170116\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00014454418793320657\n",
      "  batch 101 loss: 46.07897919911891\n",
      "  batch 201 loss: 0.07553055174648762\n",
      "  batch 301 loss: 0.030330143300816417\n",
      "  batch 401 loss: 0.009206080248113722\n",
      "LOSS train 10.428041673853256 valid 0.005751947872340679\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00010258317925035954\n",
      "  batch 101 loss: 0.0025203455769224094\n",
      "  batch 201 loss: 0.0007454259172664023\n",
      "  batch 301 loss: 0.0002743528934661299\n",
      "  batch 401 loss: 0.00011574451535125263\n",
      "LOSS train 0.0008537623048172336 valid 6.573174323420972e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.9303535120561717e-06\n",
      "  batch 101 loss: 0.00018615468216012231\n",
      "  batch 201 loss: 3.1999320362956496e-05\n",
      "  batch 301 loss: 4.778301266014751e-05\n",
      "  batch 401 loss: 5.513136145509634e-05\n",
      "LOSS train 7.925496365550692e-05 valid 0.0003119599132332951\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.339689624728635e-06\n",
      "  batch 101 loss: 0.00012857947025622706\n",
      "  batch 201 loss: 3.1348153497674504e-05\n",
      "  batch 301 loss: 1.8908698523318888e-05\n",
      "  batch 401 loss: 7.295257327996296e-05\n",
      "LOSS train 7.234017666591947e-05 valid 0.0011434417683631182\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0006204140558838844\n",
      "  batch 101 loss: 45.4444847740978\n",
      "  batch 201 loss: 0.13756845597177744\n",
      "  batch 301 loss: 0.03417751682922244\n",
      "  batch 401 loss: 0.005696057286113501\n",
      "LOSS train 10.29871569779209 valid 0.002029666444286704\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00011846678331494332\n",
      "  batch 101 loss: 0.0022248096845578404\n",
      "  batch 201 loss: 0.0001571500859427033\n",
      "  batch 301 loss: 8.151289246598026e-05\n",
      "  batch 401 loss: 4.84210288777831e-05\n",
      "LOSS train 0.0005973224408519918 valid 9.017313277581707e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.794695901684463e-06\n",
      "  batch 101 loss: 0.00012450854999769944\n",
      "  batch 201 loss: 3.530556260557205e-05\n",
      "  batch 301 loss: 4.138582763516752e-05\n",
      "  batch 401 loss: 0.00014814654722158593\n",
      "LOSS train 0.00012782723993154063 valid 0.003344131400808692\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.8056357987225056e-05\n",
      "  batch 101 loss: 0.0013777995831333102\n",
      "  batch 201 loss: 0.0005664588828221895\n",
      "  batch 301 loss: 9.686755879556586e-05\n",
      "  batch 401 loss: 0.00013763462522547342\n",
      "LOSS train 0.0005540837338286063 valid 7.291640213225037e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003240719065070152\n",
      "  batch 101 loss: 18.86529060959816\n",
      "  batch 201 loss: 0.11411057192832232\n",
      "  batch 301 loss: 0.0027545060333795846\n",
      "  batch 401 loss: 0.0007028996522421948\n",
      "LOSS train 4.285185805525584 valid 8.406166307395324e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8030065111815928e-05\n",
      "  batch 101 loss: 0.0009846964153985027\n",
      "  batch 201 loss: 0.00030058961208851543\n",
      "  batch 301 loss: 0.0009366815234534442\n",
      "  batch 401 loss: 0.0027944744311389513\n",
      "LOSS train 0.0016334464331689653 valid 0.02679053507745266\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.00020639101043343544\n",
      "  batch 101 loss: 0.041702576773241164\n",
      "  batch 201 loss: 1.7408153849840164\n",
      "  batch 301 loss: 6.4201878380775455\n",
      "  batch 401 loss: 1.677675161063671\n",
      "LOSS train 2.3193846133686855 valid 0.00780639098957181\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.0028656208515167236\n",
      "  batch 101 loss: 1.424379720389843\n",
      "  batch 201 loss: 0.9912947489321232\n",
      "  batch 301 loss: 2.1869373071193694\n",
      "  batch 401 loss: 2.5806235650181772\n",
      "LOSS train 1.7116486595661862 valid 0.5127796530723572\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 9.193343110382557e-05\n",
      "  batch 101 loss: 79.8358293014206\n",
      "  batch 201 loss: 0.014243593698702171\n",
      "  batch 301 loss: 9.494167471530091e-05\n",
      "  batch 401 loss: 7.153146435939561e-05\n",
      "LOSS train 18.024912699668533 valid 6.423886952688918e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.6751691115787255e-07\n",
      "  batch 101 loss: 7.436717524342385e-05\n",
      "  batch 201 loss: 7.622151951181876e-05\n",
      "  batch 301 loss: 7.803567232258501e-05\n",
      "  batch 401 loss: 8.068690721756865e-05\n",
      "LOSS train 7.770741018745207e-05 valid 7.118024950614199e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.856572872493416e-07\n",
      "  batch 101 loss: 8.384979584661778e-05\n",
      "  batch 201 loss: 8.588830525695811e-05\n",
      "  batch 301 loss: 8.800234451882715e-05\n",
      "  batch 401 loss: 9.104503148364529e-05\n",
      "LOSS train 8.715358838921303e-05 valid 8.770429121796042e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.055212051142007e-06\n",
      "  batch 101 loss: 9.464606992878543e-05\n",
      "  batch 201 loss: 9.695860650481336e-05\n",
      "  batch 301 loss: 9.916469484323898e-05\n",
      "  batch 401 loss: 0.00010210900234824293\n",
      "LOSS train 9.741525294962938e-05 valid 0.0001231395872309804\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001014234870672226\n",
      "  batch 101 loss: 77.27552442774177\n",
      "  batch 201 loss: 0.8400960995093919\n",
      "  batch 301 loss: 0.00023077395117070408\n",
      "  batch 401 loss: 7.168465322592965e-05\n",
      "LOSS train 17.633421316793992 valid 6.435532122850418e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.7225094021996485e-07\n",
      "  batch 101 loss: 7.462386265615351e-05\n",
      "  batch 201 loss: 7.651934377463477e-05\n",
      "  batch 301 loss: 7.837747610210499e-05\n",
      "  batch 401 loss: 8.107831758934481e-05\n",
      "LOSS train 7.802181066907912e-05 valid 7.164998532971367e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.979530007811263e-07\n",
      "  batch 101 loss: 8.430461945408751e-05\n",
      "  batch 201 loss: 8.638665557555215e-05\n",
      "  batch 301 loss: 8.854398760604453e-05\n",
      "  batch 401 loss: 9.16273409347923e-05\n",
      "LOSS train 8.764632393926823e-05 valid 8.905923459678888e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.082049566321075e-06\n",
      "  batch 101 loss: 9.526678000838729e-05\n",
      "  batch 201 loss: 9.761033977838451e-05\n",
      "  batch 301 loss: 9.98136819271167e-05\n",
      "  batch 401 loss: 0.000102727734738437\n",
      "LOSS train 9.799237690074623e-05 valid 0.00012613362923730165\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.001350925713777542\n",
      "  batch 101 loss: 73.76624056760221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 1.4517976689967327\n",
      "  batch 301 loss: 0.00026231617719531643\n",
      "  batch 401 loss: 7.215859767711663e-05\n",
      "LOSS train 16.97962883958658 valid 6.453751120716333e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.795142012881115e-07\n",
      "  batch 101 loss: 7.501411430894223e-05\n",
      "  batch 201 loss: 7.697252922298504e-05\n",
      "  batch 301 loss: 7.889759316640266e-05\n",
      "  batch 401 loss: 8.167349906671007e-05\n",
      "LOSS train 7.849949935350174e-05 valid 7.239239494083449e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.170331809902563e-07\n",
      "  batch 101 loss: 8.499498767832847e-05\n",
      "  batch 201 loss: 8.714254438928037e-05\n",
      "  batch 301 loss: 8.936374282711768e-05\n",
      "  batch 401 loss: 9.25052067100296e-05\n",
      "LOSS train 8.839043857579509e-05 valid 9.11992829060182e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.1237899161642418e-06\n",
      "  batch 101 loss: 9.61957425511173e-05\n",
      "  batch 201 loss: 9.858195839569817e-05\n",
      "  batch 301 loss: 0.00010077077463392925\n",
      "  batch 401 loss: 0.00010362517162775476\n",
      "LOSS train 9.884160384885216e-05 valid 0.00013077397306915373\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00020296024158596992\n",
      "  batch 101 loss: 86.51511195149273\n",
      "  batch 201 loss: 0.03033363206035574\n",
      "  batch 301 loss: 8.34343302904017e-05\n",
      "  batch 401 loss: 7.14963395603263e-05\n",
      "LOSS train 19.536305965075208 valid 6.42085142317228e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.66269557364285e-07\n",
      "  batch 101 loss: 7.430396721247234e-05\n",
      "  batch 201 loss: 7.61497051735205e-05\n",
      "  batch 301 loss: 7.795326951054449e-05\n",
      "  batch 401 loss: 8.059250781116134e-05\n",
      "LOSS train 7.763123517487293e-05 valid 7.106918201316148e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.827218021498994e-07\n",
      "  batch 101 loss: 8.374002929485869e-05\n",
      "  batch 201 loss: 8.576798541525931e-05\n",
      "  batch 301 loss: 8.787143946392461e-05\n",
      "  batch 401 loss: 9.090404506650884e-05\n",
      "LOSS train 8.703438029298911e-05 valid 8.738379983697087e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0488124098628759e-06\n",
      "  batch 101 loss: 9.449527403376123e-05\n",
      "  batch 201 loss: 9.679999204536216e-05\n",
      "  batch 301 loss: 9.900594886858016e-05\n",
      "  batch 401 loss: 0.00010195651478056789\n",
      "LOSS train 9.727393182369163e-05 valid 0.0001224250445375219\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00016851218417286872\n",
      "  batch 101 loss: 39.68454636942595\n",
      "  batch 201 loss: 0.0015793898371197202\n",
      "  batch 301 loss: 7.357723400673421e-05\n",
      "  batch 401 loss: 7.672523006931442e-05\n",
      "LOSS train 8.95857294943822 valid 6.840145942987874e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.085101631470025e-07\n",
      "  batch 101 loss: 8.113663689528039e-05\n",
      "  batch 201 loss: 8.409058510096657e-05\n",
      "  batch 301 loss: 8.703862930360628e-05\n",
      "  batch 401 loss: 9.086809489417647e-05\n",
      "LOSS train 8.586078178849517e-05 valid 8.892382174963132e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0793835099320858e-06\n",
      "  batch 101 loss: 9.53508455404517e-05\n",
      "  batch 201 loss: 9.831762058411186e-05\n",
      "  batch 301 loss: 0.00010100927597932241\n",
      "  batch 401 loss: 0.00010422228474908479\n",
      "LOSS train 9.875014310971843e-05 valid 0.00013586088607553393\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.8930117948912084e-06\n",
      "  batch 101 loss: 0.00010729661950449554\n",
      "  batch 201 loss: 0.00010929874683768048\n",
      "  batch 301 loss: 0.00010940082378994021\n",
      "  batch 401 loss: 0.00010922783229901256\n",
      "LOSS train 0.00010649321054586123 valid 0.00020063089323230088\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0013079801201820373\n",
      "  batch 101 loss: 1.9919510065873056\n",
      "  batch 201 loss: 9.566487194490492e-05\n",
      "  batch 301 loss: 0.0001044057383649033\n",
      "  batch 401 loss: 8.7962464276643e-05\n",
      "LOSS train 0.4500162966335653 valid 0.00019929834525100887\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.843990514520556e-06\n",
      "  batch 101 loss: 7.301448684415845e-05\n",
      "  batch 201 loss: 6.573562123094235e-05\n",
      "  batch 301 loss: 5.986157074289622e-05\n",
      "  batch 401 loss: 5.77156593391237e-05\n",
      "LOSS train 6.322477450272411e-05 valid 0.0001564348058309406\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.2125939722172914e-06\n",
      "  batch 101 loss: 6.0116630507423e-05\n",
      "  batch 201 loss: 5.977636256602637e-05\n",
      "  batch 301 loss: 5.8764316445376605e-05\n",
      "  batch 401 loss: 5.959299911125981e-05\n",
      "LOSS train 5.9687060783213816e-05 valid 0.00013591266178991646\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.893835433293134e-06\n",
      "  batch 101 loss: 5.9701037726256347e-05\n",
      "  batch 201 loss: 5.730664325369617e-05\n",
      "  batch 301 loss: 5.4423439323727507e-05\n",
      "  batch 401 loss: 5.255398421468271e-05\n",
      "LOSS train 5.574464838292426e-05 valid 8.111879287753254e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0007207532227039337\n",
      "  batch 101 loss: 100.76888171806931\n",
      "  batch 201 loss: 0.01683887067483738\n",
      "  batch 301 loss: 8.601620743320382e-05\n",
      "  batch 401 loss: 7.04748693806323e-05\n",
      "LOSS train 22.75093216126988 valid 6.372610368998721e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.456528768059798e-07\n",
      "  batch 101 loss: 7.314802596738445e-05\n",
      "  batch 201 loss: 7.48088571253902e-05\n",
      "  batch 301 loss: 7.641483064617205e-05\n",
      "  batch 401 loss: 7.882846031861845e-05\n",
      "LOSS train 7.621170034305966e-05 valid 6.914186815265566e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.299183587543667e-07\n",
      "  batch 101 loss: 8.168298662894813e-05\n",
      "  batch 201 loss: 8.351087097707932e-05\n",
      "  batch 301 loss: 8.540772464129986e-05\n",
      "  batch 401 loss: 8.823400581150054e-05\n",
      "LOSS train 8.478166605538069e-05 valid 8.1833110016305e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 9.344769932795316e-07\n",
      "  batch 101 loss: 9.160554516256525e-05\n",
      "  batch 201 loss: 9.374280772362908e-05\n",
      "  batch 301 loss: 9.589453208718623e-05\n",
      "  batch 401 loss: 9.88912385957974e-05\n",
      "LOSS train 9.449098561994307e-05 valid 0.00010969912545988336\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.286812687292695e-05\n",
      "  batch 101 loss: 38.779893565597014\n",
      "  batch 201 loss: 0.000414185149493278\n",
      "  batch 301 loss: 7.541604249126976e-05\n",
      "  batch 401 loss: 7.935125374388008e-05\n",
      "LOSS train 8.754069913118634 valid 7.148072472773492e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.93542679073289e-07\n",
      "  batch 101 loss: 8.452212320207764e-05\n",
      "  batch 201 loss: 8.800803839221771e-05\n",
      "  batch 301 loss: 9.144666841393701e-05\n",
      "  batch 401 loss: 9.56716418522774e-05\n",
      "LOSS train 8.974818523234988e-05 valid 0.00010239423136226833\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.331837265752256e-06\n",
      "  batch 101 loss: 0.00010036837890083916\n",
      "  batch 201 loss: 0.00010347917073204371\n",
      "  batch 301 loss: 0.00010576169031594418\n",
      "  batch 401 loss: 0.00010810746340212063\n",
      "LOSS train 0.00010287741070367345 valid 0.0001653755607549101\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.3475759371649474e-06\n",
      "  batch 101 loss: 0.00010962782625767887\n",
      "  batch 201 loss: 0.00011038538625655291\n",
      "  batch 301 loss: 0.00010811597943018114\n",
      "  batch 401 loss: 0.00010536477831010415\n",
      "LOSS train 0.0001056187663659071 valid 0.0002205794444307685\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004215369001030922\n",
      "  batch 101 loss: 8197.290630121595\n",
      "  batch 201 loss: 0.5526288682222367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 0.32465819776058197\n",
      "  batch 401 loss: 0.16524896405637265\n",
      "LOSS train 1850.64847413584 valid 0.07671909034252167\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0007387204468250275\n",
      "  batch 101 loss: 0.05003260234370828\n",
      "  batch 201 loss: 0.018669546227902175\n",
      "  batch 301 loss: 0.00616919350810349\n",
      "  batch 401 loss: 0.0018296432128408924\n",
      "LOSS train 0.017552138107598068 valid 0.0005256248987279832\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.7262367075309157e-06\n",
      "  batch 101 loss: 0.000289157085462648\n",
      "  batch 201 loss: 0.00010761944508971056\n",
      "  batch 301 loss: 6.944409378093042e-05\n",
      "  batch 401 loss: 6.388080947999697e-05\n",
      "LOSS train 0.0001268955068572999 valid 6.15820026723668e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.5707797249197026e-07\n",
      "  batch 101 loss: 6.311579968496517e-05\n",
      "  batch 201 loss: 6.362966310916817e-05\n",
      "  batch 301 loss: 6.363134256389458e-05\n",
      "  batch 401 loss: 6.417689245608926e-05\n",
      "LOSS train 6.411467974169971e-05 valid 6.170683627715334e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00019380377605557442\n",
      "  batch 101 loss: 80955.92828705389\n",
      "  batch 201 loss: 16.148300101161002\n",
      "  batch 301 loss: 0.48771261870861055\n",
      "  batch 401 loss: 0.3884503370523453\n",
      "LOSS train 18278.349782245972 valid 0.3046453297138214\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0029900413751602173\n",
      "  batch 101 loss: 0.26469084739685056\n",
      "  batch 201 loss: 0.1939841128885746\n",
      "  batch 301 loss: 0.1369913099706173\n",
      "  batch 401 loss: 0.09329233027994632\n",
      "LOSS train 0.1628006670127873 valid 0.06268030405044556\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0006010384485125542\n",
      "  batch 101 loss: 0.050376666523516175\n",
      "  batch 201 loss: 0.031186501644551754\n",
      "  batch 301 loss: 0.018710672045126556\n",
      "  batch 401 loss: 0.010740403495728969\n",
      "LOSS train 0.025868776026255456 valid 0.006063370034098625\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.242147482931614e-05\n",
      "  batch 101 loss: 0.004490612195804715\n",
      "  batch 201 loss: 0.0025337688892614098\n",
      "  batch 301 loss: 0.0012324229901423678\n",
      "  batch 401 loss: 0.0006239843538787681\n",
      "LOSS train 0.002059858022818739 valid 0.00029772051493637264\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 4.1023255325853827e-05\n",
      "  batch 101 loss: 234093.9461502346\n",
      "  batch 201 loss: 2.2762117886543276\n",
      "  batch 301 loss: 7.391497094631195\n",
      "  batch 401 loss: 0.48077504336833954\n",
      "LOSS train 52845.20932145266 valid 0.4189724922180176\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.004123627245426178\n",
      "  batch 101 loss: 0.39289860397577286\n",
      "  batch 201 loss: 0.32126302242279053\n",
      "  batch 301 loss: 0.2621440754830837\n",
      "  batch 401 loss: 0.2093652927875519\n",
      "LOSS train 0.2853413468088038 valid 0.16652780771255493\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0016234853863716125\n",
      "  batch 101 loss: 0.14604059673845768\n",
      "  batch 201 loss: 0.11057501770555973\n",
      "  batch 301 loss: 0.0818143393099308\n",
      "  batch 401 loss: 0.05916077319532633\n",
      "LOSS train 0.0945378149571591 valid 0.04274187982082367\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.00040608815848827364\n",
      "  batch 101 loss: 0.035622798297554256\n",
      "  batch 201 loss: 0.024273385871201755\n",
      "  batch 301 loss: 0.016107043493539096\n",
      "  batch 401 loss: 0.010417085024528205\n",
      "LOSS train 0.020326209957784978 valid 0.006747530307620764\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005556577071547509\n",
      "  batch 101 loss: 50538.4528371349\n",
      "  batch 201 loss: 0.6596440780162811\n",
      "  batch 301 loss: 0.5201877674460411\n",
      "  batch 401 loss: 0.3855020120739937\n",
      "LOSS train 11408.610448829872 valid 0.27664315700531006\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00281951904296875\n",
      "  batch 101 loss: 0.22803328141570092\n",
      "  batch 201 loss: 0.14788762532174587\n",
      "  batch 301 loss: 0.09088949382305145\n",
      "  batch 401 loss: 0.05306455947458744\n",
      "LOSS train 0.12124303806968387 valid 0.030351711437106133\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0003208576887845993\n",
      "  batch 101 loss: 0.022500942265614866\n",
      "  batch 201 loss: 0.011640362148173154\n",
      "  batch 301 loss: 0.005697037086356431\n",
      "  batch 401 loss: 0.0026109058980364353\n",
      "LOSS train 0.009784710902166732 valid 0.0011987328762188554\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.507199602201581e-05\n",
      "  batch 101 loss: 0.0008761219566804357\n",
      "  batch 201 loss: 0.00036643325722252485\n",
      "  batch 301 loss: 0.00017695647267828462\n",
      "  batch 401 loss: 0.00010305985717423027\n",
      "LOSS train 0.0003545019767125371 valid 7.512241427320987e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002913741208612919\n",
      "  batch 101 loss: 0.14643796662407113\n",
      "  batch 201 loss: 0.00039361706400086404\n",
      "  batch 301 loss: 0.00011524034633112024\n",
      "  batch 401 loss: 0.0001898179668546618\n",
      "LOSS train 0.033281579195998626 valid 0.0012769604800269008\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.7199732721783223e-06\n",
      "  batch 101 loss: 6.232435597894437e-05\n",
      "  batch 201 loss: 8.411129780370175e-05\n",
      "  batch 301 loss: 2.475117669263227e-05\n",
      "  batch 401 loss: 0.00025834800875827567\n",
      "LOSS train 0.00010581554107814361 valid 0.0013489964185282588\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.3695754245854913e-06\n",
      "  batch 101 loss: 0.00010289526653195936\n",
      "  batch 201 loss: 0.00019911998299448896\n",
      "  batch 301 loss: 4.473043250186492e-05\n",
      "  batch 401 loss: 0.0002958188954949037\n",
      "LOSS train 0.00015358463963109745 valid 0.0008648738148622215\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.3020911612547935e-07\n",
      "  batch 101 loss: 0.0001233694341726732\n",
      "  batch 201 loss: 0.00010906474138778322\n",
      "  batch 301 loss: 4.0198032132821025e-05\n",
      "  batch 401 loss: 0.002701404625552186\n",
      "LOSS train 0.0012153923483328242 valid 0.020899705588817596\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.00013699056580662727\n",
      "  batch 101 loss: 0.0053841042617114\n",
      "  batch 201 loss: 0.0011987698108350741\n",
      "  batch 301 loss: 0.003118051863293658\n",
      "  batch 401 loss: 0.004413244831557677\n",
      "LOSS train 0.003381340361388446 valid 0.0018714664038270712\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.022381810704246e-07\n",
      "  batch 101 loss: 0.0031668691853701603\n",
      "  batch 201 loss: 0.003150140863172055\n",
      "  batch 301 loss: 0.002337925904309941\n",
      "  batch 401 loss: 0.008579910597291018\n",
      "LOSS train 0.003942479091478898 valid 0.0009344128775410354\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.8550499118864538e-05\n",
      "  batch 101 loss: 0.0021483246923253317\n",
      "  batch 201 loss: 0.0026945792136211823\n",
      "  batch 301 loss: 0.0015353017535244362\n",
      "  batch 401 loss: 0.008468885794609377\n",
      "LOSS train 0.0034147125473684755 valid 0.0011659751180559397\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.6389257507398724e-05\n",
      "  batch 101 loss: 0.002343137554889836\n",
      "  batch 201 loss: 0.0039532623076593155\n",
      "  batch 301 loss: 0.0018988627729595465\n",
      "  batch 401 loss: 0.0079916936214795\n",
      "LOSS train 0.004062117672819243 valid 0.005867063533514738\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0011039792746305465\n",
      "  batch 101 loss: 0.1320353108108975\n",
      "  batch 201 loss: 0.0032386263424996286\n",
      "  batch 301 loss: 0.0015132827221532351\n",
      "  batch 401 loss: 0.0006848946675017942\n",
      "LOSS train 0.03130165569164749 valid 0.0015022590523585677\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7141615971922873e-05\n",
      "  batch 101 loss: 0.000164117188360251\n",
      "  batch 201 loss: 3.9719707438052864e-05\n",
      "  batch 301 loss: 3.506169896809297e-05\n",
      "  batch 401 loss: 2.6777878267694178e-05\n",
      "LOSS train 6.612620115625086e-05 valid 9.996600419981405e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.79473115876317e-07\n",
      "  batch 101 loss: 6.36054018377763e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 3.3004738024828836e-05\n",
      "  batch 301 loss: 4.144651832120872e-05\n",
      "  batch 401 loss: 5.7149485769514285e-05\n",
      "LOSS train 4.926161934726682e-05 valid 8.407712448388338e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.45614275324624e-07\n",
      "  batch 101 loss: 6.226617101447118e-05\n",
      "  batch 201 loss: 9.057587026291003e-05\n",
      "  batch 301 loss: 0.0012144784780957706\n",
      "  batch 401 loss: 0.013815496138849994\n",
      "LOSS train 0.003641010690193107 valid 0.001660963986068964\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.626716461032629e-05\n",
      "  batch 101 loss: 0.0008607258494521374\n",
      "  batch 201 loss: 0.00014538357503624867\n",
      "  batch 301 loss: 0.0006343538269356941\n",
      "  batch 401 loss: 0.010342623715041554\n",
      "LOSS train 0.0029108567681879255 valid 0.0012657990446314216\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.6563106328248978e-05\n",
      "  batch 101 loss: 0.0009892262245557505\n",
      "  batch 201 loss: 0.0017959444217194686\n",
      "  batch 301 loss: 0.0043945116276154295\n",
      "  batch 401 loss: 0.004111566151550505\n",
      "LOSS train 0.0031692065133016928 valid 0.014708802103996277\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.00015206394717097284\n",
      "  batch 101 loss: 0.0035865153584745714\n",
      "  batch 201 loss: 0.003468450382206356\n",
      "  batch 301 loss: 0.0013738976653257851\n",
      "  batch 401 loss: 0.006166267233056715\n",
      "LOSS train 0.0036315018602903894 valid 0.00942282471805811\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.289501070976257e-05\n",
      "  batch 101 loss: 0.004291664401243907\n",
      "  batch 201 loss: 0.0033745179729885422\n",
      "  batch 301 loss: 0.003540331682743272\n",
      "  batch 401 loss: 0.0028506777764414435\n",
      "LOSS train 0.004099286637108923 valid 0.04133839160203934\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00014653423801064491\n",
      "  batch 101 loss: 0.1592413362348452\n",
      "  batch 201 loss: 0.0049146678415127095\n",
      "  batch 301 loss: 0.0008047635095135775\n",
      "  batch 401 loss: 0.00015586361078021583\n",
      "LOSS train 0.03731009218533409 valid 7.353096589213237e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1202371388208121e-06\n",
      "  batch 101 loss: 6.751697197159956e-05\n",
      "  batch 201 loss: 1.9225010214540815e-05\n",
      "  batch 301 loss: 2.1630777901009424e-05\n",
      "  batch 401 loss: 2.495191647994943e-05\n",
      "LOSS train 3.207212647677199e-05 valid 6.345139991026372e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5469375284737906e-07\n",
      "  batch 101 loss: 3.127096547359542e-05\n",
      "  batch 201 loss: 2.7798087674000273e-05\n",
      "  batch 301 loss: 3.9435252188013695e-05\n",
      "  batch 401 loss: 6.37075513259333e-05\n",
      "LOSS train 5.356140472191393e-05 valid 0.00010861476766876876\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.21471411047969e-07\n",
      "  batch 101 loss: 0.0007211640011155396\n",
      "  batch 201 loss: 0.00027339898766513213\n",
      "  batch 301 loss: 8.219506386012653e-05\n",
      "  batch 401 loss: 0.004795285939544556\n",
      "LOSS train 0.002646943404480404 valid 0.010671020485460758\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.00016544189304113388\n",
      "  batch 101 loss: 0.012525582037051208\n",
      "  batch 201 loss: 0.0008297503867652267\n",
      "  batch 301 loss: 0.0005471730849603774\n",
      "  batch 401 loss: 0.0012845544704759959\n",
      "LOSS train 0.0035877028214443673 valid 0.0015714062610641122\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0310410289093852e-05\n",
      "  batch 101 loss: 0.006183150048309471\n",
      "  batch 201 loss: 0.0038667098394944335\n",
      "  batch 301 loss: 0.002526281437167199\n",
      "  batch 401 loss: 0.004496382244105916\n",
      "LOSS train 0.004731688579841639 valid 0.012899501249194145\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.00019698919728398324\n",
      "  batch 101 loss: 0.013021896672435106\n",
      "  batch 201 loss: 0.004310651885461993\n",
      "  batch 301 loss: 0.001096969672216801\n",
      "  batch 401 loss: 0.0010781739347294206\n",
      "LOSS train 0.004562235296676019 valid 0.00039079561247490346\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.90749254822731e-06\n",
      "  batch 101 loss: 0.007148078450700268\n",
      "  batch 201 loss: 0.005504118165699765\n",
      "  batch 301 loss: 0.003681301547330804\n",
      "  batch 401 loss: 0.0006922643601865275\n",
      "LOSS train 0.004024597994323466 valid 0.01387764886021614\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0006754177808761596\n",
      "  batch 101 loss: 0.15461935529718177\n",
      "  batch 201 loss: 0.0015777910563338083\n",
      "  batch 301 loss: 8.006057993952708e-05\n",
      "  batch 401 loss: 5.360140147786296e-05\n",
      "LOSS train 0.03544721407647137 valid 0.00024461408611387014\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.124334860127419e-06\n",
      "  batch 101 loss: 7.369667552666215e-05\n",
      "  batch 201 loss: 3.6075442690162166e-05\n",
      "  batch 301 loss: 4.9744615780582534e-05\n",
      "  batch 401 loss: 5.149925172190706e-05\n",
      "LOSS train 5.2871353300586915e-05 valid 0.00024202030908782035\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.8844250007532537e-06\n",
      "  batch 101 loss: 0.00016294734250550392\n",
      "  batch 201 loss: 8.898474899979192e-05\n",
      "  batch 301 loss: 0.00011052117311919573\n",
      "  batch 401 loss: 0.00037402267169454716\n",
      "LOSS train 0.00028179134749582735 valid 0.004983598832041025\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.670425947755575e-05\n",
      "  batch 101 loss: 0.003471281479578465\n",
      "  batch 201 loss: 0.0032676426955731584\n",
      "  batch 301 loss: 0.0015885346299910452\n",
      "  batch 401 loss: 0.008948083966388368\n",
      "LOSS train 0.0047299404145766605 valid 0.03421946242451668\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.0004732407629489899\n",
      "  batch 101 loss: 0.02204640325624496\n",
      "  batch 201 loss: 0.02215982497902587\n",
      "  batch 301 loss: 0.03852480860892683\n",
      "  batch 401 loss: 0.04364569094963372\n",
      "LOSS train 0.028892244033830273 valid 0.0007962326635606587\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.6284274170175195e-05\n",
      "  batch 101 loss: 0.00034903076993941795\n",
      "  batch 201 loss: 7.34518289391417e-05\n",
      "  batch 301 loss: 7.141537536881515e-05\n",
      "  batch 401 loss: 9.46290222418611e-05\n",
      "LOSS train 0.00018769721936393906 valid 0.0013596384087577462\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.4734517317265271e-05\n",
      "  batch 101 loss: 0.0007959750681766309\n",
      "  batch 201 loss: 0.0007388370133412537\n",
      "  batch 301 loss: 0.0001823338199756108\n",
      "  batch 401 loss: 0.0005432997435855214\n",
      "LOSS train 0.0005436586914781132 valid 0.0029173323418945074\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.7089130599051712e-05\n",
      "  batch 101 loss: 0.0024272778147133066\n",
      "  batch 201 loss: 0.004525993450079114\n",
      "  batch 301 loss: 0.006679110903060064\n",
      "  batch 401 loss: 0.0212776886112988\n",
      "LOSS train 0.014505635429164247 valid 0.17053915560245514\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.1363010853528977e-05\n",
      "  batch 101 loss: 0.09413285108102996\n",
      "  batch 201 loss: 6.68756841486129e-05\n",
      "  batch 301 loss: 7.705775415161043e-05\n",
      "  batch 401 loss: 8.207432014842197e-05\n",
      "LOSS train 0.02131043933407332 valid 7.547943823738024e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.925550016807392e-07\n",
      "  batch 101 loss: 8.796906288580431e-05\n",
      "  batch 201 loss: 9.19346330897497e-05\n",
      "  batch 301 loss: 9.57232559858312e-05\n",
      "  batch 401 loss: 0.00010008387238713112\n",
      "LOSS train 9.345684428216666e-05 valid 0.00011881526006618515\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.6155725461430848e-06\n",
      "  batch 101 loss: 0.000104517363507739\n",
      "  batch 201 loss: 0.0001073754095222057\n",
      "  batch 301 loss: 0.00010862964114267016\n",
      "  batch 401 loss: 0.00010948033717284034\n",
      "LOSS train 0.00010542004805181626 valid 0.0001909279526444152\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7234904700890183e-06\n",
      "  batch 101 loss: 0.00010900519674009956\n",
      "  batch 201 loss: 0.00010819493878216235\n",
      "  batch 301 loss: 0.00010376872364361134\n",
      "  batch 401 loss: 9.922414872107766e-05\n",
      "LOSS train 0.00010219095809898915 valid 0.00022372671810444444\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.189692797604948e-06\n",
      "  batch 101 loss: 9.439305045574997e-05\n",
      "  batch 201 loss: 9.04736956852048e-05\n",
      "  batch 301 loss: 8.355964110876357e-05\n",
      "  batch 401 loss: 7.82330517711216e-05\n",
      "LOSS train 8.448751833274996e-05 valid 0.0001958922075573355\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.7950963703915476e-06\n",
      "  batch 101 loss: 7.470018124763555e-05\n",
      "  batch 201 loss: 7.134014619850859e-05\n",
      "  batch 301 loss: 6.631671664763416e-05\n",
      "  batch 401 loss: 6.331337237725166e-05\n",
      "LOSS train 6.767868073400896e-05 valid 0.00016670292825438082\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.3674475960433484e-06\n",
      "  batch 101 loss: 6.306471235632216e-05\n",
      "  batch 201 loss: 6.129972317921784e-05\n",
      "  batch 301 loss: 5.8551239443431766e-05\n",
      "  batch 401 loss: 5.770461349925426e-05\n",
      "LOSS train 5.956788028094109e-05 valid 0.00015724347031209618\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2248872846830636e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 6.009632991094804e-05\n",
      "  batch 201 loss: 5.950395527520414e-05\n",
      "  batch 301 loss: 5.813030849765255e-05\n",
      "  batch 401 loss: 5.863061910702072e-05\n",
      "LOSS train 5.8982051645083994e-05 valid 0.00015408704348374158\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004572950303554535\n",
      "  batch 101 loss: 0.044725717011215235\n",
      "  batch 201 loss: 7.870507996813103e-05\n",
      "  batch 301 loss: 8.614750054221076e-05\n",
      "  batch 401 loss: 9.371469595635062e-05\n",
      "LOSS train 0.010266052864224181 valid 0.00010453374852659181\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.3700214913114906e-06\n",
      "  batch 101 loss: 0.00010138662668737197\n",
      "  batch 201 loss: 0.00010608131111098374\n",
      "  batch 301 loss: 0.0001085171104807614\n",
      "  batch 401 loss: 0.00010945257702729805\n",
      "LOSS train 0.00010420246641254372 valid 0.00019982142839580774\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.8514835867099463e-06\n",
      "  batch 101 loss: 0.00010780416123225223\n",
      "  batch 201 loss: 0.00010552177664521878\n",
      "  batch 301 loss: 9.904775175925806e-05\n",
      "  batch 401 loss: 9.268947472691025e-05\n",
      "LOSS train 9.834626610120036e-05 valid 0.00021794771600980312\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1086424132809043e-06\n",
      "  batch 101 loss: 8.663354523605449e-05\n",
      "  batch 201 loss: 8.187476452917508e-05\n",
      "  batch 301 loss: 7.488794639016306e-05\n",
      "  batch 401 loss: 7.004725805131784e-05\n",
      "LOSS train 7.656209771266526e-05 valid 0.00017938468954525888\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.5553023442626e-06\n",
      "  batch 101 loss: 6.768520639298003e-05\n",
      "  batch 201 loss: 6.491104766382704e-05\n",
      "  batch 301 loss: 6.1010488648207686e-05\n",
      "  batch 401 loss: 5.9163445072059065e-05\n",
      "LOSS train 6.232581966031218e-05 valid 0.00015912884555291384\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2534847084898503e-06\n",
      "  batch 101 loss: 6.047598193106296e-05\n",
      "  batch 201 loss: 5.946263672086616e-05\n",
      "  batch 301 loss: 5.765828733046874e-05\n",
      "  batch 401 loss: 5.77661381589678e-05\n",
      "LOSS train 5.856439659829206e-05 valid 0.00015657726908102632\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2147592972032727e-06\n",
      "  batch 101 loss: 6.093340223173982e-05\n",
      "  batch 201 loss: 6.0508080520378374e-05\n",
      "  batch 301 loss: 5.929109161371571e-05\n",
      "  batch 401 loss: 5.984544555758475e-05\n",
      "LOSS train 6.020975173190415e-05 valid 0.00013826662325300276\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.9311337382532656e-06\n",
      "  batch 101 loss: 6.032773863012153e-05\n",
      "  batch 201 loss: 5.8913688752397776e-05\n",
      "  batch 301 loss: 5.6931339623247366e-05\n",
      "  batch 401 loss: 5.6155938999609134e-05\n",
      "LOSS train 5.800321081620407e-05 valid 9.561834303895012e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00018329951912164687\n",
      "  batch 101 loss: 0.06992029071268917\n",
      "  batch 201 loss: 7.765162241412327e-05\n",
      "  batch 301 loss: 8.048225849506707e-05\n",
      "  batch 401 loss: 8.649143427646777e-05\n",
      "LOSS train 0.015888081656467747 valid 8.400453953072429e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.80061013251543e-07\n",
      "  batch 101 loss: 9.338676706875048e-05\n",
      "  batch 201 loss: 9.796911576131606e-05\n",
      "  batch 301 loss: 0.00010191819096689869\n",
      "  batch 401 loss: 0.00010583636687442776\n",
      "LOSS train 9.87010792515222e-05 valid 0.00015030124632176012\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1187377569731323e-06\n",
      "  batch 101 loss: 0.00010882831028482088\n",
      "  batch 201 loss: 0.00011040086351840728\n",
      "  batch 301 loss: 0.0001089074829741321\n",
      "  batch 401 loss: 0.00010653420082803677\n",
      "LOSS train 0.0001059369609721333 valid 0.0002187854697695002\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1204186961986126e-06\n",
      "  batch 101 loss: 0.00010271115722787272\n",
      "  batch 201 loss: 9.936726935166008e-05\n",
      "  batch 301 loss: 9.242824763504131e-05\n",
      "  batch 401 loss: 8.64538870598608e-05\n",
      "LOSS train 9.263582079754711e-05 valid 0.0002097978867823258\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.993594971485436e-06\n",
      "  batch 101 loss: 8.157612495665489e-05\n",
      "  batch 201 loss: 7.752529975391554e-05\n",
      "  batch 301 loss: 7.144014005689315e-05\n",
      "  batch 401 loss: 6.74197126272702e-05\n",
      "LOSS train 7.292827473819163e-05 valid 0.0001745959307299927\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.4847700842656196e-06\n",
      "  batch 101 loss: 6.594179652552156e-05\n",
      "  batch 201 loss: 6.358713166150665e-05\n",
      "  batch 301 loss: 6.0120060167037084e-05\n",
      "  batch 401 loss: 5.8617302653374284e-05\n",
      "LOSS train 6.128982637615522e-05 valid 0.0001583888370078057\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.242271148134023e-06\n",
      "  batch 101 loss: 6.027284811466416e-05\n",
      "  batch 201 loss: 5.9394006984803124e-05\n",
      "  batch 301 loss: 5.771620672931021e-05\n",
      "  batch 401 loss: 5.792815054803668e-05\n",
      "LOSS train 5.8589743983573307e-05 valid 0.0001563490368425846\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.211288083344698e-06\n",
      "  batch 101 loss: 6.107420394300789e-05\n",
      "  batch 201 loss: 6.0617579879931324e-05\n",
      "  batch 301 loss: 5.937004715718785e-05\n",
      "  batch 401 loss: 5.987279115686306e-05\n",
      "LOSS train 6.030142305787654e-05 valid 0.000136050017317757\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002147861197590828\n",
      "  batch 101 loss: 0.060388747235992926\n",
      "  batch 201 loss: 0.0001982002100680802\n",
      "  batch 301 loss: 7.797424539376153e-05\n",
      "  batch 401 loss: 9.051251754044643e-05\n",
      "LOSS train 0.013771306103472478 valid 9.44501516642049e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1858399375341833e-06\n",
      "  batch 101 loss: 9.806348058987169e-05\n",
      "  batch 201 loss: 0.00010288832053902297\n",
      "  batch 301 loss: 0.00010630292142764119\n",
      "  batch 401 loss: 0.0001088887168174324\n",
      "LOSS train 0.00010234443109668257 valid 0.00017989592743106186\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.5628035655245183e-06\n",
      "  batch 101 loss: 0.00010950751188033791\n",
      "  batch 201 loss: 0.00010903860212067684\n",
      "  batch 301 loss: 0.00010456106450760671\n",
      "  batch 401 loss: 9.951211374300329e-05\n",
      "LOSS train 0.00010269265900756664 valid 0.000223867769818753\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.191664000041783e-06\n",
      "  batch 101 loss: 9.383181669903707e-05\n",
      "  batch 201 loss: 8.926157163443804e-05\n",
      "  batch 301 loss: 8.183927446566486e-05\n",
      "  batch 401 loss: 7.622210749445912e-05\n",
      "LOSS train 8.313335821206107e-05 valid 0.0001915216416819021\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.7320743538439274e-06\n",
      "  batch 101 loss: 7.262778832796357e-05\n",
      "  batch 201 loss: 6.922902180122037e-05\n",
      "  batch 301 loss: 6.440480830178785e-05\n",
      "  batch 401 loss: 6.168060913182672e-05\n",
      "LOSS train 6.585676050867969e-05 valid 0.00016345561016350985\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.3187602346297352e-06\n",
      "  batch 101 loss: 6.189222856122001e-05\n",
      "  batch 201 loss: 6.0365235442816355e-05\n",
      "  batch 301 loss: 5.796144092840905e-05\n",
      "  batch 401 loss: 5.7481153303058366e-05\n",
      "LOSS train 5.8953406804287186e-05 valid 0.00015698384959250689\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.220941096311435e-06\n",
      "  batch 101 loss: 6.0284000401225054e-05\n",
      "  batch 201 loss: 5.98298942850306e-05\n",
      "  batch 301 loss: 5.860199945743716e-05\n",
      "  batch 401 loss: 5.923408474075131e-05\n",
      "LOSS train 5.947064986620877e-05 valid 0.00014988264592830092\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.1122905309312046e-06\n",
      "  batch 101 loss: 6.143012252763924e-05\n",
      "  batch 201 loss: 6.048785822514447e-05\n",
      "  batch 301 loss: 5.882366729110799e-05\n",
      "  batch 401 loss: 5.861709434611839e-05\n",
      "LOSS train 5.985400428476882e-05 valid 0.0001114634214900434\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0016919687390327454\n",
      "  batch 101 loss: 0.017526815500004886\n",
      "  batch 201 loss: 8.869569231364949e-05\n",
      "  batch 301 loss: 0.00010053002286895207\n",
      "  batch 401 loss: 0.00010788760027480749\n",
      "LOSS train 0.004413476495859094 valid 0.00018574764544609934\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.648315276019275e-06\n",
      "  batch 101 loss: 0.00010870835262437595\n",
      "  batch 201 loss: 0.00010559548735500357\n",
      "  batch 301 loss: 9.669795538513881e-05\n",
      "  batch 401 loss: 8.782638444301938e-05\n",
      "LOSS train 9.654951928449083e-05 valid 0.00020898794173263013\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.9821140924468638e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 7.999218954694242e-05\n",
      "  batch 201 loss: 7.439264211029694e-05\n",
      "  batch 301 loss: 6.761916574987481e-05\n",
      "  batch 401 loss: 6.353982473001452e-05\n",
      "LOSS train 6.994350212543812e-05 valid 0.00016602034156676382\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.357234770897776e-06\n",
      "  batch 101 loss: 6.265474682805917e-05\n",
      "  batch 201 loss: 6.07056858427768e-05\n",
      "  batch 301 loss: 5.8034520199043984e-05\n",
      "  batch 401 loss: 5.744779551577039e-05\n",
      "LOSS train 5.922098647034149e-05 valid 0.00015683921810705215\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2187421564012764e-06\n",
      "  batch 101 loss: 6.0300243454776137e-05\n",
      "  batch 201 loss: 5.988602566844747e-05\n",
      "  batch 301 loss: 5.871175747699908e-05\n",
      "  batch 401 loss: 5.940007919321033e-05\n",
      "LOSS train 5.959675343255987e-05 valid 0.00014720491890329868\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.0709275850094854e-06\n",
      "  batch 101 loss: 6.123534288235533e-05\n",
      "  batch 201 loss: 6.008818400573546e-05\n",
      "  batch 301 loss: 5.8250264391972454e-05\n",
      "  batch 401 loss: 5.775769823458177e-05\n",
      "LOSS train 5.930656431955785e-05 valid 0.00010419749014545232\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.3640470569953323e-06\n",
      "  batch 101 loss: 5.435983844108705e-05\n",
      "  batch 201 loss: 5.265833948215004e-05\n",
      "  batch 301 loss: 5.076842026653594e-05\n",
      "  batch 401 loss: 4.98120111052458e-05\n",
      "LOSS train 5.1749424009390155e-05 valid 7.762939640088007e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.422697283094749e-07\n",
      "  batch 101 loss: 4.6439779860207865e-05\n",
      "  batch 201 loss: 4.583934392201172e-05\n",
      "  batch 301 loss: 4.5464123895726513e-05\n",
      "  batch 401 loss: 4.588630520402148e-05\n",
      "LOSS train 4.593384727516902e-05 valid 7.251407078001648e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00013039655983448028\n",
      "  batch 101 loss: 0.001256320052461888\n",
      "  batch 201 loss: 8.394231376087192e-05\n",
      "  batch 301 loss: 5.688878402395403e-05\n",
      "  batch 401 loss: 6.174436985020292e-05\n",
      "LOSS train 0.0003644255082271687 valid 0.00010707924229791388\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.469176058890298e-06\n",
      "  batch 101 loss: 5.4819175405782514e-05\n",
      "  batch 201 loss: 5.042382939677737e-05\n",
      "  batch 301 loss: 4.735070809005037e-05\n",
      "  batch 401 loss: 4.649594377156063e-05\n",
      "LOSS train 4.958515878920624e-05 valid 7.316680421354249e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.257807010319083e-07\n",
      "  batch 101 loss: 4.448241795785179e-05\n",
      "  batch 201 loss: 4.523772110474056e-05\n",
      "  batch 301 loss: 4.658699404274103e-05\n",
      "  batch 401 loss: 4.8446243634998606e-05\n",
      "LOSS train 4.641684981888152e-05 valid 6.698555807815865e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.650872844853439e-07\n",
      "  batch 101 loss: 4.677589439552321e-05\n",
      "  batch 201 loss: 4.8312809622075295e-05\n",
      "  batch 301 loss: 4.898797488493756e-05\n",
      "  batch 401 loss: 4.84490289859707e-05\n",
      "LOSS train 4.8134410625510926e-05 valid 6.159856275189668e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.478672467987053e-07\n",
      "  batch 101 loss: 4.391354307699657e-05\n",
      "  batch 201 loss: 4.498467924065608e-05\n",
      "  batch 301 loss: 4.4263841596148266e-05\n",
      "  batch 401 loss: 4.2540897826484067e-05\n",
      "LOSS train 4.397321851065992e-05 valid 6.555189611390233e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.1394992725399789e-07\n",
      "  batch 101 loss: 3.914614996517685e-05\n",
      "  batch 201 loss: 3.956866565772543e-05\n",
      "  batch 301 loss: 3.920109651232906e-05\n",
      "  batch 401 loss: 3.79602475528884e-05\n",
      "LOSS train 3.919995704351684e-05 valid 6.584404763998464e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1614857612585184e-07\n",
      "  batch 101 loss: 3.634145736270966e-05\n",
      "  batch 201 loss: 3.653058312920621e-05\n",
      "  batch 301 loss: 3.648646529995858e-05\n",
      "  batch 401 loss: 3.6110108728451e-05\n",
      "LOSS train 3.676572478151577e-05 valid 6.35892283753492e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.4851677406113596e-07\n",
      "  batch 101 loss: 3.5772479264721824e-05\n",
      "  batch 201 loss: 3.5638842731486876e-05\n",
      "  batch 301 loss: 3.602498511554586e-05\n",
      "  batch 401 loss: 3.594260050334697e-05\n",
      "LOSS train 3.630986196367098e-05 valid 6.20234859525226e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0020905783772468566\n",
      "  batch 101 loss: 0.011524540153768612\n",
      "  batch 201 loss: 6.443833306548186e-05\n",
      "  batch 301 loss: 5.325596910552122e-05\n",
      "  batch 401 loss: 4.8376989416283325e-05\n",
      "LOSS train 0.0031156296231703777 valid 6.224493699846789e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.914832111855503e-07\n",
      "  batch 101 loss: 5.0683428526099306e-05\n",
      "  batch 201 loss: 5.0885503674180656e-05\n",
      "  batch 301 loss: 4.856404375004786e-05\n",
      "  batch 401 loss: 5.102127722693695e-05\n",
      "LOSS train 5.034392893783571e-05 valid 6.23328669462353e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.7758796679554506e-07\n",
      "  batch 101 loss: 5.190756002775743e-05\n",
      "  batch 201 loss: 5.5437428652567175e-05\n",
      "  batch 301 loss: 5.534932156024297e-05\n",
      "  batch 401 loss: 5.5160803963190116e-05\n",
      "LOSS train 5.4633315801644875e-05 valid 0.00014034047489985824\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.039478567894548e-06\n",
      "  batch 101 loss: 6.182648940637136e-05\n",
      "  batch 201 loss: 6.052034146108554e-05\n",
      "  batch 301 loss: 5.864783132210505e-05\n",
      "  batch 401 loss: 5.811355502260085e-05\n",
      "LOSS train 5.9711121824637954e-05 valid 0.0001056999753927812\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.4071569603402167e-06\n",
      "  batch 101 loss: 5.486832925498675e-05\n",
      "  batch 201 loss: 5.2838791759768355e-05\n",
      "  batch 301 loss: 5.0901230556519297e-05\n",
      "  batch 401 loss: 4.985215409419652e-05\n",
      "LOSS train 5.194221572578633e-05 valid 7.799307786626741e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.491530024912208e-07\n",
      "  batch 101 loss: 4.631859575667363e-05\n",
      "  batch 201 loss: 4.567338252911668e-05\n",
      "  batch 301 loss: 4.535464454079374e-05\n",
      "  batch 401 loss: 4.5837038632043916e-05\n",
      "LOSS train 4.583537694110019e-05 valid 7.24110213923268e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.137015927582979e-07\n",
      "  batch 101 loss: 4.477777299371155e-05\n",
      "  batch 201 loss: 4.535501737450432e-05\n",
      "  batch 301 loss: 4.649396615150181e-05\n",
      "  batch 401 loss: 4.80246987837063e-05\n",
      "LOSS train 4.633466789110236e-05 valid 6.875871622469276e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.194028537720442e-07\n",
      "  batch 101 loss: 4.6628930250562916e-05\n",
      "  batch 201 loss: 4.803866375127086e-05\n",
      "  batch 301 loss: 4.913417396409159e-05\n",
      "  batch 401 loss: 4.941707384432448e-05\n",
      "LOSS train 4.8377065784558134e-05 valid 6.206906982697546e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00140422523021698\n",
      "  batch 101 loss: 0.023693421567404584\n",
      "  batch 201 loss: 7.207297339846263e-05\n",
      "  batch 301 loss: 8.096274874787924e-05\n",
      "  batch 401 loss: 0.00010917771625827299\n",
      "LOSS train 0.005732386790570095 valid 0.00020294063142500818\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.9079726664349435e-06\n",
      "  batch 101 loss: 0.00010628217040334676\n",
      "  batch 201 loss: 0.00010014186149078342\n",
      "  batch 301 loss: 8.951079724795364e-05\n",
      "  batch 401 loss: 8.047313606653006e-05\n",
      "LOSS train 9.11644887476656e-05 valid 0.00019598015933297575\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.797008492052555e-06\n",
      "  batch 101 loss: 7.374896450869528e-05\n",
      "  batch 201 loss: 6.894325893426867e-05\n",
      "  batch 301 loss: 6.334828622470922e-05\n",
      "  batch 401 loss: 6.0392206918891134e-05\n",
      "LOSS train 6.548484657133213e-05 valid 0.00016064484952948987\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.274486469104886e-06\n",
      "  batch 101 loss: 6.087010219616218e-05\n",
      "  batch 201 loss: 5.9568167091299526e-05\n",
      "  batch 301 loss: 5.76095685730138e-05\n",
      "  batch 401 loss: 5.7679266027435006e-05\n",
      "LOSS train 5.86480904617294e-05 valid 0.00015643118240404874\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.212335675721988e-06\n",
      "  batch 101 loss: 6.0940130302924445e-05\n",
      "  batch 201 loss: 6.052928461400598e-05\n",
      "  batch 301 loss: 5.931452555159922e-05\n",
      "  batch 401 loss: 5.98333168835552e-05\n",
      "LOSS train 6.023523785208205e-05 valid 0.0001343424228252843\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.8688279669731856e-06\n",
      "  batch 101 loss: 5.976317706483769e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 5.813528674252666e-05\n",
      "  batch 301 loss: 5.600419193342532e-05\n",
      "  batch 401 loss: 5.500334412545271e-05\n",
      "LOSS train 5.710744560143216e-05 valid 9.049671643879265e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1097786045866087e-06\n",
      "  batch 101 loss: 5.087858162028169e-05\n",
      "  batch 201 loss: 4.949695874415738e-05\n",
      "  batch 301 loss: 4.80865239021e-05\n",
      "  batch 401 loss: 4.7552468660683187e-05\n",
      "LOSS train 4.8902902859778063e-05 valid 7.443709182552993e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.675332017242909e-07\n",
      "  batch 101 loss: 4.5026088784112514e-05\n",
      "  batch 201 loss: 4.487559557418308e-05\n",
      "  batch 301 loss: 4.5105911835321424e-05\n",
      "  batch 401 loss: 4.6066393940407126e-05\n",
      "LOSS train 4.536658554674556e-05 valid 7.191531040007249e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.242142800241709e-06\n",
      "  batch 101 loss: 0.013117562600540396\n",
      "  batch 201 loss: 7.522859668711135e-05\n",
      "  batch 301 loss: 3.713656641309626e-05\n",
      "  batch 401 loss: 2.0573443740374843e-05\n",
      "LOSS train 0.0029932023643470676 valid 4.307380731916055e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.622947704367107e-08\n",
      "  batch 101 loss: 1.227943583202773e-05\n",
      "  batch 201 loss: 7.455564908127599e-06\n",
      "  batch 301 loss: 7.074869720042898e-06\n",
      "  batch 401 loss: 4.631998843649399e-06\n",
      "LOSS train 7.519621263505125e-06 valid 5.304325895849615e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.176323495310499e-08\n",
      "  batch 101 loss: 4.100068930767975e-06\n",
      "  batch 201 loss: 4.074125059503331e-06\n",
      "  batch 301 loss: 0.00018220378023130478\n",
      "  batch 401 loss: 3.990247722981621e-05\n",
      "LOSS train 5.660086009815856e-05 valid 0.00011009410081896931\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.4673551777377725e-06\n",
      "  batch 101 loss: 5.602630544558451e-05\n",
      "  batch 201 loss: 6.077113811784329e-05\n",
      "  batch 301 loss: 5.926209565387808e-05\n",
      "  batch 401 loss: 5.9789082537236024e-05\n",
      "LOSS train 5.897825478872871e-05 valid 0.00013652065536007285\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.9034894648939372e-06\n",
      "  batch 101 loss: 6.005793962941652e-05\n",
      "  batch 201 loss: 5.853351467891344e-05\n",
      "  batch 301 loss: 5.645963770348317e-05\n",
      "  batch 401 loss: 5.5533968416057176e-05\n",
      "LOSS train 5.754311964867986e-05 valid 9.252834570361301e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.1493456258904188e-06\n",
      "  batch 101 loss: 5.145745497884491e-05\n",
      "  batch 201 loss: 4.9994953388932115e-05\n",
      "  batch 301 loss: 4.847704812448228e-05\n",
      "  batch 401 loss: 4.785146611482105e-05\n",
      "LOSS train 4.933235234132617e-05 valid 7.473826553905383e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.749164797132835e-07\n",
      "  batch 101 loss: 4.5167260265941424e-05\n",
      "  batch 201 loss: 4.494631549661676e-05\n",
      "  batch 301 loss: 4.508747695467719e-05\n",
      "  batch 401 loss: 4.5981022134924386e-05\n",
      "LOSS train 4.538453636813973e-05 valid 7.19460440450348e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.056117465253919e-07\n",
      "  batch 101 loss: 4.5029121761785976e-05\n",
      "  batch 201 loss: 4.5929641995599016e-05\n",
      "  batch 301 loss: 4.724448597016817e-05\n",
      "  batch 401 loss: 4.8690915795077674e-05\n",
      "LOSS train 4.6892193173097694e-05 valid 6.703777034999803e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00012674246914684773\n",
      "  batch 101 loss: 0.032627776207227725\n",
      "  batch 201 loss: 0.0005837834358680994\n",
      "  batch 301 loss: 0.0003550061160058249\n",
      "  batch 401 loss: 0.00019423805930273374\n",
      "LOSS train 0.007662291162429587 valid 0.00010325919720344245\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.245242019649595e-06\n",
      "  batch 101 loss: 8.861360598530154e-05\n",
      "  batch 201 loss: 6.27654918389453e-05\n",
      "  batch 301 loss: 5.271161318887607e-05\n",
      "  batch 401 loss: 5.299862852552906e-05\n",
      "LOSS train 6.347554979185658e-05 valid 0.00013012414274271578\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.4705531541258096e-06\n",
      "  batch 101 loss: 5.74650353087236e-05\n",
      "  batch 201 loss: 6.155093295319603e-05\n",
      "  batch 301 loss: 8.283006928991199e-05\n",
      "  batch 401 loss: 7.616176904662097e-05\n",
      "LOSS train 6.869773561259434e-05 valid 0.00018983100017067045\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7076114201918246e-06\n",
      "  batch 101 loss: 7.179312966513862e-05\n",
      "  batch 201 loss: 6.81593409819925e-05\n",
      "  batch 301 loss: 6.324738056946444e-05\n",
      "  batch 401 loss: 6.0659580377091516e-05\n",
      "LOSS train 6.489815534878371e-05 valid 0.00016089390555862337\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2801700106356292e-06\n",
      "  batch 101 loss: 6.115822189343589e-05\n",
      "  batch 201 loss: 5.987463894442158e-05\n",
      "  batch 301 loss: 5.766618049108274e-05\n",
      "  batch 401 loss: 5.7536248680207793e-05\n",
      "LOSS train 5.8687131419364434e-05 valid 0.00015685014659538865\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2442001500166954e-06\n",
      "  batch 101 loss: 6.063243824542042e-05\n",
      "  batch 201 loss: 6.013736093450461e-05\n",
      "  batch 301 loss: 5.905277797182862e-05\n",
      "  batch 401 loss: 5.9782939642616385e-05\n",
      "LOSS train 5.9960540812602236e-05 valid 0.00014276844740379602\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.9974981842096895e-06\n",
      "  batch 101 loss: 6.088963163335848e-05\n",
      "  batch 201 loss: 5.95595093460588e-05\n",
      "  batch 301 loss: 5.7636000727825374e-05\n",
      "  batch 401 loss: 5.7012031178373945e-05\n",
      "LOSS train 5.8725197902089875e-05 valid 9.958905866369605e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.287759077968076e-06\n",
      "  batch 101 loss: 5.331105418918014e-05\n",
      "  batch 201 loss: 5.179109204050292e-05\n",
      "  batch 301 loss: 5.001027201501529e-05\n",
      "  batch 401 loss: 4.916710956251791e-05\n",
      "LOSS train 5.093426235497118e-05 valid 7.664354052394629e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00021233227103948594\n",
      "  batch 101 loss: 0.01686709428249742\n",
      "  batch 201 loss: 0.00026904296501015776\n",
      "  batch 301 loss: 9.527853742838488e-05\n",
      "  batch 401 loss: 8.529697381163714e-05\n",
      "LOSS train 0.00396350568687585 valid 0.00021000549895688891\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8682529227808117e-06\n",
      "  batch 101 loss: 9.33133871717473e-05\n",
      "  batch 201 loss: 8.960541921283039e-05\n",
      "  batch 301 loss: 8.099497842863457e-05\n",
      "  batch 401 loss: 7.414166506805486e-05\n",
      "LOSS train 8.21958816410541e-05 valid 0.0001845770311774686\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.632648393046111e-06\n",
      "  batch 101 loss: 6.931716455937931e-05\n",
      "  batch 201 loss: 6.526204932697511e-05\n",
      "  batch 301 loss: 6.0488154587119426e-05\n",
      "  batch 401 loss: 5.866757130377209e-05\n",
      "LOSS train 6.255860239242225e-05 valid 0.00015726311539765447\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2412075486499814e-06\n",
      "  batch 101 loss: 6.0288032297819424e-05\n",
      "  batch 201 loss: 5.917712373047834e-05\n",
      "  batch 301 loss: 5.80854044494572e-05\n",
      "  batch 401 loss: 5.856637605063497e-05\n",
      "LOSS train 5.8887131419520307e-05 valid 0.00015407452883664519\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.1956002456136048e-06\n",
      "  batch 101 loss: 6.158207694539897e-05\n",
      "  batch 201 loss: 6.0966760944438644e-05\n",
      "  batch 301 loss: 5.917123908091071e-05\n",
      "  batch 401 loss: 5.9292944657727273e-05\n",
      "LOSS train 6.029467356578716e-05 valid 0.0001191995179397054\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.6221390978898853e-06\n",
      "  batch 101 loss: 5.732283208885747e-05\n",
      "  batch 201 loss: 5.558565183491737e-05\n",
      "  batch 301 loss: 5.346976759483368e-05\n",
      "  batch 401 loss: 5.2012120843301094e-05\n",
      "LOSS train 5.442821892603959e-05 valid 8.207074279198423e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.394351945957169e-07\n",
      "  batch 101 loss: 4.821500162620396e-05\n",
      "  batch 201 loss: 4.726752568160464e-05\n",
      "  batch 301 loss: 4.635460626730037e-05\n",
      "  batch 401 loss: 4.635149955106499e-05\n",
      "LOSS train 4.7023491239748966e-05 valid 7.284677849384025e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.313740934478119e-07\n",
      "  batch 101 loss: 4.456117306403939e-05\n",
      "  batch 201 loss: 4.476746889281458e-05\n",
      "  batch 301 loss: 4.5474369583757836e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 4.6833704930691054e-05\n",
      "LOSS train 4.554728779285306e-05 valid 7.087708218023181e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00022150322794914245\n",
      "  batch 101 loss: 0.02326404660649132\n",
      "  batch 201 loss: 0.0001918231907984591\n",
      "  batch 301 loss: 9.816593321374966e-05\n",
      "  batch 401 loss: 0.00011168564243234869\n",
      "LOSS train 0.0053999197412689616 valid 0.00022361586161423475\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8247441514395178e-06\n",
      "  batch 101 loss: 0.00010540811879309331\n",
      "  batch 201 loss: 9.929645454803904e-05\n",
      "  batch 301 loss: 8.912193109836153e-05\n",
      "  batch 401 loss: 8.142575940496499e-05\n",
      "LOSS train 9.092314464091747e-05 valid 0.0002003286499530077\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.828086435329169e-06\n",
      "  batch 101 loss: 7.496746573508517e-05\n",
      "  batch 201 loss: 6.982728152479467e-05\n",
      "  batch 301 loss: 6.418707297825677e-05\n",
      "  batch 401 loss: 6.0878307728557956e-05\n",
      "LOSS train 6.626746056961125e-05 valid 0.00016258156392723322\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.306445821886882e-06\n",
      "  batch 101 loss: 6.126329659593921e-05\n",
      "  batch 201 loss: 5.957510588075365e-05\n",
      "  batch 301 loss: 5.761160755042738e-05\n",
      "  batch 401 loss: 5.753979147414157e-05\n",
      "LOSS train 5.8679135247389065e-05 valid 0.000156895985128358\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.219248563051224e-06\n",
      "  batch 101 loss: 6.0699054662336495e-05\n",
      "  batch 201 loss: 6.0285892972729014e-05\n",
      "  batch 301 loss: 5.904083400309901e-05\n",
      "  batch 401 loss: 5.9804925198250205e-05\n",
      "LOSS train 6.005771893228741e-05 valid 0.0001383580529363826\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.9348446221556516e-06\n",
      "  batch 101 loss: 6.030285786010836e-05\n",
      "  batch 201 loss: 5.8601838468916866e-05\n",
      "  batch 301 loss: 5.6607385618008264e-05\n",
      "  batch 401 loss: 5.5594138259493776e-05\n",
      "LOSS train 5.767224932643355e-05 valid 9.30945243453607e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1648330837488175e-06\n",
      "  batch 101 loss: 5.156760517252223e-05\n",
      "  batch 201 loss: 5.008417792396358e-05\n",
      "  batch 301 loss: 4.85522047631548e-05\n",
      "  batch 401 loss: 4.7972882830435994e-05\n",
      "LOSS train 4.943746232752182e-05 valid 7.475767779396847e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.843934145057574e-07\n",
      "  batch 101 loss: 4.522372322057322e-05\n",
      "  batch 201 loss: 4.478211116278885e-05\n",
      "  batch 301 loss: 4.513130135819665e-05\n",
      "  batch 401 loss: 4.597117594755673e-05\n",
      "LOSS train 4.5375016346067835e-05 valid 7.207078306237236e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003168109431862831\n",
      "  batch 101 loss: 2.7430062437441665\n",
      "  batch 201 loss: 0.0017937178898137062\n",
      "  batch 301 loss: 0.000557263228110969\n",
      "  batch 401 loss: 0.00022171877261826012\n",
      "LOSS train 0.6198507322368257 valid 0.0003819286357611418\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.818910627160221e-06\n",
      "  batch 101 loss: 6.87507037469004e-05\n",
      "  batch 201 loss: 2.5605539021853475e-05\n",
      "  batch 301 loss: 5.85878606091228e-05\n",
      "  batch 401 loss: 7.587960566297624e-05\n",
      "LOSS train 6.529340535242957e-05 valid 0.0006031447555869818\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.242144878953695e-06\n",
      "  batch 101 loss: 0.00013219784735156283\n",
      "  batch 201 loss: 6.565414884448728e-05\n",
      "  batch 301 loss: 0.00015028100708605052\n",
      "  batch 401 loss: 0.0003524561243739299\n",
      "LOSS train 0.00018640031870233357 valid 0.0006776439258828759\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0178172960877418e-05\n",
      "  batch 101 loss: 0.0003178255469197211\n",
      "  batch 201 loss: 4.322550915048851e-05\n",
      "  batch 301 loss: 8.546496331632625e-05\n",
      "  batch 401 loss: 0.00018890956439690854\n",
      "LOSS train 0.00016268322299444415 valid 0.0005170878139324486\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.129220662638545e-06\n",
      "  batch 101 loss: 0.01838060198699168\n",
      "  batch 201 loss: 0.17949231277918443\n",
      "  batch 301 loss: 0.12581773756508483\n",
      "  batch 401 loss: 0.005305095414660173\n",
      "LOSS train 0.07521481100675641 valid 0.036400578916072845\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.00036507092416286467\n",
      "  batch 101 loss: 0.015771301697823217\n",
      "  batch 201 loss: 0.03521972567148623\n",
      "  batch 301 loss: 0.05939776753366459\n",
      "  batch 401 loss: 0.07022667528290186\n",
      "LOSS train 0.041518821213997505 valid 0.01672213524580002\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0001257559098303318\n",
      "  batch 101 loss: 0.03011787550378358\n",
      "  batch 201 loss: 0.05629691951646237\n",
      "  batch 301 loss: 0.047509221262080244\n",
      "  batch 401 loss: 0.10536687144835014\n",
      "LOSS train 0.054625856093529525 valid 0.01182160247117281\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.693904399871826e-05\n",
      "  batch 101 loss: 0.04812213733588578\n",
      "  batch 201 loss: 0.09692412085365504\n",
      "  batch 301 loss: 0.08012835730565712\n",
      "  batch 401 loss: 0.06214891481795348\n",
      "LOSS train 0.0660689682331836 valid 0.03448030725121498\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0006519574671983719\n",
      "  batch 101 loss: 2.6070946746412664\n",
      "  batch 201 loss: 0.007941552517004312\n",
      "  batch 301 loss: 0.0033143382222624498\n",
      "  batch 401 loss: 0.0011993631138466298\n",
      "LOSS train 0.5915250192830863 valid 0.0005551542853936553\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.607105166651308e-06\n",
      "  batch 101 loss: 0.0004434483219665708\n",
      "  batch 201 loss: 0.000123710882216983\n",
      "  batch 301 loss: 5.2818080184806604e-05\n",
      "  batch 401 loss: 3.0031236046852428e-05\n",
      "LOSS train 0.00015350081680716898 valid 0.00010830281098606065\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.7891888273879885e-06\n",
      "  batch 101 loss: 9.967539938315895e-05\n",
      "  batch 201 loss: 2.537540841558439e-05\n",
      "  batch 301 loss: 3.695057519962575e-05\n",
      "  batch 401 loss: 0.00015763870490900444\n",
      "LOSS train 8.437868937816149e-05 valid 7.806101348251104e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.865391677711159e-06\n",
      "  batch 101 loss: 0.0003207683597702271\n",
      "  batch 201 loss: 6.254772465581482e-05\n",
      "  batch 301 loss: 1.919769352525691e-05\n",
      "  batch 401 loss: 0.0001246710690998043\n",
      "LOSS train 0.004214519745181209 valid 0.31098946928977966\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.002651934027671814\n",
      "  batch 101 loss: 0.15962824813555926\n",
      "  batch 201 loss: 0.014671424906118773\n",
      "  batch 301 loss: 0.020838951470796018\n",
      "  batch 401 loss: 0.15394855386344716\n",
      "LOSS train 0.08177847232872991 valid 0.03135791793465614\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0003863190487027168\n",
      "  batch 101 loss: 0.010836217296891847\n",
      "  batch 201 loss: 0.02131223187141586\n",
      "  batch 301 loss: 0.07219929298385978\n",
      "  batch 401 loss: 0.06566189865581691\n",
      "LOSS train 0.04584299095482702 valid 0.21398162841796875\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0024640941619873047\n",
      "  batch 101 loss: 0.12735002587083727\n",
      "  batch 201 loss: 0.01763324795756489\n",
      "  batch 301 loss: 0.011708515812060796\n",
      "  batch 401 loss: 0.28547886030341035\n",
      "LOSS train 0.11900206698361221 valid 0.0660356655716896\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0010284452885389328\n",
      "  batch 101 loss: 0.03650004031835124\n",
      "  batch 201 loss: 0.0116503056534566\n",
      "  batch 301 loss: 0.001893284185352968\n",
      "  batch 401 loss: 0.0028276953623571897\n",
      "LOSS train 0.012482912685640911 valid 0.006960038095712662\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005856911092996598\n",
      "  batch 101 loss: 2.7162131310254334\n",
      "  batch 201 loss: 0.011042958269827067\n",
      "  batch 301 loss: 0.0025971010403009133\n",
      "  batch 401 loss: 0.0005277191081404453\n",
      "LOSS train 0.6164897587346113 valid 0.00018702921806834638\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.5327981458976866e-06\n",
      "  batch 101 loss: 0.00019463798224023777\n",
      "  batch 201 loss: 3.421641140448628e-05\n",
      "  batch 301 loss: 2.8356843054098135e-05\n",
      "  batch 401 loss: 2.6940465670577395e-05\n",
      "LOSS train 6.694776389494546e-05 valid 4.617104059434496e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.4892430044710636e-07\n",
      "  batch 101 loss: 3.0482753654723638e-05\n",
      "  batch 201 loss: 3.298055050436233e-05\n",
      "  batch 301 loss: 3.20712324401029e-05\n",
      "  batch 401 loss: 0.00017412820345271028\n",
      "LOSS train 0.0001161257753205388 valid 0.0008136380929499865\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.274665404111147e-06\n",
      "  batch 101 loss: 0.0037987734941998495\n",
      "  batch 201 loss: 0.0024828766884456856\n",
      "  batch 301 loss: 0.00023565320760098984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.0003728268063059659\n",
      "LOSS train 0.0017129452975224951 valid 0.00331152998842299\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.5278687719255685e-05\n",
      "  batch 101 loss: 0.021121802807901987\n",
      "  batch 201 loss: 0.18000512858852744\n",
      "  batch 301 loss: 0.13137427026405932\n",
      "  batch 401 loss: 0.0213396650494542\n",
      "LOSS train 0.0808823258019763 valid 0.06325409561395645\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0004966526478528976\n",
      "  batch 101 loss: 0.08790205617900938\n",
      "  batch 201 loss: 0.030338022885844113\n",
      "  batch 301 loss: 0.08583440009853803\n",
      "  batch 401 loss: 0.13475583692081272\n",
      "LOSS train 0.08090583463730269 valid 0.21316634118556976\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0018599201738834382\n",
      "  batch 101 loss: 0.07618320208974183\n",
      "  batch 201 loss: 0.020715832896530628\n",
      "  batch 301 loss: 0.013055048669921234\n",
      "  batch 401 loss: 0.1296879740303848\n",
      "LOSS train 0.07913182895897591 valid 0.16813252866268158\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0020539993047714233\n",
      "  batch 101 loss: 0.2644154670089483\n",
      "  batch 201 loss: 0.0711570662073791\n",
      "  batch 301 loss: 0.02057724424288608\n",
      "  batch 401 loss: 0.01739696128293872\n",
      "LOSS train 0.08711410959962856 valid 0.05063363164663315\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00034114502370357513\n",
      "  batch 101 loss: 2.3026629862748087\n",
      "  batch 201 loss: 0.0088312025310006\n",
      "  batch 301 loss: 0.0007092129936791025\n",
      "  batch 401 loss: 0.0001872835633548675\n",
      "LOSS train 0.5220747071257361 valid 8.917357627069578e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.22335758432746e-06\n",
      "  batch 101 loss: 0.00020443972269276856\n",
      "  batch 201 loss: 4.080819542650716e-05\n",
      "  batch 301 loss: 5.767476116034231e-05\n",
      "  batch 401 loss: 4.941813455843658e-05\n",
      "LOSS train 8.953381293867266e-05 valid 0.00022552834707312286\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5842460561543703e-06\n",
      "  batch 101 loss: 0.00024223943717515794\n",
      "  batch 201 loss: 0.0001258033373960643\n",
      "  batch 301 loss: 0.00034770621947245674\n",
      "  batch 401 loss: 0.0005777248170488747\n",
      "LOSS train 0.00035482371828330126 valid 0.0004455947782844305\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.316134287975728e-06\n",
      "  batch 101 loss: 0.003337351964728441\n",
      "  batch 201 loss: 0.058186880142893645\n",
      "  batch 301 loss: 0.2129590749181807\n",
      "  batch 401 loss: 0.5451382042467594\n",
      "LOSS train 0.25093060993894556 valid 0.13939306139945984\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.002720494270324707\n",
      "  batch 101 loss: 0.7320927218347788\n",
      "  batch 201 loss: 0.029340853825560773\n",
      "  batch 301 loss: 0.001765196576452581\n",
      "  batch 401 loss: 0.00035682266807270937\n",
      "LOSS train 0.17299398598609045 valid 0.00014442758401855826\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.205577573273331e-06\n",
      "  batch 101 loss: 0.0006551632469199831\n",
      "  batch 201 loss: 0.0014110211734077894\n",
      "  batch 301 loss: 0.0005907286293950165\n",
      "  batch 401 loss: 0.002198586699960288\n",
      "LOSS train 0.0012515700112071212 valid 0.0034983931109309196\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.024744801223278e-05\n",
      "  batch 101 loss: 0.009219529449474066\n",
      "  batch 201 loss: 0.09513107072096318\n",
      "  batch 301 loss: 0.5845969884842634\n",
      "  batch 401 loss: 1.5330018915235997\n",
      "LOSS train 0.5132117042087784 valid 0.01583615317940712\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0004226187989115715\n",
      "  batch 101 loss: 0.014973195806669537\n",
      "  batch 201 loss: 0.0005028336110626697\n",
      "  batch 301 loss: 0.000126926176926645\n",
      "  batch 401 loss: 0.00031991807114536644\n",
      "LOSS train 0.003728168713470958 valid 0.001280234195291996\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 9.294680203311146e-06\n",
      "  batch 101 loss: 0.5698186430966962\n",
      "  batch 201 loss: 7.571475526674476e-05\n",
      "  batch 301 loss: 8.957146376133096e-05\n",
      "  batch 401 loss: 9.78726367031868e-05\n",
      "LOSS train 0.1286971484423696 valid 0.00012125123612349853\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.656114327488467e-06\n",
      "  batch 101 loss: 0.00010527003534775759\n",
      "  batch 201 loss: 0.0001092146630668367\n",
      "  batch 301 loss: 0.0001094063572355708\n",
      "  batch 401 loss: 0.00010724849896632804\n",
      "LOSS train 0.00010504942270068963 valid 0.0002194579574279487\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.1298649264499544e-06\n",
      "  batch 101 loss: 0.00010213619391947759\n",
      "  batch 201 loss: 9.731236526846487e-05\n",
      "  batch 301 loss: 8.881366121670454e-05\n",
      "  batch 401 loss: 8.16998710200778e-05\n",
      "LOSS train 8.986918634791303e-05 valid 0.00020039071387145668\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.8596338233910502e-06\n",
      "  batch 101 loss: 7.624351779952576e-05\n",
      "  batch 201 loss: 7.185900801346179e-05\n",
      "  batch 301 loss: 6.609074441030316e-05\n",
      "  batch 401 loss: 6.267241311206817e-05\n",
      "LOSS train 6.79295801245147e-05 valid 0.00016482238424941897\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.339283673791215e-06\n",
      "  batch 101 loss: 6.229106429714193e-05\n",
      "  batch 201 loss: 6.052060119657199e-05\n",
      "  batch 301 loss: 5.796611623878789e-05\n",
      "  batch 401 loss: 5.7446740820807916e-05\n",
      "LOSS train 5.908702284217002e-05 valid 0.0001568593579577282\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.219049638370052e-06\n",
      "  batch 101 loss: 6.0349842473215174e-05\n",
      "  batch 201 loss: 5.9950004589381935e-05\n",
      "  batch 301 loss: 5.8793831644834423e-05\n",
      "  batch 401 loss: 5.950358139600098e-05\n",
      "LOSS train 5.9697178227468667e-05 valid 0.00014486243890132755\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.034557401202619e-06\n",
      "  batch 101 loss: 6.0996922082381387e-05\n",
      "  batch 201 loss: 5.9566803930692916e-05\n",
      "  batch 301 loss: 5.743389375254537e-05\n",
      "  batch 401 loss: 5.6468284279844736e-05\n",
      "LOSS train 5.851587576867656e-05 valid 9.527698421152309e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2013848754577338e-06\n",
      "  batch 101 loss: 5.204418144728606e-05\n",
      "  batch 201 loss: 5.01812365484966e-05\n",
      "  batch 301 loss: 4.8362957393237594e-05\n",
      "  batch 401 loss: 4.7554496497923537e-05\n",
      "LOSS train 4.939109799956829e-05 valid 7.406922668451443e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00013187121599912645\n",
      "  batch 101 loss: 0.5582034143012424\n",
      "  batch 201 loss: 9.291575025599741e-05\n",
      "  batch 301 loss: 9.071783232002417e-05\n",
      "  batch 401 loss: 9.905795166559984e-05\n",
      "LOSS train 0.12610728105484004 valid 0.00012675901234615594\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7466048302594573e-06\n",
      "  batch 101 loss: 0.0001062110207880096\n",
      "  batch 201 loss: 0.00010976504604855108\n",
      "  batch 301 loss: 0.00010912545479584424\n",
      "  batch 401 loss: 0.00010602215499602607\n",
      "LOSS train 0.00010491638993392968 valid 0.00022233626805245876\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.1702301930636167e-06\n",
      "  batch 101 loss: 0.00010009395752945238\n",
      "  batch 201 loss: 9.478426033098231e-05\n",
      "  batch 301 loss: 8.608970706006857e-05\n",
      "  batch 401 loss: 7.90945361836748e-05\n",
      "LOSS train 8.751127470310099e-05 valid 0.00019559639622457325\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.790839353110641e-06\n",
      "  batch 101 loss: 7.40490153339124e-05\n",
      "  batch 201 loss: 6.990413605080903e-05\n",
      "  batch 301 loss: 6.451948055769208e-05\n",
      "  batch 401 loss: 6.147331537704304e-05\n",
      "LOSS train 6.631054114227553e-05 valid 0.00016266021702904254\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.3067968140821903e-06\n",
      "  batch 101 loss: 6.156005736855263e-05\n",
      "  batch 201 loss: 6.002175632360718e-05\n",
      "  batch 301 loss: 5.773485220188945e-05\n",
      "  batch 401 loss: 5.747286165501464e-05\n",
      "LOSS train 5.880755791045217e-05 valid 0.00015677195915486664\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2177217761054634e-06\n",
      "  batch 101 loss: 6.0583905348039477e-05\n",
      "  batch 201 loss: 6.021296943856669e-05\n",
      "  batch 301 loss: 5.9062724724299186e-05\n",
      "  batch 401 loss: 5.972510288771105e-05\n",
      "LOSS train 5.997265434286237e-05 valid 0.0001397588348481804\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.954667823156342e-06\n",
      "  batch 101 loss: 6.0418412004139555e-05\n",
      "  batch 201 loss: 5.8733286608685374e-05\n",
      "  batch 301 loss: 5.643218083264401e-05\n",
      "  batch 401 loss: 5.5220131409896565e-05\n",
      "LOSS train 5.755556090303115e-05 valid 9.000168211059645e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1005265696439892e-06\n",
      "  batch 101 loss: 5.0594833581953935e-05\n",
      "  batch 201 loss: 4.8927559511753315e-05\n",
      "  batch 301 loss: 4.738683104676511e-05\n",
      "  batch 401 loss: 4.685488086266787e-05\n",
      "LOSS train 4.833178063372348e-05 valid 7.334900874411687e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001718638651072979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 0.44484817397583354\n",
      "  batch 201 loss: 9.536155882869935e-05\n",
      "  batch 301 loss: 9.463896631359603e-05\n",
      "  batch 401 loss: 0.06091628784268778\n",
      "LOSS train 0.11425935415668319 valid 0.0001352795516140759\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8837675452232361e-06\n",
      "  batch 101 loss: 0.00010788235510347022\n",
      "  batch 201 loss: 0.00011019201520952037\n",
      "  batch 301 loss: 0.00010843888238127874\n",
      "  batch 401 loss: 0.00010417382712830658\n",
      "LOSS train 0.00010466053649441044 valid 0.0002243590133730322\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.1985368696041405e-06\n",
      "  batch 101 loss: 9.736702209139026e-05\n",
      "  batch 201 loss: 9.158854594716103e-05\n",
      "  batch 301 loss: 8.281200652845655e-05\n",
      "  batch 401 loss: 7.607022193951707e-05\n",
      "LOSS train 8.46088353592704e-05 valid 0.00018986177747137845\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.708056999836117e-06\n",
      "  batch 101 loss: 7.158854062708996e-05\n",
      "  batch 201 loss: 6.776167749990236e-05\n",
      "  batch 301 loss: 6.284494447186261e-05\n",
      "  batch 401 loss: 6.024154680233096e-05\n",
      "LOSS train 6.456134363276143e-05 valid 0.00016055518062785268\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2750560310669244e-06\n",
      "  batch 101 loss: 6.088023963343403e-05\n",
      "  batch 201 loss: 5.96092778800994e-05\n",
      "  batch 301 loss: 5.762102301389404e-05\n",
      "  batch 401 loss: 5.764647566536496e-05\n",
      "LOSS train 5.8643102868394325e-05 valid 0.00015650258865207434\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2136251209303737e-06\n",
      "  batch 101 loss: 6.0906013424073535e-05\n",
      "  batch 201 loss: 6.0505295694213145e-05\n",
      "  batch 301 loss: 5.929491173048973e-05\n",
      "  batch 401 loss: 5.980981303878252e-05\n",
      "LOSS train 6.0218392329667396e-05 valid 0.00013231403136160225\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.8363866547588259e-06\n",
      "  batch 101 loss: 5.939273332785433e-05\n",
      "  batch 201 loss: 5.744278939346259e-05\n",
      "  batch 301 loss: 5.5004254804771335e-05\n",
      "  batch 401 loss: 5.3595578029899117e-05\n",
      "LOSS train 5.6172529329417826e-05 valid 8.480077667627484e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.964858327293769e-07\n",
      "  batch 101 loss: 4.89904431077548e-05\n",
      "  batch 201 loss: 4.7596499847486484e-05\n",
      "  batch 301 loss: 4.642473858439189e-05\n",
      "  batch 401 loss: 4.625588219255405e-05\n",
      "LOSS train 4.7255920854237604e-05 valid 7.276672113221139e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 9.764662012457847e-05\n",
      "  batch 101 loss: 0.7588016478060127\n",
      "  batch 201 loss: 0.026372260939360785\n",
      "  batch 301 loss: 9.525216653628376e-05\n",
      "  batch 401 loss: 9.390930341851344e-05\n",
      "LOSS train 0.177313265106678 valid 0.00010522446245886385\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.3822602340951562e-06\n",
      "  batch 101 loss: 0.00010159158457270223\n",
      "  batch 201 loss: 0.00010631230818262338\n",
      "  batch 301 loss: 0.00010867196664889888\n",
      "  batch 401 loss: 0.00010939886008699773\n",
      "LOSS train 0.00010428062212253682 valid 0.0002024025743594393\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.888395101763308e-06\n",
      "  batch 101 loss: 0.00010735653747815377\n",
      "  batch 201 loss: 0.00010466937687624523\n",
      "  batch 301 loss: 9.770227947967669e-05\n",
      "  batch 401 loss: 9.094633892061665e-05\n",
      "LOSS train 9.725228021087096e-05 valid 0.00021545843628700823\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.0735967447981237e-06\n",
      "  batch 101 loss: 8.465027985607776e-05\n",
      "  batch 201 loss: 7.971511574396573e-05\n",
      "  batch 301 loss: 7.275871836782245e-05\n",
      "  batch 401 loss: 6.806913908917522e-05\n",
      "LOSS train 7.459685518716914e-05 valid 0.00017520385154057294\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.4937506532296537e-06\n",
      "  batch 101 loss: 6.60306484155626e-05\n",
      "  batch 201 loss: 6.341760285636156e-05\n",
      "  batch 301 loss: 5.982546645327602e-05\n",
      "  batch 401 loss: 5.831964506342047e-05\n",
      "LOSS train 6.114268847856321e-05 valid 0.00015782857371959835\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.233771811006591e-06\n",
      "  batch 101 loss: 6.013868895252017e-05\n",
      "  batch 201 loss: 5.9364148264648974e-05\n",
      "  batch 301 loss: 5.786024117924171e-05\n",
      "  batch 401 loss: 5.830826020428504e-05\n",
      "LOSS train 5.8780237489364454e-05 valid 0.0001547849242342636\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.187451609643176e-06\n",
      "  batch 101 loss: 6.139750258597587e-05\n",
      "  batch 201 loss: 6.075673708664908e-05\n",
      "  batch 301 loss: 5.928795957487409e-05\n",
      "  batch 401 loss: 5.936125991269137e-05\n",
      "LOSS train 6.025598611827222e-05 valid 0.00011867964349221438\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.6133050667122006e-06\n",
      "  batch 101 loss: 5.71455700497836e-05\n",
      "  batch 201 loss: 5.502414293687252e-05\n",
      "  batch 301 loss: 5.261606387421125e-05\n",
      "  batch 401 loss: 5.120584870837774e-05\n",
      "LOSS train 5.3792352979327946e-05 valid 7.940573414089158e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005519335716962815\n",
      "  batch 101 loss: 0.07923226091380002\n",
      "  batch 201 loss: 0.00010438214723563988\n",
      "  batch 301 loss: 9.43461584009242e-05\n",
      "  batch 401 loss: 7.492268980854533e-05\n",
      "LOSS train 0.018076851345387925 valid 0.0001763176842359826\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.5101835490204394e-06\n",
      "  batch 101 loss: 6.487296994237113e-05\n",
      "  batch 201 loss: 6.067761610722755e-05\n",
      "  batch 301 loss: 5.751849494174621e-05\n",
      "  batch 401 loss: 5.745972961733514e-05\n",
      "LOSS train 5.9833593758146594e-05 valid 0.00015506360796280205\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1917035337537527e-06\n",
      "  batch 101 loss: 6.117909230681562e-05\n",
      "  batch 201 loss: 6.052877778898846e-05\n",
      "  batch 301 loss: 5.8798981966106114e-05\n",
      "  batch 401 loss: 5.820758314200703e-05\n",
      "LOSS train 5.965330947005342e-05 valid 0.00010276914690621197\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3385599595494568e-06\n",
      "  batch 101 loss: 5.353223143629293e-05\n",
      "  batch 201 loss: 5.08776955086887e-05\n",
      "  batch 301 loss: 4.851156613653984e-05\n",
      "  batch 401 loss: 4.739679427359533e-05\n",
      "LOSS train 4.988989990279798e-05 valid 7.356321293627843e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.463443762389943e-07\n",
      "  batch 101 loss: 4.464256147514334e-05\n",
      "  batch 201 loss: 4.4720097105255266e-05\n",
      "  batch 301 loss: 4.559243226367471e-05\n",
      "  batch 401 loss: 4.7355128741628506e-05\n",
      "LOSS train 4.5782528924155894e-05 valid 6.90412416588515e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.270499579841271e-07\n",
      "  batch 101 loss: 4.6555752070389645e-05\n",
      "  batch 201 loss: 4.8158370953501615e-05\n",
      "  batch 301 loss: 4.894052119709613e-05\n",
      "  batch 401 loss: 4.744637848759225e-05\n",
      "LOSS train 4.7763181314117484e-05 valid 6.304287671810016e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.6502921425853857e-07\n",
      "  batch 101 loss: 4.144621672082849e-05\n",
      "  batch 201 loss: 4.112446738020026e-05\n",
      "  batch 301 loss: 3.9430763623045094e-05\n",
      "  batch 401 loss: 3.756888431823313e-05\n",
      "LOSS train 4.0011290575615056e-05 valid 6.515741551993415e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1730189726222307e-07\n",
      "  batch 101 loss: 3.586907650912963e-05\n",
      "  batch 201 loss: 3.5738074714828375e-05\n",
      "  batch 301 loss: 3.6036719004926e-05\n",
      "  batch 401 loss: 3.613526288347657e-05\n",
      "LOSS train 3.642602905760168e-05 valid 6.164739170344546e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004897591471672058\n",
      "  batch 101 loss: 1.6703695649234578\n",
      "  batch 201 loss: 0.00017917190456500975\n",
      "  batch 301 loss: 7.639423289219849e-05\n",
      "  batch 401 loss: 8.131496625082946e-05\n",
      "LOSS train 0.3772530573728268 valid 7.429016113746911e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.641124830115587e-07\n",
      "  batch 101 loss: 8.702959569518498e-05\n",
      "  batch 201 loss: 9.0884367587023e-05\n",
      "  batch 301 loss: 9.461248103207254e-05\n",
      "  batch 401 loss: 9.898583806716488e-05\n",
      "LOSS train 9.249453841424874e-05 valid 0.00011437524517532438\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.540764351375401e-06\n",
      "  batch 101 loss: 0.00010356013234627426\n",
      "  batch 201 loss: 0.00010654567815777228\n",
      "  batch 301 loss: 0.00010812456388407554\n",
      "  batch 401 loss: 0.00010938021077606663\n",
      "LOSS train 0.0001049352720137411 valid 0.0001858289906522259\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.6494995108805597e-06\n",
      "  batch 101 loss: 0.00010934944815005565\n",
      "  batch 201 loss: 0.0001088443500862013\n",
      "  batch 301 loss: 0.00010474107001584798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.00010035760824905537\n",
      "LOSS train 0.00010295076026715492 valid 0.00022398997680284083\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.193375887349248e-06\n",
      "  batch 101 loss: 9.546158183809439e-05\n",
      "  batch 201 loss: 9.144406158299035e-05\n",
      "  batch 301 loss: 8.434044620969416e-05\n",
      "  batch 401 loss: 7.87830923366073e-05\n",
      "LOSS train 8.527123958947054e-05 valid 0.00019666070875246078\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.8061450575478376e-06\n",
      "  batch 101 loss: 7.496896779571216e-05\n",
      "  batch 201 loss: 7.143395752336801e-05\n",
      "  batch 301 loss: 6.626215964388394e-05\n",
      "  batch 401 loss: 6.316017319022648e-05\n",
      "LOSS train 6.770760205808346e-05 valid 0.00016623463307041675\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.360441576456651e-06\n",
      "  batch 101 loss: 6.286113449505137e-05\n",
      "  batch 201 loss: 6.107286444944294e-05\n",
      "  batch 301 loss: 5.835643695377257e-05\n",
      "  batch 401 loss: 5.75880809424234e-05\n",
      "LOSS train 5.941578380347881e-05 valid 0.00015707944112364203\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2223949781619013e-06\n",
      "  batch 101 loss: 6.014395901729586e-05\n",
      "  batch 201 loss: 5.962754812571802e-05\n",
      "  batch 301 loss: 5.8358332753414286e-05\n",
      "  batch 401 loss: 5.899056262535396e-05\n",
      "LOSS train 5.924566652267487e-05 valid 0.0001512236922280863\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0013464543223381043\n",
      "  batch 101 loss: 0.05508599444167316\n",
      "  batch 201 loss: 9.557621933083738e-05\n",
      "  batch 301 loss: 7.285365475354411e-05\n",
      "  batch 401 loss: 6.065686463728071e-05\n",
      "LOSS train 0.012795103939913774 valid 0.00015686416008975357\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.219122980022803e-06\n",
      "  batch 101 loss: 5.995782073227929e-05\n",
      "  batch 201 loss: 5.94832431784198e-05\n",
      "  batch 301 loss: 5.864253832896793e-05\n",
      "  batch 401 loss: 5.9478132222920976e-05\n",
      "LOSS train 5.9553621773220195e-05 valid 0.00012301433889660984\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.685252645984292e-06\n",
      "  batch 101 loss: 5.7270882199986774e-05\n",
      "  batch 201 loss: 5.3822952647806236e-05\n",
      "  batch 301 loss: 5.0532359705357524e-05\n",
      "  batch 401 loss: 4.861491016868058e-05\n",
      "LOSS train 5.2276619116198445e-05 valid 7.44261487852782e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.674025255255401e-07\n",
      "  batch 101 loss: 4.495008908492082e-05\n",
      "  batch 201 loss: 4.471990572568529e-05\n",
      "  batch 301 loss: 4.53832070553517e-05\n",
      "  batch 401 loss: 4.7106245791610493e-05\n",
      "LOSS train 4.5735838713299056e-05 valid 6.935689452802762e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.360057886922732e-07\n",
      "  batch 101 loss: 4.647664641595384e-05\n",
      "  batch 201 loss: 4.812354707780741e-05\n",
      "  batch 301 loss: 4.893943049069094e-05\n",
      "  batch 401 loss: 4.732721494377756e-05\n",
      "LOSS train 4.770112605572487e-05 valid 6.325303547782823e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.586734470038209e-07\n",
      "  batch 101 loss: 4.118377915119709e-05\n",
      "  batch 201 loss: 4.0757635409249816e-05\n",
      "  batch 301 loss: 3.907064598308807e-05\n",
      "  batch 401 loss: 3.728816475103258e-05\n",
      "LOSS train 3.971050723975564e-05 valid 6.488630606327206e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.219869045598898e-07\n",
      "  batch 101 loss: 3.579722363056703e-05\n",
      "  batch 201 loss: 3.567689634721205e-05\n",
      "  batch 301 loss: 3.6082468674720755e-05\n",
      "  batch 401 loss: 3.6299891696671694e-05\n",
      "LOSS train 3.645541364402367e-05 valid 6.19098573224619e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.3842647098936143e-07\n",
      "  batch 101 loss: 3.7688463925746874e-05\n",
      "  batch 201 loss: 3.801827906045219e-05\n",
      "  batch 301 loss: 4.0370237221054596e-05\n",
      "  batch 401 loss: 4.2498360221827626e-05\n",
      "LOSS train 4.029914854257588e-05 valid 8.329634874826297e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00010599778965115547\n",
      "  batch 101 loss: 0.004240519237550302\n",
      "  batch 201 loss: 4.474910704686863e-05\n",
      "  batch 301 loss: 4.065963555831331e-05\n",
      "  batch 401 loss: 3.6023469411361476e-05\n",
      "LOSS train 0.0010125233106454542 valid 6.212026346474886e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.563536301953718e-07\n",
      "  batch 101 loss: 3.8977623188998225e-05\n",
      "  batch 201 loss: 4.2816869975297326e-05\n",
      "  batch 301 loss: 5.168755580967854e-05\n",
      "  batch 401 loss: 5.892632566201428e-05\n",
      "LOSS train 4.9214357360007986e-05 valid 0.00011908415763173252\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.6200645768549294e-06\n",
      "  batch 101 loss: 6.188436072307013e-05\n",
      "  batch 201 loss: 6.02602885453507e-05\n",
      "  batch 301 loss: 5.530984932647698e-05\n",
      "  batch 401 loss: 5.222743121066742e-05\n",
      "LOSS train 5.6913135309893256e-05 valid 6.62688835291192e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.41940207767766e-07\n",
      "  batch 101 loss: 5.024581641237091e-05\n",
      "  batch 201 loss: 5.512307449521359e-05\n",
      "  batch 301 loss: 5.647773773944209e-05\n",
      "  batch 401 loss: 6.024106546050234e-05\n",
      "LOSS train 5.5858437370017854e-05 valid 6.532733095809817e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1450733836682047e-07\n",
      "  batch 101 loss: 5.363565254128844e-05\n",
      "  batch 201 loss: 5.7229223417039064e-05\n",
      "  batch 301 loss: 5.4570608567132696e-05\n",
      "  batch 401 loss: 5.254868979847061e-05\n",
      "LOSS train 5.435703382656519e-05 valid 7.062045915517956e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.888923169550253e-08\n",
      "  batch 101 loss: 4.4376169110478256e-05\n",
      "  batch 201 loss: 4.535554604558456e-05\n",
      "  batch 301 loss: 4.456893297856368e-05\n",
      "  batch 401 loss: 4.383043423501931e-05\n",
      "LOSS train 4.476206186347213e-05 valid 6.197394395712763e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.442890738369897e-07\n",
      "  batch 101 loss: 4.293713988829495e-05\n",
      "  batch 201 loss: 4.34704205304115e-05\n",
      "  batch 301 loss: 4.4793372118761e-05\n",
      "  batch 401 loss: 4.565827890928631e-05\n",
      "LOSS train 4.444316936261997e-05 valid 6.932321412023157e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.3505613070447e-07\n",
      "  batch 101 loss: 4.4354013306246996e-05\n",
      "  batch 201 loss: 4.457378851839167e-05\n",
      "  batch 301 loss: 4.4811061511040864e-05\n",
      "  batch 401 loss: 4.617130085932786e-05\n",
      "LOSS train 4.511077162421293e-05 valid 6.757359369657934e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 5.5030849762260917e-05\n",
      "  batch 101 loss: 4.865862938514911\n",
      "  batch 201 loss: 0.0009489800652954728\n",
      "  batch 301 loss: 6.673618510831147e-05\n",
      "  batch 401 loss: 7.168797017129692e-05\n",
      "LOSS train 1.0986540363089063 valid 6.432576628867537e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.7105848352657634e-07\n",
      "  batch 101 loss: 7.455977026438632e-05\n",
      "  batch 201 loss: 7.644488659025228e-05\n",
      "  batch 301 loss: 7.829180792214174e-05\n",
      "  batch 401 loss: 8.097995444131812e-05\n",
      "LOSS train 7.79430511163555e-05 valid 7.153015758376569e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.948337977519259e-07\n",
      "  batch 101 loss: 8.418987974891933e-05\n",
      "  batch 201 loss: 8.626045642358804e-05\n",
      "  batch 301 loss: 8.840628816869867e-05\n",
      "  batch 401 loss: 9.147869682237796e-05\n",
      "LOSS train 8.752128953016444e-05 valid 8.870648889569566e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0750959336291998e-06\n",
      "  batch 101 loss: 9.510750883464426e-05\n",
      "  batch 201 loss: 9.744211815586823e-05\n",
      "  batch 301 loss: 9.964537085807023e-05\n",
      "  batch 401 loss: 0.0001025665778865914\n",
      "LOSS train 9.784338721035034e-05 valid 0.00012533098924905062\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.723292371025309e-06\n",
      "  batch 101 loss: 0.00010563384563965883\n",
      "  batch 201 loss: 0.00010768608080240937\n",
      "  batch 301 loss: 0.0001084711119858639\n",
      "  batch 401 loss: 0.00010938104714000474\n",
      "LOSS train 0.00010581647331322832 valid 0.00018271223234478384\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.6040390366688372e-06\n",
      "  batch 101 loss: 0.00010959242941197545\n",
      "  batch 201 loss: 0.00010958077368087515\n",
      "  batch 301 loss: 0.00010647531895756401\n",
      "  batch 401 loss: 0.00010328291038035786\n",
      "LOSS train 0.00010447620814133368 valid 0.00022264261497184634\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.1745221349410713e-06\n",
      "  batch 101 loss: 9.959865450241523e-05\n",
      "  batch 201 loss: 9.656055925603368e-05\n",
      "  batch 301 loss: 9.014967240887017e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 8.484161181456784e-05\n",
      "LOSS train 9.03652538016542e-05 valid 0.0002078743709716946\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9663110035471618e-06\n",
      "  batch 101 loss: 8.075011261098553e-05\n",
      "  batch 201 loss: 7.713153504710135e-05\n",
      "  batch 301 loss: 7.137979007893592e-05\n",
      "  batch 401 loss: 6.756409655167772e-05\n",
      "LOSS train 7.267638217703357e-05 valid 0.00017514883074909449\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.671771734952927e-05\n",
      "  batch 101 loss: 29.19448794039432\n",
      "  batch 201 loss: 0.005715275695547461\n",
      "  batch 301 loss: 0.004335351929184981\n",
      "  batch 401 loss: 0.003416164197260514\n",
      "LOSS train 6.59345871853899 valid 0.0008336320170201361\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.5504445657134055e-05\n",
      "  batch 101 loss: 0.0016746012905787212\n",
      "  batch 201 loss: 0.0010367523378954503\n",
      "  batch 301 loss: 0.00065093061683001\n",
      "  batch 401 loss: 0.00040868434283765966\n",
      "LOSS train 0.0008827351543431656 valid 0.00011855662887683138\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.4025858622044327e-06\n",
      "  batch 101 loss: 0.00017026857236487558\n",
      "  batch 201 loss: 0.000109549886892637\n",
      "  batch 301 loss: 7.318581470826758e-05\n",
      "  batch 401 loss: 4.935915667374502e-05\n",
      "LOSS train 9.577768702088003e-05 valid 4.144160993746482e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.3208008971996604e-07\n",
      "  batch 101 loss: 3.975029896537308e-05\n",
      "  batch 201 loss: 3.9232852582244956e-05\n",
      "  batch 301 loss: 3.805149252002593e-05\n",
      "  batch 401 loss: 3.8615212438344316e-05\n",
      "LOSS train 3.8698682854057886e-05 valid 3.7936813896521926e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.9481390558648853e-07\n",
      "  batch 101 loss: 3.855344273688388e-05\n",
      "  batch 201 loss: 3.813867880126054e-05\n",
      "  batch 301 loss: 3.8373194738596796e-05\n",
      "  batch 401 loss: 4.0198646347562315e-05\n",
      "LOSS train 3.888980693159334e-05 valid 3.723367626662366e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.7754795155487956e-07\n",
      "  batch 101 loss: 3.614972425111773e-05\n",
      "  batch 201 loss: 3.927204799765605e-05\n",
      "  batch 301 loss: 4.0876701932575086e-05\n",
      "  batch 401 loss: 4.0942378295767414e-05\n",
      "LOSS train 3.9340027343046514e-05 valid 3.7597983464365825e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 4.0719252865528686e-07\n",
      "  batch 101 loss: 4.252618195550895e-05\n",
      "  batch 201 loss: 4.2495448315094106e-05\n",
      "  batch 301 loss: 4.258378703525523e-05\n",
      "  batch 401 loss: 0.0007934099969475028\n",
      "LOSS train 0.0002155672833274596 valid 0.0001050637147272937\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3794153346680104e-06\n",
      "  batch 101 loss: 0.00010043368687092879\n",
      "  batch 201 loss: 0.00010212837966037114\n",
      "  batch 301 loss: 0.00010365563948198541\n",
      "  batch 401 loss: 0.000108797915197556\n",
      "LOSS train 0.00010244521249342245 valid 0.00014206042396835983\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00015302828513085842\n",
      "  batch 101 loss: 22.02279881727882\n",
      "  batch 201 loss: 0.0071107113920152185\n",
      "  batch 301 loss: 0.004492038143798709\n",
      "  batch 401 loss: 0.002901469353819266\n",
      "LOSS train 4.9748099375082315 valid 0.003913444932550192\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.850246684625745e-05\n",
      "  batch 101 loss: 0.0016040674655232578\n",
      "  batch 201 loss: 0.0009198158458457328\n",
      "  batch 301 loss: 0.0005996116538881324\n",
      "  batch 401 loss: 0.00037173630567849614\n",
      "LOSS train 0.0008132752110022982 valid 0.0005371946026571095\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.9515029746107756e-06\n",
      "  batch 101 loss: 0.00016837860479427036\n",
      "  batch 201 loss: 0.00011778547672292916\n",
      "  batch 301 loss: 8.873086426319787e-05\n",
      "  batch 401 loss: 7.910917814115237e-05\n",
      "LOSS train 0.00011074853374702388 valid 6.511848914669827e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.075715671409853e-07\n",
      "  batch 101 loss: 7.584376415707083e-05\n",
      "  batch 201 loss: 7.696947903241381e-05\n",
      "  batch 301 loss: 7.761129895698105e-05\n",
      "  batch 401 loss: 7.922186827727273e-05\n",
      "LOSS train 7.769360308063429e-05 valid 6.894496618770063e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.222714728210121e-07\n",
      "  batch 101 loss: 8.133006022944755e-05\n",
      "  batch 201 loss: 8.238824319960258e-05\n",
      "  batch 301 loss: 8.365265917291253e-05\n",
      "  batch 401 loss: 8.581000423419028e-05\n",
      "LOSS train 8.338056547244773e-05 valid 7.698149420320988e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.268270175904036e-07\n",
      "  batch 101 loss: 8.838244736580236e-05\n",
      "  batch 201 loss: 8.996437217319909e-05\n",
      "  batch 301 loss: 9.160665617400809e-05\n",
      "  batch 401 loss: 9.416550968126103e-05\n",
      "LOSS train 9.074633117996541e-05 valid 9.392520587425679e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1759212065953762e-06\n",
      "  batch 101 loss: 9.717503089632374e-05\n",
      "  batch 201 loss: 9.902854219262736e-05\n",
      "  batch 301 loss: 0.0001007718052733253\n",
      "  batch 401 loss: 0.00010325950876904243\n",
      "LOSS train 9.91055204814643e-05 valid 0.0001271190558327362\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.7534938524477183e-06\n",
      "  batch 101 loss: 0.00010588170961682408\n",
      "  batch 201 loss: 0.00010769684731201323\n",
      "  batch 301 loss: 0.00010834261993636574\n",
      "  batch 401 loss: 0.00010929460466513774\n",
      "LOSS train 0.00010588351635691483 valid 0.0001785309286788106\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0006020625308156013\n",
      "  batch 101 loss: 26.450425646230578\n",
      "  batch 201 loss: 0.026099184844642877\n",
      "  batch 301 loss: 0.008804621691815555\n",
      "  batch 401 loss: 0.003937600824283436\n",
      "LOSS train 5.979868639776122 valid 0.0006337874219752848\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8707761773839594e-05\n",
      "  batch 101 loss: 0.0011655208945740014\n",
      "  batch 201 loss: 0.0004263355782313738\n",
      "  batch 301 loss: 0.00021361623774282634\n",
      "  batch 401 loss: 0.00012497234674810898\n",
      "LOSS train 0.0004505214793481126 valid 6.156560266390443e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.945998054812663e-07\n",
      "  batch 101 loss: 8.732078817047295e-05\n",
      "  batch 201 loss: 8.077720632172713e-05\n",
      "  batch 301 loss: 7.650965397260735e-05\n",
      "  batch 401 loss: 7.611352599269594e-05\n",
      "LOSS train 8.006210347144674e-05 valid 6.367380410665646e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.915860674576834e-07\n",
      "  batch 101 loss: 7.592361472234188e-05\n",
      "  batch 201 loss: 7.690316318075929e-05\n",
      "  batch 301 loss: 7.761181775094882e-05\n",
      "  batch 401 loss: 7.877250719502626e-05\n",
      "LOSS train 7.756797498500594e-05 valid 6.777433009119704e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.186471728142351e-07\n",
      "  batch 101 loss: 8.083952934612171e-05\n",
      "  batch 201 loss: 8.180203331903613e-05\n",
      "  batch 301 loss: 8.286055539429071e-05\n",
      "  batch 401 loss: 8.45486250182148e-05\n",
      "LOSS train 8.260601976475604e-05 valid 7.474298035958782e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.748726056888699e-07\n",
      "  batch 101 loss: 8.704311527935716e-05\n",
      "  batch 201 loss: 8.863898283379967e-05\n",
      "  batch 301 loss: 8.961060200590509e-05\n",
      "  batch 401 loss: 9.206958219010631e-05\n",
      "LOSS train 8.914344250421294e-05 valid 8.816256740828976e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0711337381508201e-06\n",
      "  batch 101 loss: 9.484016429041731e-05\n",
      "  batch 201 loss: 9.639388985760889e-05\n",
      "  batch 301 loss: 9.809970438254823e-05\n",
      "  batch 401 loss: 0.00010061352264074231\n",
      "LOSS train 9.676512703202414e-05 valid 0.00011393334716558456\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.5406691818498074e-06\n",
      "  batch 101 loss: 0.00010336307642546671\n",
      "  batch 201 loss: 0.0001052009281829669\n",
      "  batch 301 loss: 0.00010618234533183113\n",
      "  batch 401 loss: 0.00010799048819023937\n",
      "LOSS train 0.00010412513519924325 valid 0.00015787690063007176\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003702577203512192\n",
      "  batch 101 loss: 45.53514358824119\n",
      "  batch 201 loss: 0.02519442521035671\n",
      "  batch 301 loss: 0.006901211312797386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.002989785164463683\n",
      "LOSS train 10.286959293668454 valid 0.003889512736350298\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5997900627553464e-05\n",
      "  batch 101 loss: 0.0005897601510332606\n",
      "  batch 201 loss: 0.00018519499747071677\n",
      "  batch 301 loss: 0.00015578635170641065\n",
      "  batch 401 loss: 0.0003830221847601933\n",
      "LOSS train 0.00038290373762079715 valid 0.005832959897816181\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.46633854880929e-05\n",
      "  batch 101 loss: 0.0004873673975077963\n",
      "  batch 201 loss: 4.632259157688168e-05\n",
      "  batch 301 loss: 5.3880283265925757e-05\n",
      "  batch 401 loss: 0.0006025680545076284\n",
      "LOSS train 0.0008183105218009406 valid 0.016711533069610596\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.0001434817537665367\n",
      "  batch 101 loss: 0.005516155607110705\n",
      "  batch 201 loss: 0.0014043312500871253\n",
      "  batch 301 loss: 8.953646899044543e-05\n",
      "  batch 401 loss: 0.0015997858325579274\n",
      "LOSS train 0.0023476568862037532 valid 0.007196938619017601\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.8386881351470946e-05\n",
      "  batch 101 loss: 0.00433823686611504\n",
      "  batch 201 loss: 4.037968564624461\n",
      "  batch 301 loss: 0.37200317098526287\n",
      "  batch 401 loss: 0.1293816958446405\n",
      "LOSS train 1.0437485323874165 valid 0.13853158056735992\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.000336906798183918\n",
      "  batch 101 loss: 0.3824142756406218\n",
      "  batch 201 loss: 0.7180589209683239\n",
      "  batch 301 loss: 1.0688155677122995\n",
      "  batch 401 loss: 1.4054724334506319\n",
      "LOSS train 0.9072084018233196 valid 0.6179414391517639\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0009888629615306854\n",
      "  batch 101 loss: 1.341425182344392\n",
      "  batch 201 loss: 0.6066371325729415\n",
      "  batch 301 loss: 0.28041725016199054\n",
      "  batch 401 loss: 2.964621643933933\n",
      "LOSS train 1.434154263574054 valid 0.5700945854187012\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.003975529372692108\n",
      "  batch 101 loss: 0.7229607953177765\n",
      "  batch 201 loss: 0.43403359344229103\n",
      "  batch 301 loss: 0.06732336585817393\n",
      "  batch 401 loss: 1.7847030335571616\n",
      "LOSS train 0.8754587584188571 valid 3.1126351356506348\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00020455272868275643\n",
      "  batch 101 loss: 45.65105370733887\n",
      "  batch 201 loss: 0.07537869106978178\n",
      "  batch 301 loss: 0.026783360596746207\n",
      "  batch 401 loss: 0.009566506675910205\n",
      "LOSS train 10.330689626094953 valid 0.002386046340689063\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00013549174182116985\n",
      "  batch 101 loss: 0.00311285627016332\n",
      "  batch 201 loss: 0.000780034266936127\n",
      "  batch 301 loss: 0.00032443245989270507\n",
      "  batch 401 loss: 0.0001417133500945056\n",
      "LOSS train 0.0010225457066816432 valid 0.0011485915165394545\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.2383954860270026e-05\n",
      "  batch 101 loss: 0.0005105854796420317\n",
      "  batch 201 loss: 5.930548592004925e-05\n",
      "  batch 301 loss: 0.0001011669666695525\n",
      "  batch 401 loss: 6.88542286388838e-05\n",
      "LOSS train 0.00018248284384704514 valid 0.00031299376860260963\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0167009168071672e-06\n",
      "  batch 101 loss: 8.127732165121416e-05\n",
      "  batch 201 loss: 2.5091935481214022e-05\n",
      "  batch 301 loss: 4.070819356456923e-05\n",
      "  batch 401 loss: 7.170815674726328e-05\n",
      "LOSS train 5.928311380500231e-05 valid 4.4751395762432367e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.02073918748647e-08\n",
      "  batch 101 loss: 0.000267796121520405\n",
      "  batch 201 loss: 3.9962856871791157\n",
      "  batch 301 loss: 0.5234403087571263\n",
      "  batch 401 loss: 0.04929713069461286\n",
      "LOSS train 1.0330422594664643 valid 0.002566934796050191\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.514213021844625e-05\n",
      "  batch 101 loss: 0.031712646016385404\n",
      "  batch 201 loss: 1.3126138041121884\n",
      "  batch 301 loss: 0.7030191313661635\n",
      "  batch 401 loss: 2.2630025116354227\n",
      "LOSS train 1.048801693595992 valid 0.6911253929138184\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.012238978147506714\n",
      "  batch 101 loss: 0.8412984215840698\n",
      "  batch 201 loss: 0.09989495305344462\n",
      "  batch 301 loss: 0.7083530910313129\n",
      "  batch 401 loss: 2.096626742631197\n",
      "LOSS train 0.9755069833316128 valid 1.8949451446533203\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.011660165786743164\n",
      "  batch 101 loss: 1.086048012599349\n",
      "  batch 201 loss: 0.1750781739410013\n",
      "  batch 301 loss: 0.2641718970704824\n",
      "  batch 401 loss: 1.76087392821908\n",
      "LOSS train 0.9492156352957135 valid 0.06144864484667778\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00017020627856254578\n",
      "  batch 101 loss: 45.460289609730246\n",
      "  batch 201 loss: 0.1164923732727766\n",
      "  batch 301 loss: 0.02639581951778382\n",
      "  batch 401 loss: 0.005295885482337326\n",
      "LOSS train 10.295572888109918 valid 0.0013630954781547189\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00010056629776954651\n",
      "  batch 101 loss: 0.0016563748770568054\n",
      "  batch 201 loss: 0.0001224033436301397\n",
      "  batch 301 loss: 6.134203600595356e-05\n",
      "  batch 401 loss: 4.869751972364611e-05\n",
      "LOSS train 0.0004523967552374352 valid 6.782461423426867e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.419259155634791e-06\n",
      "  batch 101 loss: 8.421962214015366e-05\n",
      "  batch 201 loss: 3.599625021251995e-05\n",
      "  batch 301 loss: 4.494221235290752e-05\n",
      "  batch 401 loss: 0.00015380535837721253\n",
      "LOSS train 8.878464092648885e-05 valid 0.0017137251561507583\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.4995107194408775e-05\n",
      "  batch 101 loss: 0.0008431431063218042\n",
      "  batch 201 loss: 0.00010350638872296259\n",
      "  batch 301 loss: 0.00010833280975930393\n",
      "  batch 401 loss: 9.740382052768837e-05\n",
      "LOSS train 0.00029560850136608194 valid 0.001577801420353353\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.2804578291252256e-05\n",
      "  batch 101 loss: 0.028362403558567166\n",
      "  batch 201 loss: 3.991103174556047\n",
      "  batch 301 loss: 0.8246065140515566\n",
      "  batch 401 loss: 0.434867995493114\n",
      "LOSS train 1.2058162715784788 valid 0.36236435174942017\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.003592798113822937\n",
      "  batch 101 loss: 0.6759230687096714\n",
      "  batch 201 loss: 1.284063717201352\n",
      "  batch 301 loss: 0.9836506593972445\n",
      "  batch 401 loss: 1.134175310060382\n",
      "LOSS train 1.0418646143619013 valid 3.3545660972595215\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.03150176525115967\n",
      "  batch 101 loss: 1.580631608068943\n",
      "  batch 201 loss: 1.9160315603017808\n",
      "  batch 301 loss: 0.14981459190137683\n",
      "  batch 401 loss: 0.31623397215269505\n",
      "LOSS train 0.954671943808315 valid 7.260406017303467\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.06913774490356445\n",
      "  batch 101 loss: 3.968218073248863\n",
      "  batch 201 loss: 1.2693374913930893\n",
      "  batch 301 loss: 0.9731575068086386\n",
      "  batch 401 loss: 0.41374087244272234\n",
      "LOSS train 1.5419908707948826 valid 0.02194800041615963\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00024490367621183396\n",
      "  batch 101 loss: 39.72935973584652\n",
      "  batch 201 loss: 0.19010875390842558\n",
      "  batch 301 loss: 0.007417691295268014\n",
      "  batch 401 loss: 0.0010365218028891832\n",
      "LOSS train 9.013189407790874 valid 0.00047864875523373485\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.429870121181011e-05\n",
      "  batch 101 loss: 0.0014175086467002984\n",
      "  batch 201 loss: 5.964609002148791e-05\n",
      "  batch 301 loss: 5.187466230381688e-05\n",
      "  batch 401 loss: 6.256176502574817e-05\n",
      "LOSS train 0.00037417094740221295 valid 6.340818799799308e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.110325143206865e-07\n",
      "  batch 101 loss: 8.703267508280987e-05\n",
      "  batch 201 loss: 0.000657396916867583\n",
      "  batch 301 loss: 0.003991453885100782\n",
      "  batch 401 loss: 0.006637898945482448\n",
      "LOSS train 0.002889140015800511 valid 0.0008639379520900548\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.4947132440283894e-05\n",
      "  batch 101 loss: 0.003256719936034642\n",
      "  batch 201 loss: 0.027593696701806038\n",
      "  batch 301 loss: 0.026318794381804764\n",
      "  batch 401 loss: 2.7993328674603255\n",
      "LOSS train 2.948611943317818 valid 22.77336311340332\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.25153303146362305\n",
      "  batch 101 loss: 9.0826484310627\n",
      "  batch 201 loss: 6.156308379769325\n",
      "  batch 301 loss: 0.4140735674649477\n",
      "  batch 401 loss: 0.21196182852610945\n",
      "LOSS train 3.664898536271052 valid 2.851292133331299\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.033264541625976564\n",
      "  batch 101 loss: 1.6747027754038573\n",
      "  batch 201 loss: 0.20955381168052553\n",
      "  batch 301 loss: 0.021038510760990902\n",
      "  batch 401 loss: 0.003255513828189578\n",
      "LOSS train 0.43853282169270713 valid 0.0008060428663156927\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.5920322621241212e-05\n",
      "  batch 101 loss: 0.001575074696156662\n",
      "  batch 201 loss: 0.0006971448704280192\n",
      "  batch 301 loss: 0.0015075209335191175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.07379503490403294\n",
      "LOSS train 0.20940905929551026 valid 4.334351539611816\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.06772991180419922\n",
      "  batch 101 loss: 11.240366473197938\n",
      "  batch 201 loss: 6.420664654970169\n",
      "  batch 301 loss: 0.22361682588234544\n",
      "  batch 401 loss: 0.023789995990227908\n",
      "LOSS train 4.058587961668584 valid 0.002096942625939846\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001601489447057247\n",
      "  batch 101 loss: 122.91590033881366\n",
      "  batch 201 loss: 0.029890687958686612\n",
      "  batch 301 loss: 0.00021484700424366564\n",
      "  batch 401 loss: 6.846295782452217e-05\n",
      "LOSS train 27.7531073749635 valid 6.311695324257016e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.1685238102218134e-07\n",
      "  batch 101 loss: 7.144899271224858e-05\n",
      "  batch 201 loss: 7.284623963641934e-05\n",
      "  batch 301 loss: 7.41650305371877e-05\n",
      "  batch 401 loss: 7.62451632272132e-05\n",
      "LOSS train 7.412312043549634e-05 valid 6.67980857542716e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.592808884102851e-07\n",
      "  batch 101 loss: 7.865686093737167e-05\n",
      "  batch 201 loss: 8.018538568649092e-05\n",
      "  batch 301 loss: 8.175868521448137e-05\n",
      "  batch 401 loss: 8.423357706760726e-05\n",
      "LOSS train 8.141403649158902e-05 valid 7.516060577472672e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.850021938793362e-07\n",
      "  batch 101 loss: 8.717952554889053e-05\n",
      "  batch 201 loss: 8.9015091211877e-05\n",
      "  batch 301 loss: 9.093943195466636e-05\n",
      "  batch 401 loss: 9.379037327335027e-05\n",
      "LOSS train 9.000648613654701e-05 valid 9.367775055579841e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1712339619407431e-06\n",
      "  batch 101 loss: 9.713471283248509e-05\n",
      "  batch 201 loss: 9.92638112052191e-05\n",
      "  batch 301 loss: 0.00010119102714270411\n",
      "  batch 401 loss: 0.00010382075460711348\n",
      "LOSS train 9.936003076520885e-05 valid 0.00013094337191432714\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.8143627676181495e-06\n",
      "  batch 101 loss: 0.00010654623272216668\n",
      "  batch 201 loss: 0.00010838671646467901\n",
      "  batch 301 loss: 0.00010885148696218038\n",
      "  batch 401 loss: 0.00010946556280060803\n",
      "LOSS train 0.00010625557065426492 valid 0.00018641893984749913\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.658083976712078e-06\n",
      "  batch 101 loss: 0.00010942887845857285\n",
      "  batch 201 loss: 0.00010927094475732702\n",
      "  batch 301 loss: 0.00010600266838423523\n",
      "  batch 401 loss: 0.00010272770224105443\n",
      "LOSS train 0.00010410697662240294 valid 0.00022297611576505005\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.179190680384636e-06\n",
      "  batch 101 loss: 9.90676490715714e-05\n",
      "  batch 201 loss: 9.605981411965558e-05\n",
      "  batch 301 loss: 8.97087295743404e-05\n",
      "  batch 401 loss: 8.448513903260845e-05\n",
      "LOSS train 8.993653573323031e-05 valid 0.00020737474551424384\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.191487466916442e-05\n",
      "  batch 101 loss: 60.557171585764735\n",
      "  batch 201 loss: 0.003398262266819074\n",
      "  batch 301 loss: 7.327526730477985e-05\n",
      "  batch 401 loss: 7.339381054407567e-05\n",
      "LOSS train 13.670602935226201 valid 6.546711665578187e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.142960435478017e-07\n",
      "  batch 101 loss: 7.680531834466819e-05\n",
      "  batch 201 loss: 7.90545004656451e-05\n",
      "  batch 301 loss: 8.128641828079708e-05\n",
      "  batch 401 loss: 8.439881058166066e-05\n",
      "LOSS train 8.06825890970975e-05 valid 7.624982390552759e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.106043242150918e-07\n",
      "  batch 101 loss: 8.813321632942461e-05\n",
      "  batch 201 loss: 9.056751394382446e-05\n",
      "  batch 301 loss: 9.304349674039259e-05\n",
      "  batch 401 loss: 9.638478123065397e-05\n",
      "LOSS train 9.170654895865534e-05 valid 0.00010222936543868855\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3288765330798924e-06\n",
      "  batch 101 loss: 0.00010018495141821404\n",
      "  batch 201 loss: 0.0001026834895446882\n",
      "  batch 301 loss: 0.00010463101886898585\n",
      "  batch 401 loss: 0.00010699318306478745\n",
      "LOSS train 0.00010224705644703579 valid 0.0001528568536741659\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.1579748135991394e-06\n",
      "  batch 101 loss: 0.0001090191878290625\n",
      "  batch 201 loss: 0.00011033241672691929\n",
      "  batch 301 loss: 0.00010934504354707997\n",
      "  batch 401 loss: 0.00010814392581920629\n",
      "LOSS train 0.00010669622954458216 valid 0.00021034048404544592\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.001284203492105e-06\n",
      "  batch 101 loss: 0.00010602020497628928\n",
      "  batch 201 loss: 0.00010416066815878366\n",
      "  batch 301 loss: 9.871960597536144e-05\n",
      "  batch 401 loss: 9.372130573723325e-05\n",
      "LOSS train 9.793836513031607e-05 valid 0.0002199392329202965\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.136622253805399e-06\n",
      "  batch 101 loss: 8.911343076078992e-05\n",
      "  batch 201 loss: 8.529617607791807e-05\n",
      "  batch 301 loss: 7.878153314891278e-05\n",
      "  batch 401 loss: 7.401262000485076e-05\n",
      "LOSS train 7.987694613852159e-05 valid 0.00018795303185470402\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.6803798391483722e-06\n",
      "  batch 101 loss: 7.128581853066863e-05\n",
      "  batch 201 loss: 6.829594305031606e-05\n",
      "  batch 301 loss: 6.382168250297582e-05\n",
      "  batch 401 loss: 6.133213468274334e-05\n",
      "LOSS train 6.511080145179923e-05 valid 0.0001629548060009256\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.615369908511639e-05\n",
      "  batch 101 loss: 42.14561435827986\n",
      "  batch 201 loss: 0.0020164253882273895\n",
      "  batch 301 loss: 7.338107817304262e-05\n",
      "  batch 401 loss: 7.64916449361408e-05\n",
      "LOSS train 9.51419872093275 valid 6.816208042437211e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.014292739564553e-07\n",
      "  batch 101 loss: 8.083419165359373e-05\n",
      "  batch 201 loss: 8.373943993774447e-05\n",
      "  batch 301 loss: 8.663982090183709e-05\n",
      "  batch 401 loss: 9.042555635460303e-05\n",
      "LOSS train 8.550552575935618e-05 valid 8.787564001977444e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.058624402503483e-06\n",
      "  batch 101 loss: 9.487116342825175e-05\n",
      "  batch 201 loss: 9.781123023458349e-05\n",
      "  batch 301 loss: 0.00010051182293182137\n",
      "  batch 401 loss: 0.00010376752564297931\n",
      "LOSS train 9.831546011528867e-05 valid 0.00013327892520464957\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.8518418073654176e-06\n",
      "  batch 101 loss: 0.00010694061607978256\n",
      "  batch 201 loss: 0.00010901819576247362\n",
      "  batch 301 loss: 0.00010930250884030102\n",
      "  batch 401 loss: 0.00010935185466934172\n",
      "LOSS train 0.00010639500951855641 valid 0.00019796563719864935\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.82488064840436e-06\n",
      "  batch 101 loss: 0.00010831655507161031\n",
      "  batch 201 loss: 0.00010721670761654423\n",
      "  batch 301 loss: 0.00010255143378913089\n",
      "  batch 401 loss: 9.79531847326598e-05\n",
      "LOSS train 0.00010119510682610628 valid 0.00022319034906104207\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.1821889569982884e-06\n",
      "  batch 101 loss: 9.325523980521666e-05\n",
      "  batch 201 loss: 8.940549829333121e-05\n",
      "  batch 301 loss: 8.259356133862639e-05\n",
      "  batch 401 loss: 7.737964626358006e-05\n",
      "LOSS train 8.35345017347662e-05 valid 0.00019430754764471203\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.772281295619905e-06\n",
      "  batch 101 loss: 7.398674599357946e-05\n",
      "  batch 201 loss: 7.066840438255894e-05\n",
      "  batch 301 loss: 6.572120134251235e-05\n",
      "  batch 401 loss: 6.279433117470034e-05\n",
      "LOSS train 6.708782285241666e-05 valid 0.00016563416284043342\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.351453440496698e-06\n",
      "  batch 101 loss: 6.266269806459946e-05\n",
      "  batch 201 loss: 6.094727469303507e-05\n",
      "  batch 301 loss: 5.829446356187873e-05\n",
      "  batch 401 loss: 5.757094251123362e-05\n",
      "LOSS train 5.9326569305918564e-05 valid 0.00015707741840742528\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00011055529117584228\n",
      "  batch 101 loss: 68.46981835568323\n",
      "  batch 201 loss: 0.0032388297712986967\n",
      "  batch 301 loss: 6.761535928490048e-05\n",
      "  batch 401 loss: 7.307140896955388e-05\n",
      "LOSS train 15.456736091743073 valid 6.521695468109101e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.052652340964414e-07\n",
      "  batch 101 loss: 7.635132622908713e-05\n",
      "  batch 201 loss: 7.852650328459277e-05\n",
      "  batch 301 loss: 8.068080962857494e-05\n",
      "  batch 401 loss: 8.370939143787836e-05\n",
      "LOSS train 8.013088230116935e-05 valid 7.520002691308036e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.859384641051292e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 8.734336464385705e-05\n",
      "  batch 201 loss: 8.97075071134168e-05\n",
      "  batch 301 loss: 9.212569968212847e-05\n",
      "  batch 401 loss: 9.542774925193954e-05\n",
      "LOSS train 9.088317102944265e-05 valid 9.924911864800379e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.274885726161301e-06\n",
      "  batch 101 loss: 9.9220733503671e-05\n",
      "  batch 201 loss: 0.00010170520451083576\n",
      "  batch 301 loss: 0.00010374230911224913\n",
      "  batch 401 loss: 0.00010626363201595268\n",
      "LOSS train 0.00010146540649645963 valid 0.00014720206672791392\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.070884365821257e-06\n",
      "  batch 101 loss: 0.00010855318629808152\n",
      "  batch 201 loss: 0.00011006351487026222\n",
      "  batch 301 loss: 0.00010947965779649849\n",
      "  batch 401 loss: 0.00010872802438484541\n",
      "LOSS train 0.00010678029790173371 valid 0.0002059393300442025\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.9388052644208074e-06\n",
      "  batch 101 loss: 0.00010705558137431127\n",
      "  batch 201 loss: 0.00010555191595017277\n",
      "  batch 301 loss: 0.00010049850192444864\n",
      "  batch 401 loss: 9.574927978405867e-05\n",
      "LOSS train 9.946056723161237e-05 valid 0.0002217929722974077\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.1626189593225716e-06\n",
      "  batch 101 loss: 9.119532433544463e-05\n",
      "  batch 201 loss: 8.744158982722183e-05\n",
      "  batch 301 loss: 8.084291069735627e-05\n",
      "  batch 401 loss: 7.589762649331532e-05\n",
      "LOSS train 8.182138152010719e-05 valid 0.00019164332479704171\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.733834262471646e-06\n",
      "  batch 101 loss: 7.286229517831089e-05\n",
      "  batch 201 loss: 6.972428136123199e-05\n",
      "  batch 301 loss: 6.499740098092843e-05\n",
      "  batch 401 loss: 6.225927009381849e-05\n",
      "LOSS train 6.630862836315559e-05 valid 0.00016468203102704138\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005434801429510116\n",
      "  batch 101 loss: 8.026435345631507\n",
      "  batch 201 loss: 7.95429491716959e-05\n",
      "  batch 301 loss: 9.16399623702091e-05\n",
      "  batch 401 loss: 0.00010030140687376843\n",
      "LOSS train 1.8120287948172646 valid 0.00013298416160978377\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8471252406015992e-06\n",
      "  batch 101 loss: 0.00010710841294269358\n",
      "  batch 201 loss: 0.00011015655800008516\n",
      "  batch 301 loss: 0.00010854191455621276\n",
      "  batch 401 loss: 0.00010440767151635555\n",
      "LOSS train 0.00010456714729911395 valid 0.00022421720495913178\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.196554316673428e-06\n",
      "  batch 101 loss: 9.767872791940136e-05\n",
      "  batch 201 loss: 9.192957386289891e-05\n",
      "  batch 301 loss: 8.31379416757727e-05\n",
      "  batch 401 loss: 7.635148616031983e-05\n",
      "LOSS train 8.490607072640088e-05 valid 0.0001903799275169149\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7155599673278628e-06\n",
      "  batch 101 loss: 7.179899584571103e-05\n",
      "  batch 201 loss: 6.793271124820421e-05\n",
      "  batch 301 loss: 6.296855133996359e-05\n",
      "  batch 401 loss: 6.0324554279418405e-05\n",
      "LOSS train 6.469777664323976e-05 valid 0.0001606835867278278\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2769968200009316e-06\n",
      "  batch 101 loss: 6.0919149484561784e-05\n",
      "  batch 201 loss: 5.962847403651494e-05\n",
      "  batch 301 loss: 5.7621914452283816e-05\n",
      "  batch 401 loss: 5.763340430860353e-05\n",
      "LOSS train 5.865032602533722e-05 valid 0.00015651599096599966\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.213826955994591e-06\n",
      "  batch 101 loss: 6.089329054077552e-05\n",
      "  batch 201 loss: 6.049592943782045e-05\n",
      "  batch 301 loss: 5.928880746722598e-05\n",
      "  batch 401 loss: 5.980752757693608e-05\n",
      "LOSS train 6.021160976009745e-05 valid 0.00013228229363448918\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.835877337725833e-06\n",
      "  batch 101 loss: 5.938069277192426e-05\n",
      "  batch 201 loss: 5.740726293765874e-05\n",
      "  batch 301 loss: 5.494570197015491e-05\n",
      "  batch 401 loss: 5.3509322594891276e-05\n",
      "LOSS train 5.611997652187247e-05 valid 8.448764856439084e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.900443546939641e-07\n",
      "  batch 101 loss: 4.887820361204831e-05\n",
      "  batch 201 loss: 4.748705090491967e-05\n",
      "  batch 301 loss: 4.6337375250118387e-05\n",
      "  batch 401 loss: 4.620162368894398e-05\n",
      "LOSS train 4.716911654140735e-05 valid 7.269659545272589e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002165818810462952\n",
      "  batch 101 loss: 0.0064232301143010775\n",
      "  batch 201 loss: 3.706876168337203e-05\n",
      "  batch 301 loss: 4.614650141149923e-05\n",
      "  batch 401 loss: 6.047062377376733e-05\n",
      "LOSS train 0.0019764858681034646 valid 7.046059181448072e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.664484681095928e-07\n",
      "  batch 101 loss: 5.015524791645021e-05\n",
      "  batch 201 loss: 5.7587406860193366e-05\n",
      "  batch 301 loss: 5.3425150070722794e-05\n",
      "  batch 401 loss: 4.4494328906239387e-05\n",
      "LOSS train 5.1040748402672855e-05 valid 6.52684029773809e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.071390478406102e-07\n",
      "  batch 101 loss: 4.4110756496138494e-05\n",
      "  batch 201 loss: 4.5076847228529005e-05\n",
      "  batch 301 loss: 4.990270974531086e-05\n",
      "  batch 401 loss: 6.149306816041644e-05\n",
      "LOSS train 5.237718592460527e-05 valid 6.599506014026701e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0443676728755236e-07\n",
      "  batch 101 loss: 6.170108381269302e-05\n",
      "  batch 201 loss: 6.931515997294469e-05\n",
      "  batch 301 loss: 9.330411666041982e-05\n",
      "  batch 401 loss: 0.00010057954349164789\n",
      "LOSS train 8.234615925992968e-05 valid 0.00010504535748623312\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.379090390400961e-06\n",
      "  batch 101 loss: 8.43115367433711e-05\n",
      "  batch 201 loss: 9.105618207968292e-05\n",
      "  batch 301 loss: 0.00011467192590998821\n",
      "  batch 401 loss: 0.0001232486791261067\n",
      "LOSS train 0.00010393376528835663 valid 9.67258310993202e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.333869815629441e-08\n",
      "  batch 101 loss: 0.00011627949193325549\n",
      "  batch 201 loss: 0.00013025042909362128\n",
      "  batch 301 loss: 0.00014915258863936742\n",
      "  batch 401 loss: 0.00014301773311217402\n",
      "LOSS train 0.00013256731318733728 valid 0.00022561709920410067\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.13462834735401e-07\n",
      "  batch 101 loss: 9.178141380516535e-05\n",
      "  batch 201 loss: 9.31490763025522e-05\n",
      "  batch 301 loss: 8.935901253238398e-05\n",
      "  batch 401 loss: 7.96220118598967e-05\n",
      "LOSS train 8.664670622988831e-05 valid 0.00015493243699893355\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2618200091528705e-07\n",
      "  batch 101 loss: 6.455345195149675e-05\n",
      "  batch 201 loss: 6.461625005101724e-05\n",
      "  batch 301 loss: 6.112758084100278e-05\n",
      "  batch 401 loss: 5.532784534636903e-05\n",
      "LOSS train 6.16581389444879e-05 valid 7.059497147565708e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.6488816365599634e-05\n",
      "  batch 101 loss: 13.407107502071103\n",
      "  batch 201 loss: 0.00010300179822138489\n",
      "  batch 301 loss: 6.061389539922857e-05\n",
      "  batch 401 loss: 5.6890844555823604e-05\n",
      "LOSS train 3.0264957668169292 valid 0.00015378691023215652\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.1722067322116346e-06\n",
      "  batch 101 loss: 6.103282140941246e-05\n",
      "  batch 201 loss: 5.974546772392841e-05\n",
      "  batch 301 loss: 5.666835500562684e-05\n",
      "  batch 401 loss: 5.39556343210279e-05\n",
      "LOSS train 5.7523598830534074e-05 valid 8.104831795208156e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.176888124784455e-07\n",
      "  batch 101 loss: 4.7290454457993294e-05\n",
      "  batch 201 loss: 4.563752192694892e-05\n",
      "  batch 301 loss: 4.51299883090428e-05\n",
      "  batch 401 loss: 4.6173421236517245e-05\n",
      "LOSS train 4.614843566054657e-05 valid 7.088648271746933e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.778712850064039e-07\n",
      "  batch 101 loss: 4.58151345060287e-05\n",
      "  batch 201 loss: 4.7572673806115515e-05\n",
      "  batch 301 loss: 4.91328571301608e-05\n",
      "  batch 401 loss: 4.8330068570408005e-05\n",
      "LOSS train 4.776716526368903e-05 valid 6.252007733564824e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.838985554059036e-07\n",
      "  batch 101 loss: 4.196550127886667e-05\n",
      "  batch 201 loss: 4.132406342932882e-05\n",
      "  batch 301 loss: 3.932381331253509e-05\n",
      "  batch 401 loss: 3.7341260887160386e-05\n",
      "LOSS train 4.008957680098494e-05 valid 6.485547055490315e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.225386131409323e-07\n",
      "  batch 101 loss: 3.57827436235425e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 3.566321330168876e-05\n",
      "  batch 301 loss: 3.613944493281451e-05\n",
      "  batch 401 loss: 3.648417652854619e-05\n",
      "LOSS train 3.6516443478320514e-05 valid 6.24066378804855e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.7656514905393124e-07\n",
      "  batch 101 loss: 3.820214422148638e-05\n",
      "  batch 201 loss: 3.883090947454093e-05\n",
      "  batch 301 loss: 4.1872285663089315e-05\n",
      "  batch 401 loss: 4.4896124068714014e-05\n",
      "LOSS train 4.171501121356843e-05 valid 9.386680176248774e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1748165707103908e-06\n",
      "  batch 101 loss: 5.3300139556569095e-05\n",
      "  batch 201 loss: 5.6629206519147604e-05\n",
      "  batch 301 loss: 6.379399200298508e-05\n",
      "  batch 401 loss: 6.274189246084916e-05\n",
      "LOSS train 5.9137742772778125e-05 valid 8.836795313982293e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.392085321247578e-05\n",
      "  batch 101 loss: 0.016120001056010552\n",
      "  batch 201 loss: 3.9950614763739625e-05\n",
      "  batch 301 loss: 4.021743361590779e-05\n",
      "  batch 401 loss: 5.2638617720788264e-05\n",
      "LOSS train 0.0036933427973924703 valid 0.00011102729331469163\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.4835016918368638e-06\n",
      "  batch 101 loss: 5.6141928470765376e-05\n",
      "  batch 201 loss: 5.281766264829457e-05\n",
      "  batch 301 loss: 5.64564075295948e-05\n",
      "  batch 401 loss: 5.563284895686138e-05\n",
      "LOSS train 5.517562334656221e-05 valid 6.47981942165643e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.2357437299215234e-07\n",
      "  batch 101 loss: 4.1660785601038694e-05\n",
      "  batch 201 loss: 4.343953156279667e-05\n",
      "  batch 301 loss: 4.511085340595855e-05\n",
      "  batch 401 loss: 4.840381590724974e-05\n",
      "LOSS train 4.5363608209755115e-05 valid 6.331843178486452e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.2680141632445156e-07\n",
      "  batch 101 loss: 5.736492615142197e-05\n",
      "  batch 201 loss: 6.472031859743765e-05\n",
      "  batch 301 loss: 6.751584556695889e-05\n",
      "  batch 401 loss: 7.096013931487733e-05\n",
      "LOSS train 6.70255519360916e-05 valid 0.00010183663835050538\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.321814052062109e-06\n",
      "  batch 101 loss: 8.800188166617317e-05\n",
      "  batch 201 loss: 0.00010061158777602942\n",
      "  batch 301 loss: 0.00010938249226569497\n",
      "  batch 401 loss: 9.671849537539856e-05\n",
      "LOSS train 9.73218851898835e-05 valid 6.173712608870119e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.3237105779116973e-07\n",
      "  batch 101 loss: 0.00010079614529871605\n",
      "  batch 201 loss: 0.00011772175578187216\n",
      "  batch 301 loss: 0.0001494124254139706\n",
      "  batch 401 loss: 0.00014856944859616304\n",
      "LOSS train 0.00012819044102159978 valid 0.00016697391401976347\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.852160832844675e-07\n",
      "  batch 101 loss: 0.00010966431198824012\n",
      "  batch 201 loss: 0.00011997101398492305\n",
      "  batch 301 loss: 0.00012593953658267764\n",
      "  batch 401 loss: 0.00011055203066234753\n",
      "LOSS train 0.00011349845060362947 valid 0.00019702219287864864\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.4634325604420153e-07\n",
      "  batch 101 loss: 7.205420073105984e-05\n",
      "  batch 201 loss: 6.916983209634964e-05\n",
      "  batch 301 loss: 7.488650413847608e-05\n",
      "  batch 401 loss: 7.617269145640648e-05\n",
      "LOSS train 7.277258996072861e-05 valid 8.730914123589173e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.493475917726756e-05\n",
      "  batch 101 loss: 48303.409250445984\n",
      "  batch 201 loss: 0.6344461968541145\n",
      "  batch 301 loss: 0.37478800415992736\n",
      "  batch 401 loss: 0.284763253480196\n",
      "LOSS train 10904.017608476774 valid 0.2101796418428421\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0021480271220207215\n",
      "  batch 101 loss: 0.17591163218021394\n",
      "  batch 201 loss: 0.11826807975769044\n",
      "  batch 301 loss: 0.07581407546997071\n",
      "  batch 401 loss: 0.04631644390523434\n",
      "LOSS train 0.09740430987429538 valid 0.027804261073470116\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0002946232631802559\n",
      "  batch 101 loss: 0.0211214413959533\n",
      "  batch 201 loss: 0.011479947888292373\n",
      "  batch 301 loss: 0.005952881507109851\n",
      "  batch 401 loss: 0.002943807658739388\n",
      "LOSS train 0.009588622823662418 valid 0.0014418908394873142\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.7852124292403458e-05\n",
      "  batch 101 loss: 0.001021376788703492\n",
      "  batch 201 loss: 0.0004742992657702416\n",
      "  batch 301 loss: 0.00023048507206112844\n",
      "  batch 401 loss: 0.0001270586200917023\n",
      "LOSS train 0.000430696333749204 valid 8.529727347195148e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.0066534741781652e-06\n",
      "  batch 101 loss: 7.950952061946736e-05\n",
      "  batch 201 loss: 6.85888078987773e-05\n",
      "  batch 301 loss: 6.453597326981253e-05\n",
      "  batch 401 loss: 6.350812431264785e-05\n",
      "LOSS train 6.900508875378056e-05 valid 6.181207572808489e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.2847165130078795e-07\n",
      "  batch 101 loss: 6.307652539362607e-05\n",
      "  batch 201 loss: 6.306868950559873e-05\n",
      "  batch 301 loss: 6.282853549237188e-05\n",
      "  batch 401 loss: 6.322987893327081e-05\n",
      "LOSS train 6.351796907333552e-05 valid 6.162581848911941e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.023353929165751e-07\n",
      "  batch 101 loss: 6.325721631583292e-05\n",
      "  batch 201 loss: 6.345264065203082e-05\n",
      "  batch 301 loss: 6.330936417725752e-05\n",
      "  batch 401 loss: 6.377060735758277e-05\n",
      "LOSS train 6.391688551522243e-05 valid 6.166534876683727e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.0935836548451334e-07\n",
      "  batch 101 loss: 6.386742610629881e-05\n",
      "  batch 201 loss: 6.40758652752993e-05\n",
      "  batch 301 loss: 6.397189467861608e-05\n",
      "  batch 401 loss: 6.447335144002864e-05\n",
      "LOSS train 6.456232067669201e-05 valid 6.17366677033715e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002347835898399353\n",
      "  batch 101 loss: 79242.24063087403\n",
      "  batch 201 loss: 303.7475290173292\n",
      "  batch 301 loss: 0.4270165675878525\n",
      "  batch 401 loss: 0.3279547706246376\n",
      "LOSS train 17962.911484402473 valid 0.2530883550643921\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0025816476345062256\n",
      "  batch 101 loss: 27807.801759380294\n",
      "  batch 201 loss: 0.03373035443946719\n",
      "  batch 301 loss: 0.02376756997779012\n",
      "  batch 401 loss: 0.01612815327011049\n",
      "LOSS train 6277.174508319653 valid 0.010838020592927933\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.749211370944977e-05\n",
      "  batch 101 loss: 0.008646913673728704\n",
      "  batch 201 loss: 0.00536016974132508\n",
      "  batch 301 loss: 0.00319802139303647\n",
      "  batch 401 loss: 0.0018466278578853235\n",
      "LOSS train 0.00444515331088025 valid 0.001070193713530898\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.133936742320657e-06\n",
      "  batch 101 loss: 0.0007892515178536996\n",
      "  batch 201 loss: 0.0004334188246866688\n",
      "  batch 301 loss: 0.00024133548215104383\n",
      "  batch 401 loss: 0.0001452493811120803\n",
      "LOSS train 0.0003769822338163005 valid 0.00010005923104472458\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.876710141208605e-08\n",
      "  batch 101 loss: 8.446009788301012e-05\n",
      "  batch 201 loss: 7.097265722904922e-05\n",
      "  batch 301 loss: 6.496062654605339e-05\n",
      "  batch 401 loss: 6.3337160172523e-05\n",
      "LOSS train 7.06951309491607e-05 valid 6.189820123836398e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.178895374527201e-07\n",
      "  batch 101 loss: 6.21255895566719e-05\n",
      "  batch 201 loss: 6.240332304514595e-05\n",
      "  batch 301 loss: 6.226931935088942e-05\n",
      "  batch 401 loss: 6.273460966440326e-05\n",
      "LOSS train 6.286312122476465e-05 valid 6.157799361972138e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.90517073153751e-07\n",
      "  batch 101 loss: 6.272937016547076e-05\n",
      "  batch 201 loss: 6.294596431871468e-05\n",
      "  batch 301 loss: 6.27830064149748e-05\n",
      "  batch 401 loss: 6.321721177300788e-05\n",
      "LOSS train 6.339185760180351e-05 valid 6.162087083794177e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.013361128978431e-07\n",
      "  batch 101 loss: 6.325033386019641e-05\n",
      "  batch 201 loss: 6.344950817037898e-05\n",
      "  batch 301 loss: 6.330547078505333e-05\n",
      "  batch 401 loss: 6.376453158736695e-05\n",
      "LOSS train 6.391155543486742e-05 valid 6.166457751533017e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005114623531699181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 37188.069709569514\n",
      "  batch 201 loss: 0.6198874482512474\n",
      "  batch 301 loss: 0.4860705474019051\n",
      "  batch 401 loss: 0.36802402153611186\n",
      "LOSS train 8394.957794015205 valid 0.2585938572883606\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0027778786420822143\n",
      "  batch 101 loss: 0.2177383990585804\n",
      "  batch 201 loss: 0.14051441498100758\n",
      "  batch 301 loss: 0.08781557157635689\n",
      "  batch 401 loss: 0.05113318188115954\n",
      "LOSS train 0.11631600902274958 valid 0.028462989255785942\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0002449151873588562\n",
      "  batch 101 loss: 0.022525840867310763\n",
      "  batch 201 loss: 0.012192716021090746\n",
      "  batch 301 loss: 0.007142648780718446\n",
      "  batch 401 loss: 0.003831517754588276\n",
      "LOSS train 0.010603119710246921 valid 0.0009944505291059613\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.962392522022128e-05\n",
      "  batch 101 loss: 0.001587142798525747\n",
      "  batch 201 loss: 0.0008926457300185575\n",
      "  batch 301 loss: 0.00068948512107454\n",
      "  batch 401 loss: 0.00046145867856466793\n",
      "LOSS train 0.0008688097693079346 valid 6.170170672703534e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.533747655339539e-06\n",
      "  batch 101 loss: 0.0003057514013562468\n",
      "  batch 201 loss: 0.00023091717433999293\n",
      "  batch 301 loss: 0.00018196715329395373\n",
      "  batch 401 loss: 0.00014465912860032405\n",
      "LOSS train 0.00021051036466207578 valid 6.408521585399285e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.711070015328004e-07\n",
      "  batch 101 loss: 0.00012267413123481674\n",
      "  batch 201 loss: 0.00010371259164458024\n",
      "  batch 301 loss: 9.907075143928524e-05\n",
      "  batch 401 loss: 8.798494843176741e-05\n",
      "LOSS train 0.0001024981456506769 valid 6.28669949946925e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.9119744845666e-07\n",
      "  batch 101 loss: 8.233950511566945e-05\n",
      "  batch 201 loss: 8.124077043248689e-05\n",
      "  batch 301 loss: 7.817725145287113e-05\n",
      "  batch 401 loss: 7.607979656313545e-05\n",
      "LOSS train 7.960465385782366e-05 valid 6.238863716134802e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.007379902759567e-07\n",
      "  batch 101 loss: 7.723105586592283e-05\n",
      "  batch 201 loss: 7.336405285968795e-05\n",
      "  batch 301 loss: 7.195860845968127e-05\n",
      "  batch 401 loss: 6.766347389202564e-05\n",
      "LOSS train 7.301721123329211e-05 valid 6.203362863743678e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00035130932927131655\n",
      "  batch 101 loss: 210586.80470930517\n",
      "  batch 201 loss: 0.9640762770175934\n",
      "  batch 301 loss: 0.5421175718307495\n",
      "  batch 401 loss: 0.344947372674942\n",
      "LOSS train 47536.96844233744 valid 0.10959917306900024\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0022665272653102876\n",
      "  batch 101 loss: 0.25823913276195526\n",
      "  batch 201 loss: 0.24130600363016128\n",
      "  batch 301 loss: 0.22207324996590613\n",
      "  batch 401 loss: 0.1974863811582327\n",
      "LOSS train 0.22468754889666362 valid 0.07052795588970184\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0012355659902095794\n",
      "  batch 101 loss: 0.17057625718414784\n",
      "  batch 201 loss: 0.1508751232177019\n",
      "  batch 301 loss: 0.12979364421218634\n",
      "  batch 401 loss: 0.11863994188606738\n",
      "LOSS train 0.13880016737981252 valid 0.0414385087788105\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.0006753810495138168\n",
      "  batch 101 loss: 0.09200680505484343\n",
      "  batch 201 loss: 0.07779412794858218\n",
      "  batch 301 loss: 0.06538177669048309\n",
      "  batch 401 loss: 0.05327970817685127\n",
      "LOSS train 0.06967388647219531 valid 0.01684221439063549\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.0003925801068544388\n",
      "  batch 101 loss: 0.03869285101071\n",
      "  batch 201 loss: 0.03051396129652858\n",
      "  batch 301 loss: 0.024625448016449808\n",
      "  batch 401 loss: 0.01983122537843883\n",
      "LOSS train 0.027313993827483993 valid 0.004986141342669725\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.00022435400635004043\n",
      "  batch 101 loss: 0.01344641902949661\n",
      "  batch 201 loss: 0.010387041443027555\n",
      "  batch 301 loss: 0.007329027876257896\n",
      "  batch 401 loss: 0.0053590855514630675\n",
      "LOSS train 0.00870273014295081 valid 0.0012002073926851153\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.857321571558714e-05\n",
      "  batch 101 loss: 0.00327713092090562\n",
      "  batch 201 loss: 0.002440353267593309\n",
      "  batch 301 loss: 0.0016514202859252692\n",
      "  batch 401 loss: 0.0010844158713007345\n",
      "LOSS train 0.0019969013216064374 valid 0.0001365411008009687\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.067406597547233e-06\n",
      "  batch 101 loss: 0.0009743646621063817\n",
      "  batch 201 loss: 0.0004132887508603744\n",
      "  batch 301 loss: 0.00026522333864704707\n",
      "  batch 401 loss: 0.00018998728479346027\n",
      "LOSS train 0.00043212596513268525 valid 8.002171671250835e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00010885617695748805\n",
      "  batch 101 loss: 0.1602080866394681\n",
      "  batch 201 loss: 0.000212336231916197\n",
      "  batch 301 loss: 0.0002958085669342836\n",
      "  batch 401 loss: 0.0007669401946122889\n",
      "LOSS train 0.03648735248332281 valid 0.0008736745221540332\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0864419164136052e-06\n",
      "  batch 101 loss: 7.840929369251626e-05\n",
      "  batch 201 loss: 0.00014750749369568439\n",
      "  batch 301 loss: 0.000273377842586342\n",
      "  batch 401 loss: 0.0009565508834552361\n",
      "LOSS train 0.0003628353851110735 valid 0.0012255848851054907\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.362701242323965e-07\n",
      "  batch 101 loss: 0.00011027596789972449\n",
      "  batch 201 loss: 0.00015814278163816197\n",
      "  batch 301 loss: 0.0005402821267443869\n",
      "  batch 401 loss: 0.0032381625101334066\n",
      "LOSS train 0.0012178326993732543 valid 0.006780023220926523\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.380043156445026e-05\n",
      "  batch 101 loss: 0.0016515930801870127\n",
      "  batch 201 loss: 0.00027450820129161003\n",
      "  batch 301 loss: 0.00035475178878186854\n",
      "  batch 401 loss: 0.0028732176060202617\n",
      "LOSS train 0.0012767232778938833 valid 0.0008410135051235557\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.989189958199858e-08\n",
      "  batch 101 loss: 0.0004920360600067398\n",
      "  batch 201 loss: 0.0003782677816298019\n",
      "  batch 301 loss: 0.0017547229781666829\n",
      "  batch 401 loss: 0.017561373408407233\n",
      "LOSS train 0.005430384473554871 valid 0.008599551394581795\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.930094979703427e-05\n",
      "  batch 101 loss: 0.0015847339611354983\n",
      "  batch 201 loss: 0.00015595291358295072\n",
      "  batch 301 loss: 0.0001391547687626371\n",
      "  batch 401 loss: 0.00021986397346722696\n",
      "LOSS train 0.0006734769963046166 valid 0.004190658684819937\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.970334358513355e-05\n",
      "  batch 101 loss: 0.008464451261897921\n",
      "  batch 201 loss: 0.006123479041016253\n",
      "  batch 301 loss: 0.004365379027003655\n",
      "  batch 401 loss: 0.004999330065675167\n",
      "LOSS train 0.005543906231094535 valid 0.003013355191797018\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.3822726216167212e-05\n",
      "  batch 101 loss: 0.0005134856887400474\n",
      "  batch 201 loss: 0.0005128578061066947\n",
      "  batch 301 loss: 0.003715342867471918\n",
      "  batch 401 loss: 0.00702697715463728\n",
      "LOSS train 0.003377966794080966 valid 0.005365496966987848\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.7245139461010694e-05\n",
      "  batch 101 loss: 0.003678625496359018\n",
      "  batch 201 loss: 0.00235169634444901\n",
      "  batch 301 loss: 0.0018306642675452167\n",
      "  batch 401 loss: 0.004104694628986181\n",
      "LOSS train 0.003105283625154287 valid 0.0021251158323138952\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 6.625588284805417e-06\n",
      "  batch 101 loss: 0.008624079787787196\n",
      "  batch 201 loss: 0.0004712569984894799\n",
      "  batch 301 loss: 0.0005289879545580334\n",
      "  batch 401 loss: 0.009097743056590843\n",
      "LOSS train 0.004884685336581419 valid 0.052975501865148544\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.0005739153176546097\n",
      "  batch 101 loss: 0.007361183789525967\n",
      "  batch 201 loss: 0.00017173408721760097\n",
      "  batch 301 loss: 0.0006239797406215075\n",
      "  batch 401 loss: 0.006726067782128666\n",
      "LOSS train 0.004654809547753534 valid 0.021586736664175987\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.00014701229520142078\n",
      "  batch 101 loss: 0.0020651947097030643\n",
      "  batch 201 loss: 6.346872681433523e-05\n",
      "  batch 301 loss: 0.00015587639883278824\n",
      "  batch 401 loss: 0.005941512878443973\n",
      "LOSS train 0.0021775692689701065 valid 0.0023656629491597414\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002989623695611954\n",
      "  batch 101 loss: 0.15579012573696674\n",
      "  batch 201 loss: 0.0034895841928664594\n",
      "  batch 301 loss: 0.001671289429650642\n",
      "  batch 401 loss: 0.0006283392853219993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.03656851605687088 valid 5.784961103927344e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.6861688820645212e-06\n",
      "  batch 101 loss: 0.00020328136437456124\n",
      "  batch 201 loss: 8.92232887508726e-05\n",
      "  batch 301 loss: 3.0103864892225827e-05\n",
      "  batch 401 loss: 1.864751730863645e-05\n",
      "LOSS train 7.943384525377525e-05 valid 5.10499048687052e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.303860573098064e-07\n",
      "  batch 101 loss: 3.407419805853351e-05\n",
      "  batch 201 loss: 2.2990435472820535e-05\n",
      "  batch 301 loss: 3.779279754553499e-05\n",
      "  batch 401 loss: 4.99270293198606e-05\n",
      "LOSS train 5.2866581734666506e-05 valid 8.850127051118761e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.2101059079868717e-07\n",
      "  batch 101 loss: 0.0001559724599974288\n",
      "  batch 201 loss: 5.26177734036537e-05\n",
      "  batch 301 loss: 2.900897738072672e-05\n",
      "  batch 401 loss: 0.0016845808220296021\n",
      "LOSS train 0.0008496406112684017 valid 0.0014301579212769866\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.491159052122384e-06\n",
      "  batch 101 loss: 0.001502961936639622\n",
      "  batch 201 loss: 0.00795775474383845\n",
      "  batch 301 loss: 0.0022849926064372993\n",
      "  batch 401 loss: 0.0009245871166785946\n",
      "LOSS train 0.0038538527275107405 valid 0.049275051802396774\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0004321480542421341\n",
      "  batch 101 loss: 0.012804503373336046\n",
      "  batch 201 loss: 0.0040810495115874805\n",
      "  batch 301 loss: 0.00045001234149822267\n",
      "  batch 401 loss: 0.00014094812703660864\n",
      "LOSS train 0.0040971646107205275 valid 0.003258874174207449\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.5666896253824233e-05\n",
      "  batch 101 loss: 0.0034231559454929086\n",
      "  batch 201 loss: 0.007215905609482434\n",
      "  batch 301 loss: 0.004824238066212274\n",
      "  batch 401 loss: 0.0015540063011576421\n",
      "LOSS train 0.003942894337241697 valid 8.162458834704012e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.874744526110589e-07\n",
      "  batch 101 loss: 0.004323074882486253\n",
      "  batch 201 loss: 0.008114776782458649\n",
      "  batch 301 loss: 0.0014160602097399533\n",
      "  batch 401 loss: 0.00204225813577068\n",
      "LOSS train 0.004318176751852411 valid 0.0040073213167488575\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.498910529538989e-05\n",
      "  batch 101 loss: 0.0032579374773195014\n",
      "  batch 201 loss: 0.0014315992489719064\n",
      "  batch 301 loss: 0.002823860888820491\n",
      "  batch 401 loss: 0.008089960014622193\n",
      "LOSS train 0.0036698020213524313 valid 0.0007116908673197031\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 7.286615436896682e-06\n",
      "  batch 101 loss: 0.0019950446630537045\n",
      "  batch 201 loss: 0.0034585658613650594\n",
      "  batch 301 loss: 0.0034432301917331643\n",
      "  batch 401 loss: 0.004039738358114846\n",
      "LOSS train 0.003776137966117819 valid 0.0017146806931123137\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.304691075347364e-05\n",
      "  batch 101 loss: 0.006876742858148646\n",
      "  batch 201 loss: 0.0037582915162784046\n",
      "  batch 301 loss: 0.0005002355963006266\n",
      "  batch 401 loss: 0.002958849302594899\n",
      "LOSS train 0.003880594791791601 valid 0.006585374008864164\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 5.453896708786487e-05\n",
      "  batch 101 loss: 0.013574326531961561\n",
      "  batch 201 loss: 0.0023013727105717406\n",
      "  batch 301 loss: 0.00029597515325804127\n",
      "  batch 401 loss: 0.00016317477205120666\n",
      "LOSS train 0.003744624764826449 valid 0.0002487967722117901\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00015660129487514495\n",
      "  batch 101 loss: 0.17666716682724654\n",
      "  batch 201 loss: 0.004520320312003605\n",
      "  batch 301 loss: 0.0008116702847473789\n",
      "  batch 401 loss: 0.00014665635520941578\n",
      "LOSS train 0.04115878185103749 valid 5.7575321989133954e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2513832189142703e-06\n",
      "  batch 101 loss: 6.552598239068175e-05\n",
      "  batch 201 loss: 3.851004450552864e-05\n",
      "  batch 301 loss: 2.043788436367322e-05\n",
      "  batch 401 loss: 2.4992774792735873e-05\n",
      "LOSS train 3.603615635258627e-05 valid 4.031063144793734e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.2707024325209203e-07\n",
      "  batch 101 loss: 2.5545082698954503e-05\n",
      "  batch 201 loss: 2.800910323685457e-05\n",
      "  batch 301 loss: 3.252971653182613e-05\n",
      "  batch 401 loss: 0.00012266406526578066\n",
      "LOSS train 6.9861489950585e-05 valid 0.0006345116416923702\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.147579988464713e-06\n",
      "  batch 101 loss: 0.0003547970795625588\n",
      "  batch 201 loss: 0.00015464461142983055\n",
      "  batch 301 loss: 0.0001412608507416735\n",
      "  batch 401 loss: 0.00042681136528699426\n",
      "LOSS train 0.0003198869031727901 valid 0.013998400419950485\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.0001284078136086464\n",
      "  batch 101 loss: 0.008493273107451387\n",
      "  batch 201 loss: 0.0024283707365975716\n",
      "  batch 301 loss: 0.004234392249200028\n",
      "  batch 401 loss: 0.005328479587915353\n",
      "LOSS train 0.006559950981234488 valid 0.10651911050081253\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0010372480750083922\n",
      "  batch 101 loss: 0.021442264339420945\n",
      "  batch 201 loss: 0.0015093911753501744\n",
      "  batch 301 loss: 0.0010603548790822969\n",
      "  batch 401 loss: 0.00028127143767051163\n",
      "LOSS train 0.00574624399945622 valid 4.4689226342597976e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.784567110822536e-07\n",
      "  batch 101 loss: 0.00027573180192121073\n",
      "  batch 201 loss: 0.0018857553471752907\n",
      "  batch 301 loss: 0.006518026589765214\n",
      "  batch 401 loss: 0.00432106543739792\n",
      "LOSS train 0.0030905062286892765 valid 0.0001227719767484814\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.6956244255416094e-06\n",
      "  batch 101 loss: 0.002111447141505778\n",
      "  batch 201 loss: 0.005226865899167024\n",
      "  batch 301 loss: 0.004373867789690848\n",
      "  batch 401 loss: 0.011300713929813355\n",
      "LOSS train 0.005876821342805236 valid 0.01998087391257286\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.00025590110570192334\n",
      "  batch 101 loss: 0.005005427692958619\n",
      "  batch 201 loss: 0.00063518733230012\n",
      "  batch 301 loss: 0.0005772859508215334\n",
      "  batch 401 loss: 0.0016913248354467215\n",
      "LOSS train 0.0023378984564666966 valid 0.0014076799852773547\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.221484435722232e-05\n",
      "  batch 101 loss: 0.0029339070580317638\n",
      "  batch 201 loss: 0.01107493791205343\n",
      "  batch 301 loss: 0.005339364885585383\n",
      "  batch 401 loss: 0.007054111882462166\n",
      "LOSS train 0.00609118891106004 valid 0.00011222979082958773\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.7785639395006e-06\n",
      "  batch 101 loss: 0.001122448186724796\n",
      "  batch 201 loss: 0.0013532075980037916\n",
      "  batch 301 loss: 0.001036196688291966\n",
      "  batch 401 loss: 0.00874854765861528\n",
      "LOSS train 0.0034448435425213863 valid 0.0007052668370306492\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.293548943474889e-05\n",
      "  batch 101 loss: 0.017985090662259607\n",
      "  batch 201 loss: 0.0038721396414621266\n",
      "  batch 301 loss: 0.00048109189698152476\n",
      "  batch 401 loss: 0.000424891738439328\n",
      "LOSS train 0.005193794219678822 valid 0.00015555777645204216\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00030183464288711546\n",
      "  batch 101 loss: 0.15807946398854256\n",
      "  batch 201 loss: 0.001592274227878079\n",
      "  batch 301 loss: 0.00012349647851806367\n",
      "  batch 401 loss: 7.62032834791171e-05\n",
      "LOSS train 0.03616436670433254 valid 0.00035813148133456707\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.226416396908462e-06\n",
      "  batch 101 loss: 8.030907050851966e-05\n",
      "  batch 201 loss: 4.88722744921688e-05\n",
      "  batch 301 loss: 4.387244035569893e-05\n",
      "  batch 401 loss: 5.415842121692549e-05\n",
      "LOSS train 6.148664090669339e-05 valid 0.00038649223279207945\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.4893184783868494e-06\n",
      "  batch 101 loss: 0.0002495408512550057\n",
      "  batch 201 loss: 0.00023043171509925741\n",
      "  batch 301 loss: 0.0003831402045034338\n",
      "  batch 401 loss: 0.00035867789396434093\n",
      "LOSS train 0.00030845288664325135 valid 0.00015969133528415114\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.4953401114325971e-06\n",
      "  batch 101 loss: 0.0008702875676681287\n",
      "  batch 201 loss: 0.002689946641330607\n",
      "  batch 301 loss: 0.012396624581306241\n",
      "  batch 401 loss: 0.04115415404550731\n",
      "LOSS train 0.023788596726708276 valid 0.02436855062842369\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.00031210388988256454\n",
      "  batch 101 loss: 0.05761907760286704\n",
      "  batch 201 loss: 0.001285916929555242\n",
      "  batch 301 loss: 9.578104793035891e-05\n",
      "  batch 401 loss: 8.065873670602741e-05\n",
      "LOSS train 0.013411338049176118 valid 6.175843009259552e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.307989679044113e-07\n",
      "  batch 101 loss: 7.204685069154948e-05\n",
      "  batch 201 loss: 0.00010658554165274837\n",
      "  batch 301 loss: 7.215374329462065e-05\n",
      "  batch 401 loss: 0.00015596357917274873\n",
      "LOSS train 0.00014407504476044412 valid 0.0034138408955186605\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.58661899715662e-05\n",
      "  batch 101 loss: 0.004302832745015622\n",
      "  batch 201 loss: 0.005477386476704851\n",
      "  batch 301 loss: 0.00513650179025717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.0029684180094045587\n",
      "LOSS train 0.004754404445469451 valid 0.028977246955037117\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0003019717521965504\n",
      "  batch 101 loss: 0.013295830651768483\n",
      "  batch 201 loss: 0.0027925331227015705\n",
      "  batch 301 loss: 0.0015590441497624853\n",
      "  batch 401 loss: 0.0009595723188249394\n",
      "LOSS train 0.004522038876230389 valid 0.002484695753082633\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.0231581293046474e-05\n",
      "  batch 101 loss: 0.03860887715476565\n",
      "  batch 201 loss: 0.05754811262246221\n",
      "  batch 301 loss: 0.010951792609412224\n",
      "  batch 401 loss: 0.008938078940846026\n",
      "LOSS train 0.026653678707174566 valid 0.024624213576316833\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.00033106390386819837\n",
      "  batch 101 loss: 0.0040650507138343525\n",
      "  batch 201 loss: 0.0009903496190963779\n",
      "  batch 301 loss: 0.000622715375648113\n",
      "  batch 401 loss: 0.0006364447161467979\n",
      "LOSS train 0.001700631641955538 valid 0.0027649765834212303\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.028400406241417e-05\n",
      "  batch 101 loss: 0.0010316311329370365\n",
      "  batch 201 loss: 0.001843330950650852\n",
      "  batch 301 loss: 0.004007420421694405\n",
      "  batch 401 loss: 0.0046176790894242\n",
      "LOSS train 0.00299483238259261 valid 0.0003515330608934164\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.9058999605476856e-05\n",
      "  batch 101 loss: 0.002642677577678114\n",
      "  batch 201 loss: 0.004148891298827948\n",
      "  batch 301 loss: 0.015191029743291437\n",
      "  batch 401 loss: 0.011875370307825506\n",
      "LOSS train 0.008836717572034037 valid 0.08507281541824341\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002507006004452705\n",
      "  batch 101 loss: 0.023157771738315205\n",
      "  batch 201 loss: 5.6547700087321576e-05\n",
      "  batch 301 loss: 1.752862656474008e-05\n",
      "  batch 401 loss: 5.4114030376695155e-06\n",
      "LOSS train 0.005302386293373645 valid 5.2568266255548224e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.80849289993057e-07\n",
      "  batch 101 loss: 5.402991357073006e-06\n",
      "  batch 201 loss: 4.808597439591722e-06\n",
      "  batch 301 loss: 4.704208671597598e-06\n",
      "  batch 401 loss: 4.177480951739198e-06\n",
      "LOSS train 4.7207602461872415e-06 valid 5.1348699344089255e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.1782974979723803e-07\n",
      "  batch 101 loss: 5.181619231393597e-06\n",
      "  batch 201 loss: 3.234710009110131e-06\n",
      "  batch 301 loss: 4.955324642992309e-06\n",
      "  batch 401 loss: 4.366679184499844e-06\n",
      "LOSS train 4.527675951934475e-06 valid 6.421946454793215e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.118215656490065e-07\n",
      "  batch 101 loss: 8.339045151330993e-06\n",
      "  batch 201 loss: 7.448262795435312e-06\n",
      "  batch 301 loss: 7.3019081790448585e-06\n",
      "  batch 401 loss: 4.765439935283666e-06\n",
      "LOSS train 6.943199595730375e-06 valid 4.91357859573327e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.353594355663517e-08\n",
      "  batch 101 loss: 8.324198001616878e-06\n",
      "  batch 201 loss: 1.5275139413688522e-05\n",
      "  batch 301 loss: 1.1921051250283199e-05\n",
      "  batch 401 loss: 9.224202622135635e-06\n",
      "LOSS train 1.1842945672955851e-05 valid 5.011168832425028e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.019619938044344e-08\n",
      "  batch 101 loss: 1.3999508734627852e-05\n",
      "  batch 201 loss: 1.7060685555634335e-05\n",
      "  batch 301 loss: 9.375895266430235e-06\n",
      "  batch 401 loss: 6.188658937276159e-06\n",
      "LOSS train 1.1933407938382561e-05 valid 5.076855086372234e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.0525185391306877e-08\n",
      "  batch 101 loss: 1.0281108793037674e-05\n",
      "  batch 201 loss: 1.545890118563875e-05\n",
      "  batch 301 loss: 9.279353167812588e-06\n",
      "  batch 401 loss: 5.5992909935298485e-06\n",
      "LOSS train 9.936572503236647e-06 valid 5.048662205808796e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2209653732497827e-08\n",
      "  batch 101 loss: 6.419067985916626e-06\n",
      "  batch 201 loss: 1.2131210969528183e-05\n",
      "  batch 301 loss: 1.018257023190472e-05\n",
      "  batch 401 loss: 7.199173373493295e-06\n",
      "LOSS train 8.803649316784981e-06 valid 4.9467107601230964e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.669177092684549e-08\n",
      "  batch 101 loss: 5.300861293733306e-06\n",
      "  batch 201 loss: 7.969208657812033e-06\n",
      "  batch 301 loss: 6.299400333631411e-06\n",
      "  batch 401 loss: 5.127015729726736e-06\n",
      "LOSS train 6.026586053958895e-06 valid 6.989836401771754e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.726651993929408e-07\n",
      "  batch 101 loss: 6.628776681623094e-06\n",
      "  batch 201 loss: 4.511742016006793e-06\n",
      "  batch 301 loss: 3.6888721547256865e-06\n",
      "  batch 401 loss: 3.3019301278613967e-06\n",
      "LOSS train 4.556756541580879e-06 valid 5.818719364469871e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.5744173651910386e-07\n",
      "  batch 101 loss: 4.221276434464016e-06\n",
      "  batch 201 loss: 3.0771022136377722e-06\n",
      "  batch 301 loss: 2.7444450316238545e-06\n",
      "  batch 401 loss: 2.7432594151832745e-06\n",
      "LOSS train 3.242330613660717e-06 valid 5.5936252465471625e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.7416825762193185e-08\n",
      "  batch 101 loss: 3.1615817588814822e-06\n",
      "  batch 201 loss: 2.818371779227391e-06\n",
      "  batch 301 loss: 2.8532971682082576e-06\n",
      "  batch 401 loss: 3.2012767032085777e-06\n",
      "LOSS train 2.9896961330978867e-06 valid 5.37060150236357e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00017442131415009498\n",
      "  batch 101 loss: 0.03271702403736583\n",
      "  batch 201 loss: 8.592388124270656e-05\n",
      "  batch 301 loss: 0.00300743455702559\n",
      "  batch 401 loss: 9.213912440941385e-05\n",
      "LOSS train 0.008152200296059306 valid 0.00012017510016448796\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.638245885260403e-06\n",
      "  batch 101 loss: 0.00010506417115038858\n",
      "  batch 201 loss: 0.00010904378090685896\n",
      "  batch 301 loss: 0.00010945206801523\n",
      "  batch 401 loss: 0.00010768204195613861\n",
      "LOSS train 0.0001051370059834779 valid 0.00021764553093817085\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.1043923809193075e-06\n",
      "  batch 101 loss: 0.00010311526210216471\n",
      "  batch 201 loss: 9.876924589718783e-05\n",
      "  batch 301 loss: 9.068071431698854e-05\n",
      "  batch 401 loss: 8.377863626719773e-05\n",
      "LOSS train 9.142005526098196e-05 valid 0.0002043920394498855\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.916775119956583e-06\n",
      "  batch 101 loss: 7.831108197933645e-05\n",
      "  batch 201 loss: 7.393284392151145e-05\n",
      "  batch 301 loss: 6.796484991696161e-05\n",
      "  batch 401 loss: 6.427837756518784e-05\n",
      "LOSS train 6.972805217741692e-05 valid 0.0001680409477557987\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.387435961281881e-06\n",
      "  batch 101 loss: 6.345937045153959e-05\n",
      "  batch 201 loss: 6.146029321712376e-05\n",
      "  batch 301 loss: 5.8565845946532136e-05\n",
      "  batch 401 loss: 5.766721282839171e-05\n",
      "LOSS train 5.9696631867453e-05 valid 0.00015712820459157228\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2231369803193956e-06\n",
      "  batch 101 loss: 6.01015304567909e-05\n",
      "  batch 201 loss: 5.954365493721525e-05\n",
      "  batch 301 loss: 5.822550896368739e-05\n",
      "  batch 401 loss: 5.87968131640082e-05\n",
      "LOSS train 5.9094011675591714e-05 valid 0.0001529063010821119\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.158730931114405e-06\n",
      "  batch 101 loss: 6.149630007939777e-05\n",
      "  batch 201 loss: 6.0734791616994244e-05\n",
      "  batch 301 loss: 5.9207525530382555e-05\n",
      "  batch 401 loss: 5.9228834221016765e-05\n",
      "LOSS train 6.021102746214543e-05 valid 0.00011814094614237547\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.6042897186707705e-06\n",
      "  batch 101 loss: 5.72083187392991e-05\n",
      "  batch 201 loss: 5.549134036755277e-05\n",
      "  batch 301 loss: 5.3429705683356585e-05\n",
      "  batch 401 loss: 5.236529129888367e-05\n",
      "LOSS train 5.447461196194061e-05 valid 8.287678065244108e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 9.565376240061596e-07\n",
      "  batch 101 loss: 4.848033929306439e-05\n",
      "  batch 201 loss: 4.749834348388049e-05\n",
      "  batch 301 loss: 4.658054941785394e-05\n",
      "  batch 401 loss: 4.649005053522615e-05\n",
      "LOSS train 4.722038997027497e-05 valid 7.327990897465497e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 7.3933471867349e-07\n",
      "  batch 101 loss: 4.4573141943828885e-05\n",
      "  batch 201 loss: 4.4737726816492796e-05\n",
      "  batch 301 loss: 4.534236733661601e-05\n",
      "  batch 401 loss: 4.65914792471267e-05\n",
      "LOSS train 4.544605424148943e-05 valid 7.12488908902742e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 6.874646351207048e-07\n",
      "  batch 101 loss: 4.568644810291289e-05\n",
      "  batch 201 loss: 4.681125375554984e-05\n",
      "  batch 301 loss: 4.8168242165615994e-05\n",
      "  batch 401 loss: 4.932691653834809e-05\n",
      "LOSS train 4.763982977492735e-05 valid 6.498829316115007e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.968118082615547e-07\n",
      "  batch 101 loss: 4.6756091889506025e-05\n",
      "  batch 201 loss: 4.822021612170602e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 4.87714108334103e-05\n",
      "  batch 401 loss: 4.819637177703839e-05\n",
      "LOSS train 4.8002162012431024e-05 valid 6.166492676129565e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 6.906733382493258e-05\n",
      "  batch 101 loss: 0.042769454528606726\n",
      "  batch 201 loss: 7.64891933886247e-05\n",
      "  batch 301 loss: 8.0464538606293e-05\n",
      "  batch 401 loss: 9.530207193392926e-05\n",
      "LOSS train 0.00973542785467442 valid 0.00011062600970035419\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.4765864762011915e-06\n",
      "  batch 101 loss: 0.00010299892063244443\n",
      "  batch 201 loss: 0.00010748065127984319\n",
      "  batch 301 loss: 0.00010918240565160886\n",
      "  batch 401 loss: 0.00010906123619534469\n",
      "LOSS train 0.00010479592864501393 valid 0.00020840934303123504\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.9739030287601053e-06\n",
      "  batch 101 loss: 0.00010617384494537418\n",
      "  batch 201 loss: 0.00010297288166441376\n",
      "  batch 301 loss: 9.568607277742558e-05\n",
      "  batch 401 loss: 8.895695264982351e-05\n",
      "LOSS train 9.561126951499343e-05 valid 0.0002128117048414424\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.0362457619048655e-06\n",
      "  batch 101 loss: 8.302561999471437e-05\n",
      "  batch 201 loss: 7.836156682174078e-05\n",
      "  batch 301 loss: 7.175931466008479e-05\n",
      "  batch 401 loss: 6.739074925349086e-05\n",
      "LOSS train 7.351130801154393e-05 valid 0.00017411584849469364\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.477673115208745e-06\n",
      "  batch 101 loss: 6.568046444272113e-05\n",
      "  batch 201 loss: 6.323501715542079e-05\n",
      "  batch 301 loss: 5.9775696366841655e-05\n",
      "  batch 401 loss: 5.8343471618513835e-05\n",
      "LOSS train 6.101216843574139e-05 valid 0.00015793379861861467\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.235369320260361e-06\n",
      "  batch 101 loss: 6.0162362406686045e-05\n",
      "  batch 201 loss: 5.936765838100655e-05\n",
      "  batch 301 loss: 5.780387387517294e-05\n",
      "  batch 401 loss: 5.8146425292875395e-05\n",
      "LOSS train 5.868498464504856e-05 valid 0.00015570636605843902\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2015029389876873e-06\n",
      "  batch 101 loss: 6.126090098916848e-05\n",
      "  batch 201 loss: 6.074149445367993e-05\n",
      "  batch 301 loss: 5.942889504069626e-05\n",
      "  batch 401 loss: 5.980939532832963e-05\n",
      "LOSS train 6.038230391916063e-05 valid 0.00013029466208536178\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.8039104179479182e-06\n",
      "  batch 101 loss: 5.922833343504408e-05\n",
      "  batch 201 loss: 5.763297455473548e-05\n",
      "  batch 301 loss: 5.556434487289153e-05\n",
      "  batch 401 loss: 5.45975937870935e-05\n",
      "LOSS train 5.6637546571551216e-05 valid 8.937369420891628e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.088232165784575e-06\n",
      "  batch 101 loss: 5.05878490014311e-05\n",
      "  batch 201 loss: 4.930947621573978e-05\n",
      "  batch 301 loss: 4.7979727682729844e-05\n",
      "  batch 401 loss: 4.750861600030021e-05\n",
      "LOSS train 4.8754318783392574e-05 valid 7.442472269758582e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 7.673692016396671e-07\n",
      "  batch 101 loss: 4.5027026187867136e-05\n",
      "  batch 201 loss: 4.488643253552027e-05\n",
      "  batch 301 loss: 4.510534285458334e-05\n",
      "  batch 401 loss: 4.604764125929251e-05\n",
      "LOSS train 4.536211580348432e-05 valid 7.194657518994063e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.05624624970369e-07\n",
      "  batch 101 loss: 4.51022879667562e-05\n",
      "  batch 201 loss: 4.600522991665912e-05\n",
      "  batch 301 loss: 4.730475878432117e-05\n",
      "  batch 401 loss: 4.872560420380978e-05\n",
      "LOSS train 4.6948293525035236e-05 valid 6.709430454066023e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 5.687246084562502e-07\n",
      "  batch 101 loss: 4.682310794692057e-05\n",
      "  batch 201 loss: 4.826332685098578e-05\n",
      "  batch 301 loss: 4.9142688662016096e-05\n",
      "  batch 401 loss: 4.90525907704864e-05\n",
      "LOSS train 4.8364431108701295e-05 valid 6.162187492009252e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001115256641060114\n",
      "  batch 101 loss: 0.06316049477492924\n",
      "  batch 201 loss: 0.00017898854698387368\n",
      "  batch 301 loss: 7.776698862471676e-05\n",
      "  batch 401 loss: 8.945892176598136e-05\n",
      "LOSS train 0.014369018388188185 valid 9.15876153158024e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.131284807343036e-06\n",
      "  batch 101 loss: 9.693246745655415e-05\n",
      "  batch 201 loss: 0.00010173458532790392\n",
      "  batch 301 loss: 0.00010535467960380629\n",
      "  batch 401 loss: 0.00010835964097566375\n",
      "LOSS train 0.00010155829865302991 valid 0.0001727553317323327\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.4575323914177715e-06\n",
      "  batch 101 loss: 0.00010964563522946946\n",
      "  batch 201 loss: 0.00010973811803467016\n",
      "  batch 301 loss: 0.00010599020997688058\n",
      "  batch 401 loss: 0.00010152258277173587\n",
      "LOSS train 0.00010378692353881778 valid 0.00022420323512051255\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.196357865817845e-06\n",
      "  batch 101 loss: 9.61587098204575e-05\n",
      "  batch 201 loss: 9.178047087786467e-05\n",
      "  batch 301 loss: 8.434299411362645e-05\n",
      "  batch 401 loss: 7.854256922882996e-05\n",
      "LOSS train 8.54335341538425e-05 valid 0.00019593871547840536\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.795764012262225e-06\n",
      "  batch 101 loss: 7.45745211816029e-05\n",
      "  batch 201 loss: 7.098550300383976e-05\n",
      "  batch 301 loss: 6.584544505130907e-05\n",
      "  batch 401 loss: 6.281297685291065e-05\n",
      "LOSS train 6.732920978561248e-05 valid 0.0001655689557082951\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.350474242120981e-06\n",
      "  batch 101 loss: 6.263156979130713e-05\n",
      "  batch 201 loss: 6.0914179255746605e-05\n",
      "  batch 301 loss: 5.827711304220884e-05\n",
      "  batch 401 loss: 5.757008469970515e-05\n",
      "LOSS train 5.9307202817590826e-05 valid 0.0001570787135278806\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.222385082859546e-06\n",
      "  batch 101 loss: 6.014851954830647e-05\n",
      "  batch 201 loss: 5.962740327959182e-05\n",
      "  batch 301 loss: 5.833447116344814e-05\n",
      "  batch 401 loss: 5.891567483786275e-05\n",
      "LOSS train 5.9195145424539705e-05 valid 0.0001523751998320222\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.150593791157007e-06\n",
      "  batch 101 loss: 6.15029015204982e-05\n",
      "  batch 201 loss: 6.071443274038302e-05\n",
      "  batch 301 loss: 5.917136725088312e-05\n",
      "  batch 401 loss: 5.9169406554815396e-05\n",
      "LOSS train 6.018013475126893e-05 valid 0.00011748054384952411\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.5932114911265672e-06\n",
      "  batch 101 loss: 5.710252138385385e-05\n",
      "  batch 201 loss: 5.541504529674057e-05\n",
      "  batch 301 loss: 5.338034736951158e-05\n",
      "  batch 401 loss: 5.2337835411435664e-05\n",
      "LOSS train 5.4412595166300914e-05 valid 8.286654338007793e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 9.56322910496965e-07\n",
      "  batch 101 loss: 4.848325313446366e-05\n",
      "  batch 201 loss: 4.751366105949728e-05\n",
      "  batch 301 loss: 4.660004188480116e-05\n",
      "  batch 401 loss: 4.65080804332274e-05\n",
      "LOSS train 4.7234429121808775e-05 valid 7.330753578571603e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.400194590445608e-07\n",
      "  batch 101 loss: 4.458160328226768e-05\n",
      "  batch 201 loss: 4.473640604828688e-05\n",
      "  batch 301 loss: 4.532569533296282e-05\n",
      "  batch 401 loss: 4.656109006418774e-05\n",
      "LOSS train 4.5434345251438e-05 valid 7.129464211175218e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 6.886668415972963e-07\n",
      "  batch 101 loss: 4.5656238386015956e-05\n",
      "  batch 201 loss: 4.676861468396964e-05\n",
      "  batch 301 loss: 4.812359264292354e-05\n",
      "  batch 401 loss: 4.930035189431692e-05\n",
      "LOSS train 4.76051058187103e-05 valid 6.511412357212976e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.365956135094166e-05\n",
      "  batch 101 loss: 0.005216667158547921\n",
      "  batch 201 loss: 0.00010650733551585745\n",
      "  batch 301 loss: 9.75579378882685e-05\n",
      "  batch 401 loss: 7.887639615319131e-05\n",
      "LOSS train 0.0012656110677423803 valid 0.0001838373573264107\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.6204704772680996e-06\n",
      "  batch 101 loss: 6.745800188809881e-05\n",
      "  batch 201 loss: 6.232289366835175e-05\n",
      "  batch 301 loss: 5.8204274722015726e-05\n",
      "  batch 401 loss: 5.7283147793327774e-05\n",
      "LOSS train 6.076836941224582e-05 valid 0.00015618406177964061\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.208778023486957e-06\n",
      "  batch 101 loss: 6.053959393739206e-05\n",
      "  batch 201 loss: 6.026291521266103e-05\n",
      "  batch 301 loss: 5.915395732131401e-05\n",
      "  batch 401 loss: 5.971887883333693e-05\n",
      "LOSS train 6.002921360426314e-05 valid 0.00012974174751434475\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.7949861648958176e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 5.892156762570267e-05\n",
      "  batch 201 loss: 5.6806939320495075e-05\n",
      "  batch 301 loss: 5.433322584622147e-05\n",
      "  batch 401 loss: 5.292269575306818e-05\n",
      "LOSS train 5.5557740069896794e-05 valid 8.330933633260429e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 9.655983740231021e-07\n",
      "  batch 101 loss: 4.8526039728358227e-05\n",
      "  batch 201 loss: 4.733396754545538e-05\n",
      "  batch 301 loss: 4.633987812155738e-05\n",
      "  batch 401 loss: 4.6269580096236496e-05\n",
      "LOSS train 4.707365880684874e-05 valid 7.293290400411934e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.306775660254061e-07\n",
      "  batch 101 loss: 4.4494670980839146e-05\n",
      "  batch 201 loss: 4.480988322370649e-05\n",
      "  batch 301 loss: 4.5638935929446234e-05\n",
      "  batch 401 loss: 4.7071973010588405e-05\n",
      "LOSS train 4.566266496612717e-05 valid 7.046003156574443e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.66433697915636e-07\n",
      "  batch 101 loss: 4.6109068026680685e-05\n",
      "  batch 201 loss: 4.739278201043362e-05\n",
      "  batch 301 loss: 4.8725165019050106e-05\n",
      "  batch 401 loss: 4.95596111602481e-05\n",
      "LOSS train 4.8064785336460304e-05 valid 6.33509480394423e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.283640009816736e-07\n",
      "  batch 101 loss: 4.639654110178526e-05\n",
      "  batch 201 loss: 4.782908471042901e-05\n",
      "  batch 301 loss: 4.804566821803746e-05\n",
      "  batch 401 loss: 4.705910042815731e-05\n",
      "LOSS train 4.733433397416757e-05 valid 6.230632425285876e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.9348974092281424e-07\n",
      "  batch 101 loss: 4.272034643662437e-05\n",
      "  batch 201 loss: 4.3799327106626154e-05\n",
      "  batch 301 loss: 4.3212169910589185e-05\n",
      "  batch 401 loss: 4.174417293427268e-05\n",
      "LOSS train 4.2953118373410846e-05 valid 6.564665818586946e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.0952146112686023e-07\n",
      "  batch 101 loss: 3.862264919717973e-05\n",
      "  batch 201 loss: 3.913819315357614e-05\n",
      "  batch 301 loss: 3.874313031815291e-05\n",
      "  batch 401 loss: 3.7771195934368503e-05\n",
      "LOSS train 3.8831860877438205e-05 valid 6.565936928382143e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.093298942578258e-07\n",
      "  batch 101 loss: 3.631606178529978e-05\n",
      "  batch 201 loss: 3.6456677681258045e-05\n",
      "  batch 301 loss: 3.652138119036863e-05\n",
      "  batch 401 loss: 3.6076992120399606e-05\n",
      "LOSS train 3.674403108628029e-05 valid 6.371202471200377e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.4644884686276782e-07\n",
      "  batch 101 loss: 3.573757240758369e-05\n",
      "  batch 201 loss: 3.561550133781566e-05\n",
      "  batch 301 loss: 3.599923617940704e-05\n",
      "  batch 401 loss: 3.586822519537236e-05\n",
      "LOSS train 3.6273796184965345e-05 valid 6.205443787621334e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00010304851457476616\n",
      "  batch 101 loss: 0.0005517494456211125\n",
      "  batch 201 loss: 5.6536499620278846e-05\n",
      "  batch 301 loss: 5.3594387970861135e-05\n",
      "  batch 401 loss: 4.816026018090724e-05\n",
      "LOSS train 0.00018786760951560862 valid 7.228915637824684e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.120045484043658e-07\n",
      "  batch 101 loss: 4.440152246047546e-05\n",
      "  batch 201 loss: 4.561455988891794e-05\n",
      "  batch 301 loss: 4.765956404412464e-05\n",
      "  batch 401 loss: 4.9383091389927355e-05\n",
      "LOSS train 4.701062861077951e-05 valid 6.334002682706341e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.260170317138545e-07\n",
      "  batch 101 loss: 4.628391183587155e-05\n",
      "  batch 201 loss: 4.745683812302559e-05\n",
      "  batch 301 loss: 4.6934697616336505e-05\n",
      "  batch 401 loss: 4.50928941754114e-05\n",
      "LOSS train 4.640526269778394e-05 valid 6.405823660315946e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3837596270604991e-07\n",
      "  batch 101 loss: 4.05366752562486e-05\n",
      "  batch 201 loss: 4.106721367236332e-05\n",
      "  batch 301 loss: 4.0199030528924596e-05\n",
      "  batch 401 loss: 3.8808821832887474e-05\n",
      "LOSS train 4.0318930887588044e-05 valid 6.599483458558097e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.0443986866448541e-07\n",
      "  batch 101 loss: 3.6726000753617424e-05\n",
      "  batch 201 loss: 3.684846809704823e-05\n",
      "  batch 301 loss: 3.6821353555751555e-05\n",
      "  batch 401 loss: 3.6219665463192994e-05\n",
      "LOSS train 3.7026762353506204e-05 valid 6.398152618203312e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4010085578775032e-07\n",
      "  batch 101 loss: 3.57286935448542e-05\n",
      "  batch 201 loss: 3.5774521581828367e-05\n",
      "  batch 301 loss: 3.601093850591042e-05\n",
      "  batch 401 loss: 3.587764899720014e-05\n",
      "LOSS train 3.631000956553148e-05 valid 6.210808351170272e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.03991039597895e-07\n",
      "  batch 101 loss: 3.6076951435575214e-05\n",
      "  batch 201 loss: 3.572246261427381e-05\n",
      "  batch 301 loss: 3.626236266768501e-05\n",
      "  batch 401 loss: 3.628503007973904e-05\n",
      "LOSS train 3.656745174495736e-05 valid 6.15540993749164e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.7840455004479734e-07\n",
      "  batch 101 loss: 3.670656723841148e-05\n",
      "  batch 201 loss: 3.615092945125298e-05\n",
      "  batch 301 loss: 3.67590230547421e-05\n",
      "  batch 401 loss: 3.6735386012409774e-05\n",
      "LOSS train 3.705997731566387e-05 valid 6.189963460201398e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.3744803658919407e-07\n",
      "  batch 101 loss: 3.7231487989970444e-05\n",
      "  batch 201 loss: 3.649392913700922e-05\n",
      "  batch 301 loss: 3.71303251563404e-05\n",
      "  batch 401 loss: 3.7094563697053215e-05\n",
      "LOSS train 3.7438087876529725e-05 valid 6.222967203939334e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.6450215702643617e-07\n",
      "  batch 101 loss: 3.75891179919563e-05\n",
      "  batch 201 loss: 3.674833923355436e-05\n",
      "  batch 301 loss: 3.736949638209808e-05\n",
      "  batch 401 loss: 3.727920466303658e-05\n",
      "LOSS train 3.769537369148319e-05 valid 6.26158362138085e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.8952719478402286e-07\n",
      "  batch 101 loss: 3.774166835910364e-05\n",
      "  batch 201 loss: 3.6881502501273646e-05\n",
      "  batch 301 loss: 3.7455683945495365e-05\n",
      "  batch 401 loss: 3.7378531334866235e-05\n",
      "LOSS train 3.7812569035705433e-05 valid 6.282565300352871e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.0149621781893075e-07\n",
      "  batch 101 loss: 3.784771863138303e-05\n",
      "  batch 201 loss: 3.692525874726016e-05\n",
      "  batch 301 loss: 3.751408081313912e-05\n",
      "  batch 401 loss: 3.7454266537224614e-05\n",
      "LOSS train 3.7879466769328485e-05 valid 6.28987472737208e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.455881865695119e-06\n",
      "  batch 101 loss: 0.0004370666572208393\n",
      "  batch 201 loss: 5.800691524200374e-05\n",
      "  batch 301 loss: 5.2073542465223e-05\n",
      "  batch 401 loss: 4.4371602002684084e-05\n",
      "LOSS train 0.00013817241897267344 valid 7.087781705195084e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.834895611973479e-07\n",
      "  batch 101 loss: 4.5473885961087035e-05\n",
      "  batch 201 loss: 4.754545012218614e-05\n",
      "  batch 301 loss: 4.911183397325658e-05\n",
      "  batch 401 loss: 4.893262751011207e-05\n",
      "LOSS train 4.787612551108108e-05 valid 6.160766497487202e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.5186864149873145e-07\n",
      "  batch 101 loss: 4.396245281895972e-05\n",
      "  batch 201 loss: 4.452621279995128e-05\n",
      "  batch 301 loss: 4.342164395609416e-05\n",
      "  batch 401 loss: 4.1409237330185536e-05\n",
      "LOSS train 4.335181834651255e-05 valid 6.59428333165124e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0516500879020895e-07\n",
      "  batch 101 loss: 3.805832418549926e-05\n",
      "  batch 201 loss: 3.8300511657780586e-05\n",
      "  batch 301 loss: 3.784146637144659e-05\n",
      "  batch 401 loss: 3.694302008582895e-05\n",
      "LOSS train 3.8079650129961394e-05 valid 6.497045251308009e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.2051059457007795e-07\n",
      "  batch 101 loss: 3.590483522827981e-05\n",
      "  batch 201 loss: 3.5910529033031934e-05\n",
      "  batch 301 loss: 3.611929949528303e-05\n",
      "  batch 401 loss: 3.584694836945346e-05\n",
      "LOSS train 3.638617101460365e-05 valid 6.269357982091606e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.7715748981572688e-07\n",
      "  batch 101 loss: 3.5875488091932083e-05\n",
      "  batch 201 loss: 3.5613706532586776e-05\n",
      "  batch 301 loss: 3.611801060230846e-05\n",
      "  batch 401 loss: 3.608689042181368e-05\n",
      "LOSS train 3.640418700559815e-05 valid 6.160214252304286e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.5206183636328204e-07\n",
      "  batch 101 loss: 3.647653569657905e-05\n",
      "  batch 201 loss: 3.5983539389405907e-05\n",
      "  batch 301 loss: 3.658157160998598e-05\n",
      "  batch 401 loss: 3.659087417901219e-05\n",
      "LOSS train 3.6882924270078764e-05 valid 6.173659494379535e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.1954117730492725e-07\n",
      "  batch 101 loss: 3.706475177182256e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 3.640088331906099e-05\n",
      "  batch 301 loss: 3.7010839112667784e-05\n",
      "  batch 401 loss: 3.6990238115919286e-05\n",
      "LOSS train 3.732876386317525e-05 valid 6.22130319243297e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.6330096918391065e-07\n",
      "  batch 101 loss: 3.746506822920992e-05\n",
      "  batch 201 loss: 3.6686409985122736e-05\n",
      "  batch 301 loss: 3.728593522453138e-05\n",
      "  batch 401 loss: 3.723043202484178e-05\n",
      "LOSS train 3.761971385581346e-05 valid 6.257308996282518e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.8697195122949777e-07\n",
      "  batch 101 loss: 3.7691054142499066e-05\n",
      "  batch 201 loss: 3.684566544222889e-05\n",
      "  batch 301 loss: 3.7434720779288e-05\n",
      "  batch 401 loss: 3.735648270755121e-05\n",
      "LOSS train 3.777933984001343e-05 valid 6.277571810642257e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.98725678678602e-07\n",
      "  batch 101 loss: 3.780617791989016e-05\n",
      "  batch 201 loss: 3.692610650659844e-05\n",
      "  batch 301 loss: 3.750875371849816e-05\n",
      "  batch 401 loss: 3.7418308720873484e-05\n",
      "LOSS train 3.785941896405859e-05 valid 6.287777796387672e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.043404260301031e-07\n",
      "  batch 101 loss: 3.786192360109908e-05\n",
      "  batch 201 loss: 3.696488655407393e-05\n",
      "  batch 301 loss: 3.7544184876310284e-05\n",
      "  batch 401 loss: 3.744769736975684e-05\n",
      "LOSS train 3.7897902888572216e-05 valid 6.2926861573942e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001230862457305193\n",
      "  batch 101 loss: 0.006269368584617041\n",
      "  batch 201 loss: 0.00010780554616758309\n",
      "  batch 301 loss: 6.66322200322611e-05\n",
      "  batch 401 loss: 5.9270148518208996e-05\n",
      "LOSS train 0.0015004330837383069 valid 0.00015374644135590643\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.210399543400854e-06\n",
      "  batch 101 loss: 6.001749706541659e-05\n",
      "  batch 201 loss: 5.9693879769042724e-05\n",
      "  batch 301 loss: 5.8805882125057e-05\n",
      "  batch 401 loss: 5.9550049742256305e-05\n",
      "LOSS train 5.966738386822804e-05 valid 0.00012568949023261666\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.7317934543825686e-06\n",
      "  batch 101 loss: 5.80154784233855e-05\n",
      "  batch 201 loss: 5.5267233544782354e-05\n",
      "  batch 301 loss: 5.242402148951442e-05\n",
      "  batch 401 loss: 5.0728425326269646e-05\n",
      "LOSS train 5.38729878750967e-05 valid 7.797231955919415e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.490226900903508e-07\n",
      "  batch 101 loss: 4.647433803910417e-05\n",
      "  batch 201 loss: 4.563252376044602e-05\n",
      "  batch 301 loss: 4.529414315015856e-05\n",
      "  batch 401 loss: 4.585503972407423e-05\n",
      "LOSS train 4.5855932368590246e-05 valid 7.219841063488275e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.113413448678329e-07\n",
      "  batch 101 loss: 4.48533004868068e-05\n",
      "  batch 201 loss: 4.5659993933213624e-05\n",
      "  batch 301 loss: 4.707196066419783e-05\n",
      "  batch 401 loss: 4.858267185852583e-05\n",
      "LOSS train 4.6728116583263e-05 valid 6.729084998369217e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.744041118305176e-07\n",
      "  batch 101 loss: 4.68211284970721e-05\n",
      "  batch 201 loss: 4.833683077663409e-05\n",
      "  batch 301 loss: 4.914998467029363e-05\n",
      "  batch 401 loss: 4.894571719034957e-05\n",
      "LOSS train 4.8352427307443566e-05 valid 6.156167364679277e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.8380822186591104e-07\n",
      "  batch 101 loss: 4.470012624864239e-05\n",
      "  batch 201 loss: 4.6091483000623156e-05\n",
      "  batch 301 loss: 4.5647797650758546e-05\n",
      "  batch 401 loss: 4.4149435493636704e-05\n",
      "LOSS train 4.516114709675876e-05 valid 6.44096071482636e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3249579751573039e-07\n",
      "  batch 101 loss: 4.016335421852091e-05\n",
      "  batch 201 loss: 4.0919733509099386e-05\n",
      "  batch 301 loss: 4.039504301374564e-05\n",
      "  batch 401 loss: 3.907072400707534e-05\n",
      "LOSS train 4.032607657347904e-05 valid 6.58434146316722e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.0622074114507996e-07\n",
      "  batch 101 loss: 3.7018343275008195e-05\n",
      "  batch 201 loss: 3.7247022860071865e-05\n",
      "  batch 301 loss: 3.714509837095647e-05\n",
      "  batch 401 loss: 3.651631384116172e-05\n",
      "LOSS train 3.7334633514510584e-05 valid 6.447654595831409e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.2917725143779536e-07\n",
      "  batch 101 loss: 3.575963450870745e-05\n",
      "  batch 201 loss: 3.581114488170556e-05\n",
      "  batch 301 loss: 3.607020626162694e-05\n",
      "  batch 401 loss: 3.584576544241713e-05\n",
      "LOSS train 3.632686249552258e-05 valid 6.26084438408725e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.8100163288181647e-07\n",
      "  batch 101 loss: 3.592153501358553e-05\n",
      "  batch 201 loss: 3.571534042194457e-05\n",
      "  batch 301 loss: 3.612618770574727e-05\n",
      "  batch 401 loss: 3.60974005764092e-05\n",
      "LOSS train 3.644723414814095e-05 valid 6.159437180031091e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 2.5089255359489473e-07\n",
      "  batch 101 loss: 3.645750349505761e-05\n",
      "  batch 201 loss: 3.5997918509451665e-05\n",
      "  batch 301 loss: 3.657621878460304e-05\n",
      "  batch 401 loss: 3.660343752301287e-05\n",
      "LOSS train 3.6879047596645535e-05 valid 6.173922884045169e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.788136601448059e-05\n",
      "  batch 101 loss: 0.0068823869838630005\n",
      "  batch 201 loss: 3.868631294466241e-05\n",
      "  batch 301 loss: 8.498195074537307e-05\n",
      "  batch 401 loss: 8.991375595115869e-05\n",
      "LOSS train 0.0016251860662183286 valid 0.0002038565871771425\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.909142931457609e-06\n",
      "  batch 101 loss: 7.518049004318073e-05\n",
      "  batch 201 loss: 6.760052456002086e-05\n",
      "  batch 301 loss: 6.116624764729295e-05\n",
      "  batch 401 loss: 5.842900784642779e-05\n",
      "LOSS train 6.457424798326055e-05 valid 0.00015717292262706906\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.2238161182031037e-06\n",
      "  batch 101 loss: 6.003301443826103e-05\n",
      "  batch 201 loss: 5.940473130749524e-05\n",
      "  batch 301 loss: 5.8199261919753555e-05\n",
      "  batch 401 loss: 5.8995332556719406e-05\n",
      "LOSS train 5.91766941110854e-05 valid 0.00014885519340168685\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.0964469877071677e-06\n",
      "  batch 101 loss: 6.12906199438612e-05\n",
      "  batch 201 loss: 6.004594988894496e-05\n",
      "  batch 301 loss: 5.8053696454862803e-05\n",
      "  batch 401 loss: 5.732600101396201e-05\n",
      "LOSS train 5.912268436154786e-05 valid 0.00010026526433648542\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.2933992547914386e-06\n",
      "  batch 101 loss: 5.3330085994502954e-05\n",
      "  batch 201 loss: 5.147562822799045e-05\n",
      "  batch 301 loss: 4.9576809310565295e-05\n",
      "  batch 401 loss: 4.8652553731471924e-05\n",
      "LOSS train 5.0606238866577325e-05 valid 7.566389831481501e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.9690245911479e-07\n",
      "  batch 101 loss: 4.5564012972505455e-05\n",
      "  batch 201 loss: 4.517211649073261e-05\n",
      "  batch 301 loss: 4.5138925439687224e-05\n",
      "  batch 401 loss: 4.590832211817997e-05\n",
      "LOSS train 4.5516933373915805e-05 valid 7.210925105027854e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.098053174559027e-07\n",
      "  batch 101 loss: 4.49046186349733e-05\n",
      "  batch 201 loss: 4.574386925355611e-05\n",
      "  batch 301 loss: 4.703393364422937e-05\n",
      "  batch 401 loss: 4.8528001410090835e-05\n",
      "LOSS train 4.6725382595996176e-05 valid 6.75479750498198e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.828581197420135e-07\n",
      "  batch 101 loss: 4.679122638322042e-05\n",
      "  batch 201 loss: 4.822688150170507e-05\n",
      "  batch 301 loss: 4.9161149290171126e-05\n",
      "  batch 401 loss: 4.913567870431734e-05\n",
      "LOSS train 4.83789926500996e-05 valid 6.166117236716673e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.086791912210174e-07\n",
      "  batch 101 loss: 4.511856803759429e-05\n",
      "  batch 201 loss: 4.645178953779805e-05\n",
      "  batch 301 loss: 4.62040906992911e-05\n",
      "  batch 401 loss: 4.482810520016756e-05\n",
      "LOSS train 4.566471454124427e-05 valid 6.388933979906142e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.4221302990335972e-07\n",
      "  batch 101 loss: 4.0826652334544636e-05\n",
      "  batch 201 loss: 4.167118859754737e-05\n",
      "  batch 301 loss: 4.107884584925614e-05\n",
      "  batch 401 loss: 3.9775790228304685e-05\n",
      "LOSS train 4.099745097291262e-05 valid 6.609056435991079e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.0310235666111112e-07\n",
      "  batch 101 loss: 3.740560231946688e-05\n",
      "  batch 201 loss: 3.7733019560164394e-05\n",
      "  batch 301 loss: 3.7542576438909234e-05\n",
      "  batch 401 loss: 3.681637134832272e-05\n",
      "LOSS train 3.770413880533287e-05 valid 6.491516978712752e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.2147420420660636e-07\n",
      "  batch 101 loss: 3.59030301657981e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 3.59349339231585e-05\n",
      "  batch 301 loss: 3.6152296978002596e-05\n",
      "  batch 401 loss: 3.586539799186994e-05\n",
      "LOSS train 3.6401710309192127e-05 valid 6.290867167990655e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.922872324939817e-06\n",
      "  batch 101 loss: 0.020720105804866763\n",
      "  batch 201 loss: 0.00010835652188688983\n",
      "  batch 301 loss: 6.14401610346249e-05\n",
      "  batch 401 loss: 4.147242173758059e-05\n",
      "LOSS train 0.004729426103761811 valid 6.695219053654e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.674569496884942e-07\n",
      "  batch 101 loss: 6.175972455281453e-05\n",
      "  batch 201 loss: 0.00010043111304639751\n",
      "  batch 301 loss: 9.670215422431738e-05\n",
      "  batch 401 loss: 8.738927309622113e-05\n",
      "LOSS train 8.393756188712342e-05 valid 0.00014583482698071748\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.0634685643017294e-06\n",
      "  batch 101 loss: 7.5760317928939e-05\n",
      "  batch 201 loss: 7.418280516731101e-05\n",
      "  batch 301 loss: 6.744060846813226e-05\n",
      "  batch 401 loss: 6.333831897336494e-05\n",
      "LOSS train 6.863717767519879e-05 valid 0.00016553582099732012\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.349979185964912e-06\n",
      "  batch 101 loss: 6.25537049647562e-05\n",
      "  batch 201 loss: 6.0546220914829974e-05\n",
      "  batch 301 loss: 5.811094956698071e-05\n",
      "  batch 401 loss: 5.739441076229923e-05\n",
      "LOSS train 5.918130436561618e-05 valid 0.00015662053192500025\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2154179168865083e-06\n",
      "  batch 101 loss: 6.040584730271803e-05\n",
      "  batch 201 loss: 5.995947839892324e-05\n",
      "  batch 301 loss: 5.881151422443054e-05\n",
      "  batch 401 loss: 5.947001895151516e-05\n",
      "LOSS train 5.9691628798015956e-05 valid 0.00014679126616101712\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.066837914753705e-06\n",
      "  batch 101 loss: 6.123527467650546e-05\n",
      "  batch 201 loss: 6.004350891203103e-05\n",
      "  batch 301 loss: 5.817316129878236e-05\n",
      "  batch 401 loss: 5.764671676445232e-05\n",
      "LOSS train 5.924208275773318e-05 valid 0.0001032960499287583\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.3479843619279563e-06\n",
      "  batch 101 loss: 5.4170202517980214e-05\n",
      "  batch 201 loss: 5.247111587550535e-05\n",
      "  batch 301 loss: 5.061460124181849e-05\n",
      "  batch 401 loss: 4.96626985380999e-05\n",
      "LOSS train 5.158183573232552e-05 valid 7.733047095825896e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.380697545362636e-07\n",
      "  batch 101 loss: 4.63530382849342e-05\n",
      "  batch 201 loss: 4.577711836191156e-05\n",
      "  batch 301 loss: 4.542899543935164e-05\n",
      "  batch 401 loss: 4.58924611882594e-05\n",
      "LOSS train 4.5891951745358656e-05 valid 7.247990288306028e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 7.192569319158792e-07\n",
      "  batch 101 loss: 4.4597014647251856e-05\n",
      "  batch 201 loss: 4.516088482205305e-05\n",
      "  batch 301 loss: 4.620431279931836e-05\n",
      "  batch 401 loss: 4.820039839671608e-05\n",
      "LOSS train 4.622077608640583e-05 valid 6.952035619178787e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 6.405973545042798e-07\n",
      "  batch 101 loss: 4.6523721354674305e-05\n",
      "  batch 201 loss: 4.7827136058344876e-05\n",
      "  batch 301 loss: 4.9102141895218664e-05\n",
      "  batch 401 loss: 4.9566338491331406e-05\n",
      "LOSS train 4.8341492827323123e-05 valid 6.255819607758895e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.860692959278822e-07\n",
      "  batch 101 loss: 4.6130820195742216e-05\n",
      "  batch 201 loss: 4.754702364039076e-05\n",
      "  batch 301 loss: 4.749506328607822e-05\n",
      "  batch 401 loss: 4.644757158985158e-05\n",
      "LOSS train 4.6905849069428155e-05 valid 6.270922312978655e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.7645652405917645e-07\n",
      "  batch 101 loss: 4.219295573591353e-05\n",
      "  batch 201 loss: 4.323507289313966e-05\n",
      "  batch 301 loss: 4.2676827577565744e-05\n",
      "  batch 401 loss: 4.120988008821769e-05\n",
      "LOSS train 4.243346963708566e-05 valid 6.58270510029979e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00010691864416003228\n",
      "  batch 101 loss: 0.07332107401511166\n",
      "  batch 201 loss: 0.0008408364505157806\n",
      "  batch 301 loss: 0.00041678749199490993\n",
      "  batch 401 loss: 0.00021795249267597683\n",
      "LOSS train 0.01692079372519987 valid 0.00020477593352552503\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.2434941385872662e-06\n",
      "  batch 101 loss: 0.00013510805936675752\n",
      "  batch 201 loss: 0.0001416079445516516\n",
      "  batch 301 loss: 0.0001348156846688653\n",
      "  batch 401 loss: 0.000124462766925717\n",
      "LOSS train 0.00013006164536278738 valid 0.00021582641056738794\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0718673951923847e-06\n",
      "  batch 101 loss: 0.00011123093598143896\n",
      "  batch 201 loss: 0.00010538632640418655\n",
      "  batch 301 loss: 9.168603324269498e-05\n",
      "  batch 401 loss: 8.779062828125462e-05\n",
      "LOSS train 9.638754126527164e-05 valid 0.00021264019596856087\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.9719312442466616e-06\n",
      "  batch 101 loss: 8.893854063671825e-05\n",
      "  batch 201 loss: 8.740651777543463e-05\n",
      "  batch 301 loss: 8.659907287892565e-05\n",
      "  batch 401 loss: 8.345228089979174e-05\n",
      "LOSS train 8.467051156784615e-05 valid 0.00020168354967609048\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.86249938653782e-06\n",
      "  batch 101 loss: 7.877217749523879e-05\n",
      "  batch 201 loss: 7.515129739658732e-05\n",
      "  batch 301 loss: 6.944117169950914e-05\n",
      "  batch 401 loss: 6.564860553794461e-05\n",
      "LOSS train 7.08469104844324e-05 valid 0.0001711033401079476\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.436064532957971e-06\n",
      "  batch 101 loss: 6.461885233761677e-05\n",
      "  batch 201 loss: 6.244011580065489e-05\n",
      "  batch 301 loss: 5.924136328076202e-05\n",
      "  batch 401 loss: 5.813543134536303e-05\n",
      "LOSS train 6.0423257126489094e-05 valid 0.00015762598195578903\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2365234326571226e-06\n",
      "  batch 101 loss: 6.0170640788328454e-05\n",
      "  batch 201 loss: 5.944393794266034e-05\n",
      "  batch 301 loss: 5.7815437180579466e-05\n",
      "  batch 401 loss: 5.831001153183024e-05\n",
      "LOSS train 5.872284478061392e-05 valid 0.00015609190450049937\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2080432972870765e-06\n",
      "  batch 101 loss: 6.118038925819746e-05\n",
      "  batch 201 loss: 6.069213685975683e-05\n",
      "  batch 301 loss: 5.9411310715518084e-05\n",
      "  batch 401 loss: 5.986307039052008e-05\n",
      "LOSS train 6.0357017321885054e-05 valid 0.00013354032125789672\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.8562312470749021e-06\n",
      "  batch 101 loss: 5.971240106418918e-05\n",
      "  batch 201 loss: 5.820545870449223e-05\n",
      "  batch 301 loss: 5.618675107143645e-05\n",
      "  batch 401 loss: 5.5312628878709804e-05\n",
      "LOSS train 5.7254435525724654e-05 valid 9.210566349793226e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.14115682663396e-06\n",
      "  batch 101 loss: 5.138593647643574e-05\n",
      "  batch 201 loss: 5.0032558648354096e-05\n",
      "  batch 301 loss: 4.858319535514966e-05\n",
      "  batch 401 loss: 4.799750593463159e-05\n",
      "LOSS train 4.939361413189811e-05 valid 7.505100074922666e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.823342457413673e-07\n",
      "  batch 101 loss: 4.5308867057656244e-05\n",
      "  batch 201 loss: 4.505573917455763e-05\n",
      "  batch 301 loss: 4.512102059862855e-05\n",
      "  batch 401 loss: 4.592668946401091e-05\n",
      "LOSS train 4.5430960355923885e-05 valid 7.213615026557818e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.104238466126845e-07\n",
      "  batch 101 loss: 4.490691806836367e-05\n",
      "  batch 201 loss: 4.5708612950647876e-05\n",
      "  batch 301 loss: 4.6946092772941484e-05\n",
      "  batch 401 loss: 4.841578316927553e-05\n",
      "LOSS train 4.6661671369639854e-05 valid 6.792735803173855e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00038472887128591536\n",
      "  batch 101 loss: 0.07554617604473605\n",
      "  batch 201 loss: 0.0016367010268731974\n",
      "  batch 301 loss: 0.00022874974267324432\n",
      "  batch 401 loss: 0.00012748355829899082\n",
      "LOSS train 0.017599705612693854 valid 0.0001111866658902727\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.921413640957326e-06\n",
      "  batch 101 loss: 0.00011197191538030892\n",
      "  batch 201 loss: 0.00011486701121611987\n",
      "  batch 301 loss: 0.00011250813456854302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.00011062502469712854\n",
      "LOSS train 0.0001101336354542549 valid 0.00019136720220558345\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.7586071519181134e-06\n",
      "  batch 101 loss: 0.00010999076604548464\n",
      "  batch 201 loss: 0.00010891983571127639\n",
      "  batch 301 loss: 0.00010281753465505972\n",
      "  batch 401 loss: 9.921067835193754e-05\n",
      "LOSS train 0.00010228548000687003 valid 0.00023247904027812183\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1963948276825247e-06\n",
      "  batch 101 loss: 9.335280175093886e-05\n",
      "  batch 201 loss: 8.857112697000957e-05\n",
      "  batch 301 loss: 8.157811844284879e-05\n",
      "  batch 401 loss: 7.58086674727565e-05\n",
      "LOSS train 8.272878951383172e-05 valid 0.0001956207415787503\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.70044372882694e-06\n",
      "  batch 101 loss: 7.280175555479218e-05\n",
      "  batch 201 loss: 6.955551137707516e-05\n",
      "  batch 301 loss: 6.488551174925305e-05\n",
      "  batch 401 loss: 6.180415582662135e-05\n",
      "LOSS train 6.612440447487382e-05 valid 0.0001625522127142176\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2797644487582147e-06\n",
      "  batch 101 loss: 6.210737565709223e-05\n",
      "  batch 201 loss: 6.0526322313876336e-05\n",
      "  batch 301 loss: 5.801577044593387e-05\n",
      "  batch 401 loss: 5.7522377466625586e-05\n",
      "LOSS train 5.905386919175058e-05 valid 0.00015797359810676426\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.220499445684254e-06\n",
      "  batch 101 loss: 6.022270412586295e-05\n",
      "  batch 201 loss: 5.987064118812668e-05\n",
      "  batch 301 loss: 5.844858723435209e-05\n",
      "  batch 401 loss: 5.91660214854528e-05\n",
      "LOSS train 5.941347457410425e-05 valid 0.00015109751257114112\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.1208432735875248e-06\n",
      "  batch 101 loss: 6.164278439442228e-05\n",
      "  batch 201 loss: 6.063261290705668e-05\n",
      "  batch 301 loss: 5.89266992597004e-05\n",
      "  batch 401 loss: 5.8745371644590706e-05\n",
      "LOSS train 6.000707608379326e-05 valid 0.00011243726476095617\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.49817846249789e-06\n",
      "  batch 101 loss: 5.617830883238639e-05\n",
      "  batch 201 loss: 5.4470626753300164e-05\n",
      "  batch 301 loss: 5.238103103920366e-05\n",
      "  batch 401 loss: 5.145376658902023e-05\n",
      "LOSS train 5.3469322597471305e-05 valid 8.088011963991448e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 9.117810259340331e-07\n",
      "  batch 101 loss: 4.777956436726072e-05\n",
      "  batch 201 loss: 4.688950584409213e-05\n",
      "  batch 301 loss: 4.617255859216129e-05\n",
      "  batch 401 loss: 4.62487617556917e-05\n",
      "LOSS train 4.6747814456468635e-05 valid 7.313038076972589e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.306875340873375e-07\n",
      "  batch 101 loss: 4.4504885324272436e-05\n",
      "  batch 201 loss: 4.477586444750159e-05\n",
      "  batch 301 loss: 4.55056813984811e-05\n",
      "  batch 401 loss: 4.682582162217841e-05\n",
      "LOSS train 4.555242796730117e-05 valid 7.085825927788392e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 6.769218452973291e-07\n",
      "  batch 101 loss: 4.589959531386967e-05\n",
      "  batch 201 loss: 4.705952092876942e-05\n",
      "  batch 301 loss: 4.847485766447335e-05\n",
      "  batch 401 loss: 4.939324165832204e-05\n",
      "LOSS train 4.7840187314702394e-05 valid 6.43281964585185e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003900444135069847\n",
      "  batch 101 loss: 2.7272136205830613\n",
      "  batch 201 loss: 0.0014687694083841052\n",
      "  batch 301 loss: 0.0005679156389305718\n",
      "  batch 401 loss: 0.000791216211300707\n",
      "LOSS train 0.6164009456562789 valid 0.0014429588336497545\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.248351590125821e-07\n",
      "  batch 101 loss: 0.00017318865762717907\n",
      "  batch 201 loss: 0.00011313102147823884\n",
      "  batch 301 loss: 9.181031563912256e-05\n",
      "  batch 401 loss: 0.000644464375957341\n",
      "LOSS train 0.0002721081626546767 valid 0.0008206249913200736\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.109205910935998e-06\n",
      "  batch 101 loss: 8.887254579462933e-05\n",
      "  batch 201 loss: 0.00010735190305695141\n",
      "  batch 301 loss: 8.437076786549369e-05\n",
      "  batch 401 loss: 0.0022082975122248174\n",
      "LOSS train 0.0006668889718483004 valid 0.0027615416329354048\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.7461153008043766e-06\n",
      "  batch 101 loss: 0.00026715269993928816\n",
      "  batch 201 loss: 0.0002389466382953742\n",
      "  batch 301 loss: 0.00015038137009867113\n",
      "  batch 401 loss: 0.002080276451742975\n",
      "LOSS train 0.0007911356805710705 valid 0.0009916707640513778\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.745426900219172e-06\n",
      "  batch 101 loss: 0.02547710053018818\n",
      "  batch 201 loss: 0.07005495637422428\n",
      "  batch 301 loss: 0.061772749749943616\n",
      "  batch 401 loss: 0.08782693100190954\n",
      "LOSS train 0.06257726394295239 valid 0.04186372831463814\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.00010978343896567822\n",
      "  batch 101 loss: 0.06294756413961294\n",
      "  batch 201 loss: 0.04797561441606377\n",
      "  batch 301 loss: 0.030198362929513677\n",
      "  batch 401 loss: 0.057009409696620425\n",
      "LOSS train 0.04854078188704897 valid 0.0968848168849945\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0006997491419315339\n",
      "  batch 101 loss: 0.07773343645385467\n",
      "  batch 201 loss: 0.03570967407693388\n",
      "  batch 301 loss: 0.014019408609892708\n",
      "  batch 401 loss: 0.08804425686976174\n",
      "LOSS train 0.0531432434683797 valid 0.014408638700842857\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0005553317070007324\n",
      "  batch 101 loss: 0.143768343795673\n",
      "  batch 201 loss: 0.12360804588010069\n",
      "  batch 301 loss: 0.01980722598993452\n",
      "  batch 401 loss: 0.02522740819840692\n",
      "LOSS train 0.07337325224914361 valid 0.035584840923547745\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.0015552327036857604\n",
      "  batch 101 loss: 0.05512390927731758\n",
      "  batch 201 loss: 0.02653642110672081\n",
      "  batch 301 loss: 0.03891886187469936\n",
      "  batch 401 loss: 0.11045501154803787\n",
      "LOSS train 0.05989002243018303 valid 0.07447387278079987\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.0001799500547349453\n",
      "  batch 101 loss: 0.09784031336428597\n",
      "  batch 201 loss: 0.038062395239830946\n",
      "  batch 301 loss: 0.01564224416288198\n",
      "  batch 401 loss: 0.09635143197374418\n",
      "LOSS train 0.06142801249339471 valid 0.017173297703266144\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.00018050462007522582\n",
      "  batch 101 loss: 0.05720585901173763\n",
      "  batch 201 loss: 0.005991134494106518\n",
      "  batch 301 loss: 0.09311462212761398\n",
      "  batch 401 loss: 0.058847647479269655\n",
      "LOSS train 0.05416110546003452 valid 0.03571729362010956\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.00019393917173147202\n",
      "  batch 101 loss: 0.13299147850077134\n",
      "  batch 201 loss: 0.01096484695619438\n",
      "  batch 301 loss: 0.01394913519907277\n",
      "  batch 401 loss: 0.09595583904534578\n",
      "LOSS train 0.07377119565588364 valid 0.19723941385746002\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0027188196778297423\n",
      "  batch 101 loss: 2.4787907666992397\n",
      "  batch 201 loss: 0.008711752605158836\n",
      "  batch 301 loss: 0.00400264362921007\n",
      "  batch 401 loss: 0.0014993721060454846\n",
      "LOSS train 0.5634230228554054 valid 0.00016334459360223264\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.888229447416961e-06\n",
      "  batch 101 loss: 0.000385816528723808\n",
      "  batch 201 loss: 9.944208100932883e-05\n",
      "  batch 301 loss: 3.6032044317835246e-05\n",
      "  batch 401 loss: 2.062476540231728e-05\n",
      "LOSS train 0.00012645231444188186 valid 4.1677558328956366e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.740014214301482e-07\n",
      "  batch 101 loss: 6.791985542804468e-05\n",
      "  batch 201 loss: 2.159143196877267e-05\n",
      "  batch 301 loss: 1.2844540897276602e-05\n",
      "  batch 401 loss: 4.598388473368686e-05\n",
      "LOSS train 4.0385056515208183e-05 valid 0.00030633475398644805\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.235995245631784e-06\n",
      "  batch 101 loss: 0.00015775720783494762\n",
      "  batch 201 loss: 6.638054687755358e-05\n",
      "  batch 301 loss: 5.4278683960546915e-05\n",
      "  batch 401 loss: 0.00030818252030712757\n",
      "LOSS train 0.016603141340386565 valid 0.3638080656528473\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.0029549747705459595\n",
      "  batch 101 loss: 0.09494636704679578\n",
      "  batch 201 loss: 0.04299550899071619\n",
      "  batch 301 loss: 0.02706046084756963\n",
      "  batch 401 loss: 0.07510516211157664\n",
      "LOSS train 0.0627042325158366 valid 0.005726009141653776\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.577757515013218e-05\n",
      "  batch 101 loss: 0.05208516616141424\n",
      "  batch 201 loss: 0.03785629813792184\n",
      "  batch 301 loss: 0.03188077161903493\n",
      "  batch 401 loss: 0.12824820500332862\n",
      "LOSS train 0.06466446215634783 valid 0.2858789265155792\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0025901132822036743\n",
      "  batch 101 loss: 0.08703749292530119\n",
      "  batch 201 loss: 0.032449390311958266\n",
      "  batch 301 loss: 0.010898131203721278\n",
      "  batch 401 loss: 0.056026208614348434\n",
      "LOSS train 0.04999982224331651 valid 1.1414936780929565\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.010935667753219604\n",
      "  batch 101 loss: 0.48892165372148155\n",
      "  batch 201 loss: 0.012225988067220896\n",
      "  batch 301 loss: 0.00390166339580901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.0008006428012595279\n",
      "LOSS train 0.11669667115687675 valid 0.00034504756331443787\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.9900403246283534e-06\n",
      "  batch 101 loss: 0.0010309500389848836\n",
      "  batch 201 loss: 0.00023774133927872753\n",
      "  batch 301 loss: 0.01079593118007324\n",
      "  batch 401 loss: 0.1921135709132068\n",
      "LOSS train 0.049252627795684116 valid 0.07408716529607773\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.000710124745965004\n",
      "  batch 101 loss: 0.04050446670968086\n",
      "  batch 201 loss: 0.019105831198394298\n",
      "  batch 301 loss: 0.03587799486529548\n",
      "  batch 401 loss: 0.11147653413936495\n",
      "LOSS train 0.05038737352797131 valid 0.013959981501102448\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.00021850652992725373\n",
      "  batch 101 loss: 0.0828969986201264\n",
      "  batch 201 loss: 0.03327949234982953\n",
      "  batch 301 loss: 0.04226298462599516\n",
      "  batch 401 loss: 0.11287274647969753\n",
      "LOSS train 0.06801299031184772 valid 0.20399019122123718\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.00189706951379776\n",
      "  batch 101 loss: 0.06446689781732858\n",
      "  batch 201 loss: 0.015010663317516446\n",
      "  batch 301 loss: 0.041225409667240455\n",
      "  batch 401 loss: 0.07928363928105682\n",
      "LOSS train 0.05266967577293187 valid 0.020927593111991882\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00014600838534533977\n",
      "  batch 101 loss: 2.7748848143033684\n",
      "  batch 201 loss: 0.011088924279902131\n",
      "  batch 301 loss: 0.002264275021443609\n",
      "  batch 401 loss: 0.00046017458575079215\n",
      "LOSS train 0.6295480051343204 valid 0.00018822751007974148\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.859713033307344e-06\n",
      "  batch 101 loss: 0.0001474586344193085\n",
      "  batch 201 loss: 3.898446118910215e-05\n",
      "  batch 301 loss: 2.4719713669583142e-05\n",
      "  batch 401 loss: 2.5514592980471207e-05\n",
      "LOSS train 5.740539000247773e-05 valid 9.041947487276047e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.4478508091997356e-06\n",
      "  batch 101 loss: 2.9500549535441677e-05\n",
      "  batch 201 loss: 3.936161323053966e-05\n",
      "  batch 301 loss: 4.3824238787237846e-05\n",
      "  batch 401 loss: 0.0002583619419874594\n",
      "LOSS train 0.00013234073138935004 valid 0.0003064011107198894\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.41974470554851e-06\n",
      "  batch 101 loss: 0.0007619783121845103\n",
      "  batch 201 loss: 7.525544342570356e-05\n",
      "  batch 301 loss: 9.040331175128812e-05\n",
      "  batch 401 loss: 0.00019738700302696088\n",
      "LOSS train 0.0003802479663111546 valid 0.023420143872499466\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.00021867148578166962\n",
      "  batch 101 loss: 0.05245782808749937\n",
      "  batch 201 loss: 0.061733438228257\n",
      "  batch 301 loss: 0.12555041818879545\n",
      "  batch 401 loss: 0.15866447690874338\n",
      "LOSS train 0.09736059143961645 valid 0.006942802108824253\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0002608793042600155\n",
      "  batch 101 loss: 0.024035350908525287\n",
      "  batch 201 loss: 0.006148359969956801\n",
      "  batch 301 loss: 0.028286556807579473\n",
      "  batch 401 loss: 0.10346563627943396\n",
      "LOSS train 0.045267024709218714 valid 0.05525696650147438\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.00019012998789548873\n",
      "  batch 101 loss: 0.27822541607543827\n",
      "  batch 201 loss: 0.05239990218542516\n",
      "  batch 301 loss: 0.01793155732098967\n",
      "  batch 401 loss: 0.016964120785705746\n",
      "LOSS train 0.08323230708496089 valid 0.021103577688336372\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.00014933258295059205\n",
      "  batch 101 loss: 0.02536841239547357\n",
      "  batch 201 loss: 0.11999922554939985\n",
      "  batch 301 loss: 0.0436401680810377\n",
      "  batch 401 loss: 0.10172668929211795\n",
      "LOSS train 0.07444709288450399 valid 0.0020320392213761806\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.00012274054810404778\n",
      "  batch 101 loss: 0.029941164870979264\n",
      "  batch 201 loss: 0.016293860327568837\n",
      "  batch 301 loss: 0.09103976923506707\n",
      "  batch 401 loss: 0.11643233480863273\n",
      "LOSS train 0.07126590571800903 valid 0.015515511855483055\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.0005851195380091667\n",
      "  batch 101 loss: 0.08307832288555801\n",
      "  batch 201 loss: 0.018091259161010384\n",
      "  batch 301 loss: 0.02710602015373297\n",
      "  batch 401 loss: 0.08063603999093175\n",
      "LOSS train 0.05193111196220232 valid 0.22328104078769684\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.0023532596230506895\n",
      "  batch 101 loss: 0.13955003981478512\n",
      "  batch 201 loss: 0.0698910350818187\n",
      "  batch 301 loss: 0.02854733229614794\n",
      "  batch 401 loss: 0.020656969322590157\n",
      "LOSS train 0.06260653828406332 valid 0.06893450766801834\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.0008622181415557862\n",
      "  batch 101 loss: 0.1683589337579906\n",
      "  batch 201 loss: 0.13906629315577448\n",
      "  batch 301 loss: 0.07194676883053035\n",
      "  batch 401 loss: 0.10591310418210924\n",
      "LOSS train 0.11827977999737886 valid 0.02374875359237194\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0010137921571731568\n",
      "  batch 101 loss: 2.7013376840204\n",
      "  batch 201 loss: 0.009323494711425155\n",
      "  batch 301 loss: 0.0004942947554445709\n",
      "  batch 401 loss: 6.447482345720346e-05\n",
      "LOSS train 0.6122480947130127 valid 6.447012128774077e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.0252605210989715e-06\n",
      "  batch 101 loss: 9.187544339511077e-05\n",
      "  batch 201 loss: 5.1056017000519204e-05\n",
      "  batch 301 loss: 3.8089957247393616e-05\n",
      "  batch 401 loss: 3.847872150799958e-05\n",
      "LOSS train 5.7679350384225995e-05 valid 0.00014379383355844766\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.8138127052225172e-06\n",
      "  batch 101 loss: 6.927088516022195e-05\n",
      "  batch 201 loss: 6.187669944665686e-05\n",
      "  batch 301 loss: 9.728161299790372e-05\n",
      "  batch 401 loss: 0.0006428778356712427\n",
      "LOSS train 0.00023886981120615377 valid 0.0003561282064765692\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.0907161999493837e-06\n",
      "  batch 101 loss: 0.0007968298024934484\n",
      "  batch 201 loss: 0.000957244985474972\n",
      "  batch 301 loss: 0.0023518620486720466\n",
      "  batch 401 loss: 0.0066598023852566255\n",
      "LOSS train 0.013974932793956318 valid 3.1386775970458984\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.030140781402587892\n",
      "  batch 101 loss: 0.853330398723483\n",
      "  batch 201 loss: 0.16762098198756575\n",
      "  batch 301 loss: 0.04351380186155438\n",
      "  batch 401 loss: 0.010842798404628411\n",
      "LOSS train 0.24969626504782813 valid 0.00166083755902946\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.794058131054044e-05\n",
      "  batch 101 loss: 0.004393434138037265\n",
      "  batch 201 loss: 0.0012963795887480955\n",
      "  batch 301 loss: 0.000778466335759731\n",
      "  batch 401 loss: 0.003932413179718423\n",
      "LOSS train 0.009307793744234788 valid 0.005242438521236181\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.00045377034693956376\n",
      "  batch 101 loss: 0.1883524478599429\n",
      "  batch 201 loss: 0.2321680174395442\n",
      "  batch 301 loss: 0.10029155567288399\n",
      "  batch 401 loss: 0.16043480006977917\n",
      "LOSS train 0.16071234489990815 valid 0.12845826148986816\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0017375603318214416\n",
      "  batch 101 loss: 0.2519884025491774\n",
      "  batch 201 loss: 0.5162849210202693\n",
      "  batch 301 loss: 0.3897819161787629\n",
      "  batch 401 loss: 0.12841300110332668\n",
      "LOSS train 0.3018375857171874 valid 0.16612280905246735\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.0018953171372413636\n",
      "  batch 101 loss: 0.9863785965740681\n",
      "  batch 201 loss: 0.10628420379478484\n",
      "  batch 301 loss: 0.003704318278469145\n",
      "  batch 401 loss: 0.002336233089445159\n",
      "LOSS train 0.24926174893233144 valid 0.14257605373859406\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.001416534036397934\n",
      "  batch 101 loss: 0.059837376275099816\n",
      "  batch 201 loss: 0.0038199963443912564\n",
      "  batch 301 loss: 0.0007757130760001019\n",
      "  batch 401 loss: 0.0003624802092963364\n",
      "LOSS train 0.015019459965102466 valid 0.0003351198974996805\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.280875727999955e-06\n",
      "  batch 101 loss: 0.007375318226404488\n",
      "  batch 201 loss: 0.005319431600510143\n",
      "  batch 301 loss: 0.018301407464314252\n",
      "  batch 401 loss: 0.05599808885715902\n",
      "LOSS train 0.058426712427756586 valid 0.039498284459114075\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.0020675386488437654\n",
      "  batch 101 loss: 0.1361866768449545\n",
      "  batch 201 loss: 0.03652859568595886\n",
      "  batch 301 loss: 0.09171020779758692\n",
      "  batch 401 loss: 0.031283166552893815\n",
      "LOSS train 0.06899729940796409 valid 0.0110938036814332\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00032528989017009734\n",
      "  batch 101 loss: 1.2635964574640093\n",
      "  batch 201 loss: 9.585449767655518e-05\n",
      "  batch 301 loss: 7.891061087320849e-05\n",
      "  batch 401 loss: 0.06285828429658068\n",
      "LOSS train 0.3000589320289287 valid 0.0001001036143861711\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8847885005234274e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 8.165104761701514e-05\n",
      "  batch 201 loss: 9.375077190725279e-05\n",
      "  batch 301 loss: 9.781553195580273e-05\n",
      "  batch 401 loss: 0.00010217079245194326\n",
      "LOSS train 9.320847094056896e-05 valid 0.00012871624494437128\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.7783990188036113e-06\n",
      "  batch 101 loss: 0.00010631802297098148\n",
      "  batch 201 loss: 0.00010890973330049292\n",
      "  batch 301 loss: 0.00010937325414772658\n",
      "  batch 401 loss: 0.00010914135870706332\n",
      "LOSS train 0.00010607708834797304 valid 0.00020386312098708004\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.909236936829984e-06\n",
      "  batch 101 loss: 0.00010731324379833041\n",
      "  batch 201 loss: 0.0001053785996350598\n",
      "  batch 301 loss: 9.960081833014556e-05\n",
      "  batch 401 loss: 9.402458099174283e-05\n",
      "LOSS train 9.874639785859039e-05 valid 0.00021988400840200484\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.135845763608813e-06\n",
      "  batch 101 loss: 8.865691377025087e-05\n",
      "  batch 201 loss: 8.430771526434456e-05\n",
      "  batch 301 loss: 7.74154731800536e-05\n",
      "  batch 401 loss: 7.244776057973467e-05\n",
      "LOSS train 7.880416415884557e-05 valid 0.00018438769620843232\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.6284981868229805e-06\n",
      "  batch 101 loss: 6.970694960386936e-05\n",
      "  batch 201 loss: 6.671019688837987e-05\n",
      "  batch 301 loss: 6.241965461299514e-05\n",
      "  batch 401 loss: 6.0178223203593004e-05\n",
      "LOSS train 6.377210720167699e-05 valid 0.00016078479529824108\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.278523606946692e-06\n",
      "  batch 101 loss: 6.098635417544074e-05\n",
      "  batch 201 loss: 5.973651317134454e-05\n",
      "  batch 301 loss: 5.7675573741136075e-05\n",
      "  batch 401 loss: 5.755485085785494e-05\n",
      "LOSS train 5.8637604858240826e-05 valid 0.0001568097504787147\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.218295558122918e-06\n",
      "  batch 101 loss: 6.068175923473973e-05\n",
      "  batch 201 loss: 6.029266980476678e-05\n",
      "  batch 301 loss: 5.911983304940804e-05\n",
      "  batch 401 loss: 5.9754017306374864e-05\n",
      "LOSS train 6.003195886686478e-05 valid 0.00013999150542076677\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.958330540219322e-06\n",
      "  batch 101 loss: 6.047367758014843e-05\n",
      "  batch 201 loss: 5.887321931368206e-05\n",
      "  batch 301 loss: 5.6651319878824326e-05\n",
      "  batch 401 loss: 5.5543622611367024e-05\n",
      "LOSS train 5.7756307881880615e-05 valid 9.149735706159845e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.1295451258774847e-06\n",
      "  batch 101 loss: 5.104920368466992e-05\n",
      "  batch 201 loss: 4.936782625577507e-05\n",
      "  batch 301 loss: 4.775756292303868e-05\n",
      "  batch 401 loss: 4.7134981480212444e-05\n",
      "LOSS train 4.870719873399024e-05 valid 7.36606671125628e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.487437687814236e-07\n",
      "  batch 101 loss: 4.468873627899939e-05\n",
      "  batch 201 loss: 4.472502473845452e-05\n",
      "  batch 301 loss: 4.537721193116795e-05\n",
      "  batch 401 loss: 4.686775945287991e-05\n",
      "LOSS train 4.558225842987581e-05 valid 7.03108889865689e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 6.623943045269698e-07\n",
      "  batch 101 loss: 4.616586075911755e-05\n",
      "  batch 201 loss: 4.771816273290597e-05\n",
      "  batch 301 loss: 4.9128924210037895e-05\n",
      "  batch 401 loss: 4.8993654455387056e-05\n",
      "LOSS train 4.808296907639371e-05 valid 6.163774378364906e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.3347001115325838e-06\n",
      "  batch 101 loss: 0.621823595209371\n",
      "  batch 201 loss: 0.003215017823122253\n",
      "  batch 301 loss: 8.619888111297768e-05\n",
      "  batch 401 loss: 9.754734695547995e-05\n",
      "LOSS train 0.1411426542816938 valid 0.00011989027552772313\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.633505744393915e-06\n",
      "  batch 101 loss: 0.00010501465888410166\n",
      "  batch 201 loss: 0.0001090468494084007\n",
      "  batch 301 loss: 0.00010943907656610463\n",
      "  batch 401 loss: 0.00010752070160492622\n",
      "LOSS train 0.00010505537255459239 valid 0.00021854326769243926\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.117015294265002e-06\n",
      "  batch 101 loss: 0.00010262933244632677\n",
      "  batch 201 loss: 9.794401462130508e-05\n",
      "  batch 301 loss: 8.95151530804128e-05\n",
      "  batch 401 loss: 8.238561264931831e-05\n",
      "LOSS train 9.046877479936555e-05 valid 0.0002016236976487562\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.8772657969966532e-06\n",
      "  batch 101 loss: 7.683290060811032e-05\n",
      "  batch 201 loss: 7.239079067176136e-05\n",
      "  batch 301 loss: 6.652466133346025e-05\n",
      "  batch 401 loss: 6.300953743448189e-05\n",
      "LOSS train 6.83732027936084e-05 valid 0.00016544487152714282\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.3486156715080144e-06\n",
      "  batch 101 loss: 6.250567100977377e-05\n",
      "  batch 201 loss: 6.067385163021299e-05\n",
      "  batch 301 loss: 5.8047382761685636e-05\n",
      "  batch 401 loss: 5.745892785057549e-05\n",
      "LOSS train 5.91815874660196e-05 valid 0.00015687943960074335\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2193542099557815e-06\n",
      "  batch 101 loss: 6.029807877098392e-05\n",
      "  batch 201 loss: 5.988376021377917e-05\n",
      "  batch 301 loss: 5.871823597317416e-05\n",
      "  batch 401 loss: 5.942986690115504e-05\n",
      "LOSS train 5.962060629551631e-05 valid 0.00014601332077290863\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.052448398899287e-06\n",
      "  batch 101 loss: 6.110770060786308e-05\n",
      "  batch 201 loss: 5.9745463257741e-05\n",
      "  batch 301 loss: 5.7661482745174905e-05\n",
      "  batch 401 loss: 5.6768842064798264e-05\n",
      "LOSS train 5.87329083251178e-05 valid 9.678224159870297e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.229460904141888e-06\n",
      "  batch 101 loss: 5.243145914334946e-05\n",
      "  batch 201 loss: 5.052423736970013e-05\n",
      "  batch 301 loss: 4.863976271906267e-05\n",
      "  batch 401 loss: 4.776472735443349e-05\n",
      "LOSS train 4.968719737243346e-05 valid 7.430104597005993e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 7.643762364750729e-07\n",
      "  batch 101 loss: 4.4941690297406464e-05\n",
      "  batch 201 loss: 4.478063557172618e-05\n",
      "  batch 301 loss: 4.520044579976456e-05\n",
      "  batch 401 loss: 4.6517502799474644e-05\n",
      "LOSS train 4.550304226035214e-05 valid 7.08975421730429e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 6.781671254429966e-07\n",
      "  batch 101 loss: 4.587210881311421e-05\n",
      "  batch 201 loss: 4.7368357655841464e-05\n",
      "  batch 301 loss: 4.8970709463560525e-05\n",
      "  batch 401 loss: 4.93372342236853e-05\n",
      "LOSS train 4.800793018309047e-05 valid 6.156216113595292e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.840743036358617e-07\n",
      "  batch 101 loss: 4.443238920160297e-05\n",
      "  batch 201 loss: 4.4635454793819916e-05\n",
      "  batch 301 loss: 4.278725839185427e-05\n",
      "  batch 401 loss: 4.017099408713421e-05\n",
      "LOSS train 4.297682271651123e-05 valid 6.62011225358583e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.0158722034248058e-07\n",
      "  batch 101 loss: 3.687583101324776e-05\n",
      "  batch 201 loss: 3.66696113272269e-05\n",
      "  batch 301 loss: 3.63676164579374e-05\n",
      "  batch 401 loss: 3.585907467765992e-05\n",
      "LOSS train 3.6835239595349754e-05 valid 6.227478297660127e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.7827476151287557e-05\n",
      "  batch 101 loss: 0.6946912496443838\n",
      "  batch 201 loss: 0.00015617387557540497\n",
      "  batch 301 loss: 8.737779730608963e-05\n",
      "  batch 401 loss: 9.570517418637792e-05\n",
      "LOSS train 0.15690642996042475 valid 0.00011195967817911878\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.499529753345996e-06\n",
      "  batch 101 loss: 0.00010332418997904824\n",
      "  batch 201 loss: 0.00010778591779057933\n",
      "  batch 301 loss: 0.00010929910018717237\n",
      "  batch 401 loss: 0.00010881308295978442\n",
      "LOSS train 0.00010484517789131632 valid 0.000211322185350582\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0151833198033274e-06\n",
      "  batch 101 loss: 0.0001053671105387366\n",
      "  batch 201 loss: 0.00010165027572099916\n",
      "  batch 301 loss: 9.38365378516437e-05\n",
      "  batch 401 loss: 8.676283140886198e-05\n",
      "LOSS train 9.409705217563931e-05 valid 0.00020911920000799\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.983973827213049e-06\n",
      "  batch 101 loss: 8.07179147534498e-05\n",
      "  batch 201 loss: 7.596694009407656e-05\n",
      "  batch 301 loss: 6.9509566940269e-05\n",
      "  batch 401 loss: 6.538606086564869e-05\n",
      "LOSS train 7.138690495489642e-05 valid 0.00016995628539007157\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.415976341580972e-06\n",
      "  batch 101 loss: 6.410149083876604e-05\n",
      "  batch 201 loss: 6.187452211861455e-05\n",
      "  batch 301 loss: 5.877166654386201e-05\n",
      "  batch 401 loss: 5.772862120124955e-05\n",
      "LOSS train 5.999597919339597e-05 valid 0.0001571379107190296\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.223283663624898e-06\n",
      "  batch 101 loss: 6.009125013804351e-05\n",
      "  batch 201 loss: 5.9529811629772666e-05\n",
      "  batch 301 loss: 5.8238622223143466e-05\n",
      "  batch 401 loss: 5.88751751899963e-05\n",
      "LOSS train 5.914268177384379e-05 valid 0.00015166500816121697\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.1397000818978996e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 6.147415931764045e-05\n",
      "  batch 201 loss: 6.0521804420829995e-05\n",
      "  batch 301 loss: 5.8764939224147385e-05\n",
      "  batch 401 loss: 5.837416714371102e-05\n",
      "LOSS train 5.977658259159796e-05 valid 0.00010738939454313368\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.4203498722054064e-06\n",
      "  batch 101 loss: 5.490302546604653e-05\n",
      "  batch 201 loss: 5.279996034801116e-05\n",
      "  batch 301 loss: 5.05711453308777e-05\n",
      "  batch 401 loss: 4.934439216583542e-05\n",
      "LOSS train 5.171187650569953e-05 valid 7.631319749634713e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 8.120774873532355e-07\n",
      "  batch 101 loss: 4.581009166820138e-05\n",
      "  batch 201 loss: 4.524291711874184e-05\n",
      "  batch 301 loss: 4.513713737367198e-05\n",
      "  batch 401 loss: 4.598357033898992e-05\n",
      "LOSS train 4.5621524588994595e-05 valid 7.178078521974385e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 7.013439608272165e-07\n",
      "  batch 101 loss: 4.5184376231190984e-05\n",
      "  batch 201 loss: 4.6408234426280613e-05\n",
      "  batch 301 loss: 4.8149309727136824e-05\n",
      "  batch 401 loss: 4.9487201043803e-05\n",
      "LOSS train 4.7503561067913256e-05 valid 6.276929343584925e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.9836784708313645e-07\n",
      "  batch 101 loss: 4.596397760195714e-05\n",
      "  batch 201 loss: 4.6669262516161326e-05\n",
      "  batch 301 loss: 4.52650413859601e-05\n",
      "  batch 401 loss: 4.241880436154588e-05\n",
      "LOSS train 4.499209480319843e-05 valid 6.596724415430799e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.0482976904313546e-07\n",
      "  batch 101 loss: 3.800581221696575e-05\n",
      "  batch 201 loss: 3.7758321443277506e-05\n",
      "  batch 301 loss: 3.7039045645315126e-05\n",
      "  batch 401 loss: 3.613489862317465e-05\n",
      "LOSS train 3.7548157035281304e-05 valid 6.328908784780651e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 9.180446155369281e-05\n",
      "  batch 101 loss: 1.3343589091394188\n",
      "  batch 201 loss: 0.0003084588597266702\n",
      "  batch 301 loss: 8.00165461259894e-05\n",
      "  batch 401 loss: 8.683761182055605e-05\n",
      "LOSS train 0.30134584190892266 valid 8.483847341267392e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.972602856578306e-07\n",
      "  batch 101 loss: 9.382979465726748e-05\n",
      "  batch 201 loss: 9.848336944742186e-05\n",
      "  batch 301 loss: 0.00010244994902450344\n",
      "  batch 401 loss: 0.00010630560473487094\n",
      "LOSS train 9.912391505235512e-05 valid 0.00015405949670821428\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1763748372904956e-06\n",
      "  batch 101 loss: 0.00010908778317229917\n",
      "  batch 201 loss: 0.00011044098849254169\n",
      "  batch 301 loss: 0.00010849532281753226\n",
      "  batch 401 loss: 0.00010557382939964555\n",
      "LOSS train 0.0001055942834816444 valid 0.00022112857550382614\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.153305733576417e-06\n",
      "  batch 101 loss: 0.00010115718650695271\n",
      "  batch 201 loss: 9.731626923951352e-05\n",
      "  batch 301 loss: 8.994770836494581e-05\n",
      "  batch 401 loss: 8.374617627794123e-05\n",
      "LOSS train 9.050495245512608e-05 valid 0.00020505685824900866\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.9262446332722902e-06\n",
      "  batch 101 loss: 7.887942529009706e-05\n",
      "  batch 201 loss: 7.47873524392162e-05\n",
      "  batch 301 loss: 6.889870905865792e-05\n",
      "  batch 401 loss: 6.51616737002314e-05\n",
      "LOSS train 7.05003562714299e-05 valid 0.0001699076674412936\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.4152526748366653e-06\n",
      "  batch 101 loss: 6.41511458320565e-05\n",
      "  batch 201 loss: 6.202583962021891e-05\n",
      "  batch 301 loss: 5.8939918185387794e-05\n",
      "  batch 401 loss: 5.784903091921478e-05\n",
      "LOSS train 6.009673991465746e-05 valid 0.00015731962048448622\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2260440164245663e-06\n",
      "  batch 101 loss: 6.0082178900984215e-05\n",
      "  batch 201 loss: 5.9457682081074384e-05\n",
      "  batch 301 loss: 5.808951375854576e-05\n",
      "  batch 401 loss: 5.8645299557724687e-05\n",
      "LOSS train 5.898459294278449e-05 valid 0.00015335509669966996\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.1656035096384587e-06\n",
      "  batch 101 loss: 6.148596108687343e-05\n",
      "  batch 201 loss: 6.070287249144712e-05\n",
      "  batch 301 loss: 5.910691744986707e-05\n",
      "  batch 401 loss: 5.8979629340001335e-05\n",
      "LOSS train 6.0098235941183596e-05 valid 0.00011360401549609378\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.5276423073373735e-06\n",
      "  batch 101 loss: 5.619916704063144e-05\n",
      "  batch 201 loss: 5.410149620047378e-05\n",
      "  batch 301 loss: 5.1777181635088706e-05\n",
      "  batch 401 loss: 5.044391590672604e-05\n",
      "LOSS train 5.292999212127042e-05 valid 7.808035297784954e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 8.52453667903319e-07\n",
      "  batch 101 loss: 4.6554010691863825e-05\n",
      "  batch 201 loss: 4.5755498667858774e-05\n",
      "  batch 301 loss: 4.5331974138775874e-05\n",
      "  batch 401 loss: 4.586772074389955e-05\n",
      "LOSS train 4.5913103338606056e-05 valid 7.211927731987089e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.100618677213788e-07\n",
      "  batch 101 loss: 4.48306334044446e-05\n",
      "  batch 201 loss: 4.582084562400723e-05\n",
      "  batch 301 loss: 4.7455353583245594e-05\n",
      "  batch 401 loss: 4.9137582104208376e-05\n",
      "LOSS train 4.7031432338374896e-05 valid 6.441943696700037e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.748272112919949e-07\n",
      "  batch 101 loss: 4.655150862106438e-05\n",
      "  batch 201 loss: 4.761017719602023e-05\n",
      "  batch 301 loss: 4.679412946387629e-05\n",
      "  batch 401 loss: 4.410483704361923e-05\n",
      "LOSS train 4.617295185684913e-05 valid 6.529120582854375e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00215207502245903\n",
      "  batch 101 loss: 0.028157595085704087\n",
      "  batch 201 loss: 7.928512722742198e-05\n",
      "  batch 301 loss: 6.0120975856534645e-05\n",
      "  batch 401 loss: 5.68704740061321e-05\n",
      "LOSS train 0.006891159344901138 valid 0.0001534172479296103\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.1665541862603277e-06\n",
      "  batch 101 loss: 6.1080192612053e-05\n",
      "  batch 201 loss: 5.95608473855691e-05\n",
      "  batch 301 loss: 5.6297088947872e-05\n",
      "  batch 401 loss: 5.346953897628737e-05\n",
      "LOSS train 5.7258377624906476e-05 valid 8.010353485587984e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.972359501058236e-07\n",
      "  batch 101 loss: 4.6980971320635944e-05\n",
      "  batch 201 loss: 4.547932440971181e-05\n",
      "  batch 301 loss: 4.510774537152429e-05\n",
      "  batch 401 loss: 4.624399822603209e-05\n",
      "LOSS train 4.605701492211386e-05 valid 7.077483314787969e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.748938176315278e-07\n",
      "  batch 101 loss: 4.588821565505441e-05\n",
      "  batch 201 loss: 4.7635495652968986e-05\n",
      "  batch 301 loss: 4.913732831610673e-05\n",
      "  batch 401 loss: 4.833192177102319e-05\n",
      "LOSS train 4.78008365038157e-05 valid 6.245235272217542e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.8678110791370273e-07\n",
      "  batch 101 loss: 4.207845587558268e-05\n",
      "  batch 201 loss: 4.152889395641069e-05\n",
      "  batch 301 loss: 3.955686954839166e-05\n",
      "  batch 401 loss: 3.7536281397763104e-05\n",
      "LOSS train 4.026752055380012e-05 valid 6.506111094495282e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.1893371265614405e-07\n",
      "  batch 101 loss: 3.5830845559701176e-05\n",
      "  batch 201 loss: 3.5698629655769306e-05\n",
      "  batch 301 loss: 3.6073487918883985e-05\n",
      "  batch 401 loss: 3.628497120729435e-05\n",
      "LOSS train 3.6461530621138624e-05 valid 6.190250860527158e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.3772343158489096e-07\n",
      "  batch 101 loss: 3.7694711382414424e-05\n",
      "  batch 201 loss: 3.8061278738723557e-05\n",
      "  batch 301 loss: 4.048037463576293e-05\n",
      "  batch 401 loss: 4.269635746169342e-05\n",
      "LOSS train 4.0398756815968613e-05 valid 8.417510980507359e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.835941455094143e-07\n",
      "  batch 101 loss: 4.888860835109199e-05\n",
      "  batch 201 loss: 5.10344210753999e-05\n",
      "  batch 301 loss: 5.757092105426409e-05\n",
      "  batch 401 loss: 6.044413782689162e-05\n",
      "LOSS train 5.5122795709266724e-05 valid 0.00012156418961239979\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.6612984472885728e-06\n",
      "  batch 101 loss: 6.29566038151097e-05\n",
      "  batch 201 loss: 6.307698874479683e-05\n",
      "  batch 301 loss: 6.138895311948999e-05\n",
      "  batch 401 loss: 5.670880561126523e-05\n",
      "LOSS train 6.0444025892623114e-05 valid 7.367882790276781e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 7.49190803617239e-07\n",
      "  batch 101 loss: 5.051708493169826e-05\n",
      "  batch 201 loss: 5.2301926328937045e-05\n",
      "  batch 301 loss: 5.157572682492173e-05\n",
      "  batch 401 loss: 5.375202124014322e-05\n",
      "LOSS train 5.213063872852279e-05 valid 6.301182293100283e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.1145460272673517e-07\n",
      "  batch 101 loss: 5.298807401516115e-05\n",
      "  batch 201 loss: 5.810949628198614e-05\n",
      "  batch 301 loss: 5.813791037724059e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 6.061480666176067e-05\n",
      "LOSS train 5.760761777465112e-05 valid 6.783900607842952e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 8.228176739066839e-08\n",
      "  batch 101 loss: 5.258763480242124e-05\n",
      "  batch 201 loss: 5.618316052732552e-05\n",
      "  batch 301 loss: 5.37592381206764e-05\n",
      "  batch 401 loss: 5.194403473609555e-05\n",
      "LOSS train 5.353173495671093e-05 valid 7.032733265077695e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 5.2067753858864305e-05\n",
      "  batch 101 loss: 0.15896563101487915\n",
      "  batch 201 loss: 0.00010935055334925891\n",
      "  batch 301 loss: 0.00010885848143971089\n",
      "  batch 401 loss: 0.00010155803099792137\n",
      "LOSS train 0.035974357389351896 valid 0.00022300965792965144\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.1796589610166848e-06\n",
      "  batch 101 loss: 8.646725795756538e-05\n",
      "  batch 201 loss: 7.645181149428026e-05\n",
      "  batch 301 loss: 6.703950400833491e-05\n",
      "  batch 401 loss: 6.190568297597564e-05\n",
      "LOSS train 7.134099862492253e-05 valid 0.00016172835603356361\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.2927603276912124e-06\n",
      "  batch 101 loss: 6.109648657584899e-05\n",
      "  batch 201 loss: 5.9498355611964374e-05\n",
      "  batch 301 loss: 5.754679553774622e-05\n",
      "  batch 401 loss: 5.786723340975186e-05\n",
      "LOSS train 5.8816286552864994e-05 valid 0.00015504939074162394\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.1914880198892206e-06\n",
      "  batch 101 loss: 6.129207407212789e-05\n",
      "  batch 201 loss: 6.065426256270712e-05\n",
      "  batch 301 loss: 5.9090121987424026e-05\n",
      "  batch 401 loss: 5.890568066263313e-05\n",
      "LOSS train 6.001473563122349e-05 valid 0.00011111880303360522\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.4850779552944004e-06\n",
      "  batch 101 loss: 5.550928617594764e-05\n",
      "  batch 201 loss: 5.30411637259931e-05\n",
      "  batch 301 loss: 5.0537234702119346e-05\n",
      "  batch 401 loss: 4.9122457608064e-05\n",
      "LOSS train 5.183330360398306e-05 valid 7.570379239041358e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.978404028108344e-07\n",
      "  batch 101 loss: 4.552266425818629e-05\n",
      "  batch 201 loss: 4.5021392995181483e-05\n",
      "  batch 301 loss: 4.510530954718206e-05\n",
      "  batch 401 loss: 4.619583676429784e-05\n",
      "LOSS train 4.557114286570969e-05 valid 7.133514736779034e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.897319690324366e-07\n",
      "  batch 101 loss: 4.556360479909927e-05\n",
      "  batch 201 loss: 4.700769030364427e-05\n",
      "  batch 301 loss: 4.8751001302775875e-05\n",
      "  batch 401 loss: 4.9478602333010715e-05\n",
      "LOSS train 4.785306169268044e-05 valid 6.167672836454585e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.111508340225555e-07\n",
      "  batch 101 loss: 4.485652300218135e-05\n",
      "  batch 201 loss: 4.509675746248831e-05\n",
      "  batch 301 loss: 4.3217781311568614e-05\n",
      "  batch 401 loss: 4.0472952373988843e-05\n",
      "LOSS train 4.336205356545532e-05 valid 6.622245564358309e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.0129871952813118e-07\n",
      "  batch 101 loss: 3.698282284673837e-05\n",
      "  batch 201 loss: 3.6754241357357386e-05\n",
      "  batch 301 loss: 3.6405560996968234e-05\n",
      "  batch 401 loss: 3.5866585767792004e-05\n",
      "LOSS train 3.6888241164749476e-05 valid 6.231466977624223e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.930841426656116e-07\n",
      "  batch 101 loss: 3.611486120121299e-05\n",
      "  batch 201 loss: 3.6072789205832124e-05\n",
      "  batch 301 loss: 3.7241660883182704e-05\n",
      "  batch 401 loss: 3.8191149677828665e-05\n",
      "LOSS train 3.743946955061763e-05 valid 6.767470767954364e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 5.867402796866372e-07\n",
      "  batch 101 loss: 4.1164306508107985e-05\n",
      "  batch 201 loss: 4.220716091936083e-05\n",
      "  batch 301 loss: 4.634847350644122e-05\n",
      "  batch 401 loss: 4.978355013548707e-05\n",
      "LOSS train 4.5663794168085094e-05 valid 0.00010792381362989545\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.4296907465904952e-06\n",
      "  batch 101 loss: 5.7924386743053445e-05\n",
      "  batch 201 loss: 5.958783701046855e-05\n",
      "  batch 301 loss: 6.457423666972773e-05\n",
      "  batch 401 loss: 6.315705487963897e-05\n",
      "LOSS train 6.134616566824912e-05 valid 0.00010028170072473586\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00011253759264945985\n",
      "  batch 101 loss: 0.0043760723447360305\n",
      "  batch 201 loss: 4.912223093640477e-05\n",
      "  batch 301 loss: 4.614647073708511e-05\n",
      "  batch 401 loss: 4.8658710389730684e-05\n",
      "LOSS train 0.0010502040701248826 valid 6.526739889523014e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1548183465492911e-07\n",
      "  batch 101 loss: 3.845333468433409e-05\n",
      "  batch 201 loss: 3.6568916511896534e-05\n",
      "  batch 301 loss: 3.6104945477006825e-05\n",
      "  batch 401 loss: 3.651979669626826e-05\n",
      "LOSS train 3.7340061655922235e-05 valid 6.339717947412282e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.305661059333943e-07\n",
      "  batch 101 loss: 3.9301296060898496e-05\n",
      "  batch 201 loss: 4.127540450213018e-05\n",
      "  batch 301 loss: 4.678529441150658e-05\n",
      "  batch 401 loss: 5.208037107195196e-05\n",
      "LOSS train 4.585849079752422e-05 valid 0.00011747704411391169\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.593153428984806e-06\n",
      "  batch 101 loss: 6.15055801449671e-05\n",
      "  batch 201 loss: 6.30616392754746e-05\n",
      "  batch 301 loss: 6.424044415609842e-05\n",
      "  batch 401 loss: 5.916484491649498e-05\n",
      "LOSS train 6.146402136554782e-05 valid 7.703436858719215e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.287101081805304e-07\n",
      "  batch 101 loss: 5.148237125240485e-05\n",
      "  batch 201 loss: 5.260881546632845e-05\n",
      "  batch 301 loss: 5.1511493714997415e-05\n",
      "  batch 401 loss: 5.368677561136792e-05\n",
      "LOSS train 5.241709703359735e-05 valid 6.284989649429917e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 4.0282451664097606e-07\n",
      "  batch 101 loss: 5.318785514845104e-05\n",
      "  batch 201 loss: 5.845213778968628e-05\n",
      "  batch 301 loss: 5.836228818481004e-05\n",
      "  batch 401 loss: 6.0454174419533044e-05\n",
      "LOSS train 5.7739874803829044e-05 valid 6.937347643543035e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.823274816269986e-08\n",
      "  batch 101 loss: 5.173691317907014e-05\n",
      "  batch 201 loss: 5.485223488818747e-05\n",
      "  batch 301 loss: 5.223261438970894e-05\n",
      "  batch 401 loss: 5.006767406655399e-05\n",
      "LOSS train 5.214093641408434e-05 valid 6.810827471781522e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.957624802656937e-08\n",
      "  batch 101 loss: 4.345190711660507e-05\n",
      "  batch 201 loss: 4.436085119152722e-05\n",
      "  batch 301 loss: 4.415417770189833e-05\n",
      "  batch 401 loss: 4.377884142172661e-05\n",
      "LOSS train 4.421176589463895e-05 valid 6.266385753406212e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.923483018297702e-07\n",
      "  batch 101 loss: 4.322533723836841e-05\n",
      "  batch 201 loss: 4.365563677907858e-05\n",
      "  batch 301 loss: 4.4897205174834195e-05\n",
      "  batch 401 loss: 4.5790302315822373e-05\n",
      "LOSS train 4.460347949744824e-05 valid 6.942953768884763e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 6.380498234648257e-07\n",
      "  batch 101 loss: 4.4329561220024516e-05\n",
      "  batch 201 loss: 4.456901077674047e-05\n",
      "  batch 301 loss: 4.478925679507029e-05\n",
      "  batch 401 loss: 4.619088873965893e-05\n",
      "LOSS train 4.510536098255462e-05 valid 6.743752601323649e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 5.794515527668409e-07\n",
      "  batch 101 loss: 4.4273049002470085e-05\n",
      "  batch 201 loss: 4.474384893057959e-05\n",
      "  batch 301 loss: 4.5334631256110924e-05\n",
      "  batch 401 loss: 4.7189186667537796e-05\n",
      "LOSS train 4.55631818199197e-05 valid 6.622233922826126e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 5.40381734026596e-07\n",
      "  batch 101 loss: 4.535791340458673e-05\n",
      "  batch 201 loss: 4.569067982544084e-05\n",
      "  batch 301 loss: 4.649538789294638e-05\n",
      "  batch 401 loss: 4.8379504118543085e-05\n",
      "LOSS train 4.664602920655561e-05 valid 6.58984572510235e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0011342687159776688\n",
      "  batch 101 loss: 0.15214711602788156\n",
      "  batch 201 loss: 6.783978831435888e-05\n",
      "  batch 301 loss: 0.00010080437400574737\n",
      "  batch 401 loss: 6.887513094511633e-05\n",
      "LOSS train 0.03465932398032989 valid 0.00016943983791861683\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.408287255093455e-06\n",
      "  batch 101 loss: 6.282894417779517e-05\n",
      "  batch 201 loss: 5.971807903094373e-05\n",
      "  batch 301 loss: 5.740565838891598e-05\n",
      "  batch 401 loss: 5.7948563549530265e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 5.936141215290492e-05 valid 0.00015254062600433826\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1531301899813117e-06\n",
      "  batch 101 loss: 6.135465433018794e-05\n",
      "  batch 201 loss: 6.0087243613224926e-05\n",
      "  batch 301 loss: 5.7744372558659055e-05\n",
      "  batch 401 loss: 5.6334771429646934e-05\n",
      "LOSS train 5.872794747853601e-05 valid 9.161820344161242e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.1318739416310564e-06\n",
      "  batch 101 loss: 5.075397383734526e-05\n",
      "  batch 201 loss: 4.8524930960525126e-05\n",
      "  batch 301 loss: 4.677828318790489e-05\n",
      "  batch 401 loss: 4.6311557139802064e-05\n",
      "LOSS train 4.798412204656978e-05 valid 7.260379788931459e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.223947613965719e-07\n",
      "  batch 101 loss: 4.4474546775745694e-05\n",
      "  batch 201 loss: 4.509445555413549e-05\n",
      "  batch 301 loss: 4.655274324534275e-05\n",
      "  batch 401 loss: 4.851761177064873e-05\n",
      "LOSS train 4.6404527191933586e-05 valid 6.60617952235043e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.349710409063846e-07\n",
      "  batch 101 loss: 4.6755233248063634e-05\n",
      "  batch 201 loss: 4.799124826774914e-05\n",
      "  batch 301 loss: 4.7465689614796244e-05\n",
      "  batch 401 loss: 4.4737584411222996e-05\n",
      "LOSS train 4.66421446109077e-05 valid 6.50754664093256e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1868808542203623e-07\n",
      "  batch 101 loss: 3.9246684273166466e-05\n",
      "  batch 201 loss: 3.888839810542777e-05\n",
      "  batch 301 loss: 3.7749027243307864e-05\n",
      "  batch 401 loss: 3.6498248006751056e-05\n",
      "LOSS train 3.833734452261024e-05 valid 6.395018135663122e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.4081305380386766e-07\n",
      "  batch 101 loss: 3.573633371814822e-05\n",
      "  batch 201 loss: 3.564584992048481e-05\n",
      "  batch 301 loss: 3.631674049245248e-05\n",
      "  batch 401 loss: 3.677958495714506e-05\n",
      "LOSS train 3.6627261342272266e-05 valid 6.306143768597394e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.1402126953471453e-07\n",
      "  batch 101 loss: 3.860372052258754e-05\n",
      "  batch 201 loss: 3.913521666959241e-05\n",
      "  batch 301 loss: 4.2007469272107304e-05\n",
      "  batch 401 loss: 4.45783605852057e-05\n",
      "LOSS train 4.178590056302192e-05 valid 9.073793626157567e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.1148556950502098e-06\n",
      "  batch 101 loss: 5.15521419420395e-05\n",
      "  batch 201 loss: 5.364342798742428e-05\n",
      "  batch 301 loss: 6.0146363365021215e-05\n",
      "  batch 401 loss: 6.198145806905586e-05\n",
      "LOSS train 5.7324071798964e-05 valid 0.00011759233893826604\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.595088542671874e-06\n",
      "  batch 101 loss: 6.218724724931235e-05\n",
      "  batch 201 loss: 6.213615502929314e-05\n",
      "  batch 301 loss: 5.961490170079742e-05\n",
      "  batch 401 loss: 5.5301281091715284e-05\n",
      "LOSS train 5.922183325815517e-05 valid 7.15853602741845e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 6.962721090530977e-07\n",
      "  batch 101 loss: 4.9885354130765335e-05\n",
      "  batch 201 loss: 5.2125050244740124e-05\n",
      "  batch 301 loss: 5.1884019502494996e-05\n",
      "  batch 401 loss: 5.4610309487088674e-05\n",
      "LOSS train 5.228943180546567e-05 valid 6.236845365492627e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00027041492983698846\n",
      "  batch 101 loss: 20.000009047072236\n",
      "  batch 201 loss: 0.0021011693606851622\n",
      "  batch 301 loss: 0.0004903800473402953\n",
      "  batch 401 loss: 9.76018051369465e-05\n",
      "LOSS train 4.5153506568926485 valid 0.000408941472414881\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.64088244875893e-06\n",
      "  batch 101 loss: 7.983754765518825e-05\n",
      "  batch 201 loss: 6.065784438760602e-05\n",
      "  batch 301 loss: 5.362831366710452e-05\n",
      "  batch 401 loss: 4.812332086657989e-05\n",
      "LOSS train 6.060317267485278e-05 valid 9.135000436799601e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5247922419803216e-07\n",
      "  batch 101 loss: 4.7953366147339695e-05\n",
      "  batch 201 loss: 3.805017194508764e-05\n",
      "  batch 301 loss: 2.9550456379183742e-05\n",
      "  batch 401 loss: 4.415905826363087e-05\n",
      "LOSS train 4.647588633010968e-05 valid 0.0001630135375307873\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.09065757796634e-07\n",
      "  batch 101 loss: 0.0001555173305553126\n",
      "  batch 201 loss: 5.866694039468712e-05\n",
      "  batch 301 loss: 4.280356060007762e-05\n",
      "  batch 401 loss: 0.0001637483521346894\n",
      "LOSS train 0.00010897350138776894 valid 0.0001462676009396091\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.0380662539973857e-07\n",
      "  batch 101 loss: 8.592308611582667e-05\n",
      "  batch 201 loss: 0.027688167696115613\n",
      "  batch 301 loss: 4.659770514876982e-05\n",
      "  batch 401 loss: 4.0168958340132124e-05\n",
      "LOSS train 0.006293321022700226 valid 6.618569750571623e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0179659511777572e-07\n",
      "  batch 101 loss: 3.681675521306715e-05\n",
      "  batch 201 loss: 3.651455773649559e-05\n",
      "  batch 301 loss: 3.626181414318808e-05\n",
      "  batch 401 loss: 3.591608521105627e-05\n",
      "LOSS train 3.6790142617403454e-05 valid 6.169851985760033e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.3681939637754112e-07\n",
      "  batch 101 loss: 3.6638898216097004e-05\n",
      "  batch 201 loss: 3.645514906281733e-05\n",
      "  batch 301 loss: 3.777708903669463e-05\n",
      "  batch 401 loss: 3.896414428851358e-05\n",
      "LOSS train 3.795481887833841e-05 valid 7.21724791219458e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.114249456208199e-07\n",
      "  batch 101 loss: 4.2384018128132085e-05\n",
      "  batch 201 loss: 4.3557257832276264e-05\n",
      "  batch 301 loss: 4.8110968772903104e-05\n",
      "  batch 401 loss: 5.1707080117182614e-05\n",
      "LOSS train 4.721754237360835e-05 valid 0.00011308977991575375\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.518870412837714e-06\n",
      "  batch 101 loss: 5.956468280921001e-05\n",
      "  batch 201 loss: 6.141133789981268e-05\n",
      "  batch 301 loss: 6.626555336424644e-05\n",
      "  batch 401 loss: 6.500330152846345e-05\n",
      "LOSS train 6.31606310473031e-05 valid 9.978477464755997e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.2846580648329109e-06\n",
      "  batch 101 loss: 6.0887133454343715e-05\n",
      "  batch 201 loss: 6.176701320157463e-05\n",
      "  batch 301 loss: 5.867599167686421e-05\n",
      "  batch 401 loss: 5.48197925400018e-05\n",
      "LOSS train 5.844463888491173e-05 valid 6.666941771982238e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 5.551226058742032e-07\n",
      "  batch 101 loss: 4.931636220476321e-05\n",
      "  batch 201 loss: 5.163372104675546e-05\n",
      "  batch 301 loss: 5.088923324422012e-05\n",
      "  batch 401 loss: 5.281744094929764e-05\n",
      "LOSS train 5.1223486871371955e-05 valid 6.162117642816156e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.013888272107579e-07\n",
      "  batch 101 loss: 5.107568212736169e-05\n",
      "  batch 201 loss: 5.5810660029465e-05\n",
      "  batch 301 loss: 5.553740250206829e-05\n",
      "  batch 401 loss: 5.7684445200720804e-05\n",
      "LOSS train 5.5180760887735736e-05 valid 6.884448521304876e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.0855845175683498e-05\n",
      "  batch 101 loss: 14.929713103888208\n",
      "  batch 201 loss: 0.004577546987566166\n",
      "  batch 301 loss: 0.0022528722276911137\n",
      "  batch 401 loss: 0.14027707463363184\n",
      "LOSS train 3.4035021576409057 valid 0.00029651689692400396\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.181113326922059e-06\n",
      "  batch 101 loss: 0.00020817700721636356\n",
      "  batch 201 loss: 8.081992875304423e-05\n",
      "  batch 301 loss: 7.00977555334248e-05\n",
      "  batch 401 loss: 7.657595514046988e-05\n",
      "LOSS train 0.00010705211767677328 valid 6.37907869531773e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.485134923015721e-07\n",
      "  batch 101 loss: 7.330131507842453e-05\n",
      "  batch 201 loss: 7.436766731188982e-05\n",
      "  batch 301 loss: 7.535804866165563e-05\n",
      "  batch 401 loss: 9.260617521704262e-05\n",
      "LOSS train 7.883356823701062e-05 valid 6.724653212586418e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.735089871450328e-07\n",
      "  batch 101 loss: 7.918953473563306e-05\n",
      "  batch 201 loss: 8.03913999152428e-05\n",
      "  batch 301 loss: 8.167350975782028e-05\n",
      "  batch 401 loss: 8.385500015720026e-05\n",
      "LOSS train 8.145382898966619e-05 valid 7.428236131090671e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.63923380873166e-07\n",
      "  batch 101 loss: 8.642503283226688e-05\n",
      "  batch 201 loss: 8.797101597338042e-05\n",
      "  batch 301 loss: 8.961943494114167e-05\n",
      "  batch 401 loss: 9.220778431654253e-05\n",
      "LOSS train 8.888774980601311e-05 valid 8.929467730922624e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.0866802040254697e-06\n",
      "  batch 101 loss: 9.5291158636428e-05\n",
      "  batch 201 loss: 0.00010685259964247962\n",
      "  batch 301 loss: 9.860265549605174e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.00010162122659494343\n",
      "LOSS train 9.957040350229945e-05 valid 0.00011964223813265562\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.6293732915073632e-06\n",
      "  batch 101 loss: 0.00010450898019030319\n",
      "  batch 201 loss: 0.00010642933389021892\n",
      "  batch 301 loss: 0.00010742347233758665\n",
      "  batch 401 loss: 0.00010879208108349303\n",
      "LOSS train 0.00010503557631802065 valid 0.0001696713879937306\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.411735331406817e-06\n",
      "  batch 101 loss: 0.00010976881997294185\n",
      "  batch 201 loss: 0.00011041715851490607\n",
      "  batch 301 loss: 0.0001085271026121859\n",
      "  batch 401 loss: 0.00010650854617438198\n",
      "LOSS train 0.0001061831670652946 valid 0.00021658148034475744\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.0894172959961e-06\n",
      "  batch 101 loss: 0.00010392964163827401\n",
      "  batch 201 loss: 0.00010180558772447056\n",
      "  batch 301 loss: 9.618562050661694e-05\n",
      "  batch 401 loss: 9.125692310306021e-05\n",
      "LOSS train 9.56872980955894e-05 valid 0.00021734887559432536\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.10022005578503e-06\n",
      "  batch 101 loss: 8.700278650167092e-05\n",
      "  batch 201 loss: 8.340184603866874e-05\n",
      "  batch 301 loss: 7.72000348979418e-05\n",
      "  batch 401 loss: 7.275265848193158e-05\n",
      "LOSS train 7.826600915655702e-05 valid 0.00018569147505331784\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.647498331498355e-06\n",
      "  batch 101 loss: 7.040222311729849e-05\n",
      "  batch 201 loss: 6.760158296742702e-05\n",
      "  batch 301 loss: 6.332914332119798e-05\n",
      "  batch 401 loss: 6.0999035812869804e-05\n",
      "LOSS train 0.0002922702814573978 valid 0.00023975901422090828\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.412460791878402e-06\n",
      "  batch 101 loss: 6.626852125179994e-05\n",
      "  batch 201 loss: 6.0182949093814384e-05\n",
      "  batch 301 loss: 5.787576344204126e-05\n",
      "  batch 401 loss: 5.748026632915071e-05\n",
      "LOSS train 6.014335539027649e-05 valid 0.0001569895539432764\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.544644340872764e-05\n",
      "  batch 101 loss: 35.230313252075575\n",
      "  batch 201 loss: 0.013217377015389503\n",
      "  batch 301 loss: 0.007624282273463905\n",
      "  batch 401 loss: 0.003952301426324994\n",
      "LOSS train 7.958494903088513 valid 0.0008919771644286811\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.188202528283e-05\n",
      "  batch 101 loss: 0.0012370212553651071\n",
      "  batch 201 loss: 0.0005672595182841179\n",
      "  batch 301 loss: 0.0002635188051499426\n",
      "  batch 401 loss: 0.00014620708781876602\n",
      "LOSS train 0.000513074348266816 valid 6.049891089787707e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.10422861832194e-07\n",
      "  batch 101 loss: 8.953694275533053e-05\n",
      "  batch 201 loss: 7.816273464413825e-05\n",
      "  batch 301 loss: 7.486706255804165e-05\n",
      "  batch 401 loss: 7.303998718271032e-05\n",
      "LOSS train 7.863530032393047e-05 valid 6.291733734542504e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.3065319914603607e-07\n",
      "  batch 101 loss: 7.34353524785547e-05\n",
      "  batch 201 loss: 8.089694607406272e-05\n",
      "  batch 301 loss: 7.394920820843254e-05\n",
      "  batch 401 loss: 7.525141270889435e-05\n",
      "LOSS train 7.60228731974731e-05 valid 6.521877367049456e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.241925464360975e-07\n",
      "  batch 101 loss: 7.663380377834982e-05\n",
      "  batch 201 loss: 8.103575259156059e-05\n",
      "  batch 301 loss: 7.841992970497813e-05\n",
      "  batch 401 loss: 8.003285856830188e-05\n",
      "LOSS train 7.919454954915526e-05 valid 6.922674219822511e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.432297959690914e-07\n",
      "  batch 101 loss: 8.197040153845592e-05\n",
      "  batch 201 loss: 8.304723946821469e-05\n",
      "  batch 301 loss: 9.168740829863964e-05\n",
      "  batch 401 loss: 8.647708169064572e-05\n",
      "LOSS train 8.566637714765939e-05 valid 7.733617530902848e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.466903818771243e-07\n",
      "  batch 101 loss: 8.895496329614617e-05\n",
      "  batch 201 loss: 9.04405970368316e-05\n",
      "  batch 301 loss: 9.192099610345394e-05\n",
      "  batch 401 loss: 9.445664197528458e-05\n",
      "LOSS train 9.114045064397244e-05 valid 9.419039270142093e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1899502715095877e-06\n",
      "  batch 101 loss: 9.741104932345479e-05\n",
      "  batch 201 loss: 0.00010578397964764008\n",
      "  batch 301 loss: 0.0001008806727963929\n",
      "  batch 401 loss: 0.00010329720563504453\n",
      "LOSS train 0.00010072364848491371 valid 0.00012708138092420995\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.7563885194249452e-06\n",
      "  batch 101 loss: 0.00010592771096412434\n",
      "  batch 201 loss: 0.00010766328862587216\n",
      "  batch 301 loss: 0.00010829737196388578\n",
      "  batch 401 loss: 0.00010925989742361253\n",
      "LOSS train 0.00010587392709503568 valid 0.00017789647972676903\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.533330989535898e-06\n",
      "  batch 101 loss: 0.0001097564582846644\n",
      "  batch 201 loss: 0.00011008348447148819\n",
      "  batch 301 loss: 0.00010762509431231138\n",
      "  batch 401 loss: 0.00010515828734469323\n",
      "LOSS train 0.00010548700121277698 valid 0.00021927579655312002\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.1302246497943997e-06\n",
      "  batch 101 loss: 0.00010226847638762137\n",
      "  batch 201 loss: 9.987208746395026e-05\n",
      "  batch 301 loss: 9.401581267979964e-05\n",
      "  batch 401 loss: 9.623944853785815e-05\n",
      "LOSS train 9.536124552391657e-05 valid 0.0002137416013283655\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.0541192973032595e-06\n",
      "  batch 101 loss: 8.49167764295089e-05\n",
      "  batch 201 loss: 8.135190574307672e-05\n",
      "  batch 301 loss: 7.534693783327384e-05\n",
      "  batch 401 loss: 7.108912766057074e-05\n",
      "LOSS train 7.64514120750387e-05 valid 0.00018211931455880404\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00020958483219146729\n",
      "  batch 101 loss: 28.970442327260972\n",
      "  batch 201 loss: 0.01823846949264407\n",
      "  batch 301 loss: 0.006844767618458718\n",
      "  batch 401 loss: 0.0019032401038566605\n",
      "LOSS train 6.54581241880045 valid 9.106825018534437e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.91930720070377e-06\n",
      "  batch 101 loss: 0.0002670067412691424\n",
      "  batch 201 loss: 0.00011703881717721743\n",
      "  batch 301 loss: 9.486054441026681e-05\n",
      "  batch 401 loss: 9.248363564893226e-05\n",
      "LOSS train 0.00013685241056579262 valid 0.00012346678704489022\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.097233373206109e-06\n",
      "  batch 101 loss: 9.519186378383892e-05\n",
      "  batch 201 loss: 9.495702613094182e-05\n",
      "  batch 301 loss: 9.415826368240232e-05\n",
      "  batch 401 loss: 1.1085755967909063\n",
      "LOSS train 0.2510535032749141 valid 0.0016353228129446507\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.865258302539587e-05\n",
      "  batch 101 loss: 0.0009057342916639755\n",
      "  batch 201 loss: 0.00012690644480244372\n",
      "  batch 301 loss: 8.283312652565655e-05\n",
      "  batch 401 loss: 7.923711863895733e-05\n",
      "LOSS train 0.00028613796803769525 valid 6.816336826886982e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.387333294493147e-07\n",
      "  batch 101 loss: 7.911460204013566e-05\n",
      "  batch 201 loss: 7.988654486325686e-05\n",
      "  batch 301 loss: 8.137428789268597e-05\n",
      "  batch 401 loss: 8.314736045122118e-05\n",
      "LOSS train 8.106939157665656e-05 valid 7.337162969633937e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.317359995795414e-07\n",
      "  batch 101 loss: 8.559179057556321e-05\n",
      "  batch 201 loss: 8.680476645167801e-05\n",
      "  batch 301 loss: 8.782088551470224e-05\n",
      "  batch 401 loss: 9.030288545091026e-05\n",
      "LOSS train 8.75168355221207e-05 valid 8.483474812237546e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 9.9718410638161e-07\n",
      "  batch 101 loss: 9.319815018443478e-05\n",
      "  batch 201 loss: 9.489971310131296e-05\n",
      "  batch 301 loss: 9.658718564651281e-05\n",
      "  batch 401 loss: 9.908089960504185e-05\n",
      "LOSS train 9.534968907478359e-05 valid 0.00010931905853794888\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.453975710319355e-06\n",
      "  batch 101 loss: 0.00010214105258683049\n",
      "  batch 201 loss: 0.00010372949265502029\n",
      "  batch 301 loss: 0.000105149459010363\n",
      "  batch 401 loss: 0.00010713101009855563\n",
      "LOSS train 0.00010312531869116927 valid 0.0001526791020296514\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.1467008627951146e-06\n",
      "  batch 101 loss: 0.00010939958120559368\n",
      "  batch 201 loss: 0.000110349305458044\n",
      "  batch 301 loss: 0.0001094797085204391\n",
      "  batch 401 loss: 0.00011823634073209632\n",
      "LOSS train 0.000109260977693933 valid 0.0002053551870631054\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.9304911731742323e-06\n",
      "  batch 101 loss: 0.00010761274041783509\n",
      "  batch 201 loss: 0.00010628389329326637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 0.00010180901802470998\n",
      "  batch 401 loss: 9.768448914485362e-05\n",
      "LOSS train 0.00010064057863970698 valid 0.00022368962527252734\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.1891744583845138e-06\n",
      "  batch 101 loss: 9.355884059544906e-05\n",
      "  batch 201 loss: 9.018516505648221e-05\n",
      "  batch 301 loss: 8.389467815106855e-05\n",
      "  batch 401 loss: 7.889518519505146e-05\n",
      "LOSS train 8.450136213395143e-05 valid 0.00019851501565426588\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 2.832760801538825e-06\n",
      "  batch 101 loss: 7.564092701784376e-05\n",
      "  batch 201 loss: 7.254466723793484e-05\n",
      "  batch 301 loss: 6.744234630559731e-05\n",
      "  batch 401 loss: 6.42575341612428e-05\n",
      "LOSS train 6.867356666446131e-05 valid 0.00016881281044334173\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 5.6536546908318995e-05\n",
      "  batch 101 loss: 45.926123844292015\n",
      "  batch 201 loss: 0.02225427721394226\n",
      "  batch 301 loss: 0.005496393899084069\n",
      "  batch 401 loss: 0.0022533711013056745\n",
      "LOSS train 10.374012150507374 valid 0.002312267431989312\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.307435687631368e-05\n",
      "  batch 101 loss: 0.0005961623340044752\n",
      "  batch 201 loss: 0.0002884469075524976\n",
      "  batch 301 loss: 9.501431122316717e-05\n",
      "  batch 401 loss: 7.552916630856998e-05\n",
      "LOSS train 0.0002546318825425383 valid 0.0010383282788097858\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.2757621482014657e-05\n",
      "  batch 101 loss: 0.00030577005889654175\n",
      "  batch 201 loss: 0.0004074997986640483\n",
      "  batch 301 loss: 9.655859033273373e-05\n",
      "  batch 401 loss: 0.0005242151238689985\n",
      "LOSS train 0.0003608773213421643 valid 0.0009705115808174014\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.338133683428168e-07\n",
      "  batch 101 loss: 0.001655123437558359\n",
      "  batch 201 loss: 0.0007727227140185278\n",
      "  batch 301 loss: 8.733426315757242e-05\n",
      "  batch 401 loss: 0.0001500211731422496\n",
      "LOSS train 0.0006156167902620756 valid 0.0010268244659528136\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.511009806767106e-05\n",
      "  batch 101 loss: 0.0018281554136228807\n",
      "  batch 201 loss: 2.5904204897403544\n",
      "  batch 301 loss: 2.2004794859420507\n",
      "  batch 401 loss: 0.799012626921758\n",
      "LOSS train 1.2686559308947107 valid 0.18321917951107025\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0009551528841257095\n",
      "  batch 101 loss: 0.043477398212999105\n",
      "  batch 201 loss: 0.232791686890414\n",
      "  batch 301 loss: 0.885261780419387\n",
      "  batch 401 loss: 1.297344112189021\n",
      "LOSS train 0.646920114563532 valid 0.3028689920902252\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0020840343832969664\n",
      "  batch 101 loss: 0.7084845338878222\n",
      "  batch 201 loss: 0.7336075703566894\n",
      "  batch 301 loss: 0.820495551158674\n",
      "  batch 401 loss: 1.3258838021801784\n",
      "LOSS train 0.8594048344451328 valid 0.3465079367160797\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0027556112408638\n",
      "  batch 101 loss: 1.0304677221830933\n",
      "  batch 201 loss: 0.15665725114580709\n",
      "  batch 301 loss: 0.9979571534856223\n",
      "  batch 401 loss: 1.907936885068193\n",
      "LOSS train 0.9820050502451647 valid 0.27790653705596924\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.004859027564525604\n",
      "  batch 101 loss: 0.2418665674130898\n",
      "  batch 201 loss: 0.574919857934583\n",
      "  batch 301 loss: 1.0494482273980976\n",
      "  batch 401 loss: 1.6349733016267418\n",
      "LOSS train 0.9486730233127563 valid 0.9910524487495422\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.004636924564838409\n",
      "  batch 101 loss: 2.214711664579809\n",
      "  batch 201 loss: 0.6260223751608283\n",
      "  batch 301 loss: 0.19096117401495577\n",
      "  batch 401 loss: 0.5497898258862551\n",
      "LOSS train 0.837325066614751 valid 0.49811384081840515\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.0014911431074142456\n",
      "  batch 101 loss: 0.6484802713058889\n",
      "  batch 201 loss: 0.6948203304642812\n",
      "  batch 301 loss: 1.4543189510214143\n",
      "  batch 401 loss: 0.6258186098840087\n",
      "LOSS train 0.8275972173636481 valid 0.9605928659439087\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.01903590440750122\n",
      "  batch 101 loss: 1.7429977362602949\n",
      "  batch 201 loss: 1.1419386472553015\n",
      "  batch 301 loss: 0.24834023696632357\n",
      "  batch 401 loss: 0.7770631944178603\n",
      "LOSS train 0.9355697400172732 valid 0.46257370710372925\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ") 0.1 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0008259133249521256\n",
      "  batch 101 loss: 44.849873807281256\n",
      "  batch 201 loss: 0.08313882982358337\n",
      "  batch 301 loss: 0.03416151479817927\n",
      "  batch 401 loss: 0.011019499725662171\n",
      "LOSS train 10.153819358718957 valid 0.012040568515658379\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0002729885280132294\n",
      "  batch 101 loss: 0.0033738766028545797\n",
      "  batch 201 loss: 0.0007857533128117211\n",
      "  batch 301 loss: 0.00024442719368380497\n",
      "  batch 401 loss: 9.627971865484142e-05\n",
      "LOSS train 0.0010816430500388084 valid 8.948520553531125e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.869678137358278e-06\n",
      "  batch 101 loss: 5.442134261102183e-05\n",
      "  batch 201 loss: 1.903839000078733e-05\n",
      "  batch 301 loss: 3.374921540853393e-05\n",
      "  batch 401 loss: 0.0001506889857864735\n",
      "LOSS train 8.486529200937773e-05 valid 0.00015339787933044136\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3863299682270735e-06\n",
      "  batch 101 loss: 0.0001694835907437664\n",
      "  batch 201 loss: 0.00017446858249059005\n",
      "  batch 301 loss: 3.968537675063999e-05\n",
      "  batch 401 loss: 5.129821232003451e-05\n",
      "LOSS train 0.0001092789644248418 valid 0.0006238655187189579\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.866991123184562e-06\n",
      "  batch 101 loss: 0.0010824589826734154\n",
      "  batch 201 loss: 7.285191244615707\n",
      "  batch 301 loss: 0.9088759315013886\n",
      "  batch 401 loss: 0.21644347846508027\n",
      "LOSS train 1.9107675588714363 valid 0.12200197577476501\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0006372465193271637\n",
      "  batch 101 loss: 0.01448320270050317\n",
      "  batch 201 loss: 0.002827166078495793\n",
      "  batch 301 loss: 0.0026217014086432756\n",
      "  batch 401 loss: 1.1151983350384398\n",
      "LOSS train 0.5711586238453338 valid 0.2764192521572113\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0029188045859336854\n",
      "  batch 101 loss: 0.44156286004930734\n",
      "  batch 201 loss: 0.4781193983182311\n",
      "  batch 301 loss: 0.8247946712281555\n",
      "  batch 401 loss: 1.2091714164614678\n",
      "LOSS train 0.7266185097627628 valid 0.1959705501794815\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0015804696083068848\n",
      "  batch 101 loss: 1.0200987664610148\n",
      "  batch 201 loss: 0.7207303629443049\n",
      "  batch 301 loss: 0.7846047638729214\n",
      "  batch 401 loss: 2.266528969667852\n",
      "LOSS train 1.1495736464942148 valid 0.2581360936164856\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.003302077353000641\n",
      "  batch 101 loss: 1.3326472512632608\n",
      "  batch 201 loss: 0.8022353699430823\n",
      "  batch 301 loss: 0.13038430355489253\n",
      "  batch 401 loss: 1.4017826786544174\n",
      "LOSS train 1.123056739995479 valid 2.0827040672302246\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.02286116123199463\n",
      "  batch 101 loss: 0.7016174469515681\n",
      "  batch 201 loss: 0.15269037331454455\n",
      "  batch 301 loss: 0.5103360296692699\n",
      "  batch 401 loss: 1.9883090307563542\n",
      "LOSS train 0.8566148824123645 valid 0.38745981454849243\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.0043519452214241024\n",
      "  batch 101 loss: 0.974868788830936\n",
      "  batch 201 loss: 0.2548292649537325\n",
      "  batch 301 loss: 0.48327129628509286\n",
      "  batch 401 loss: 2.1279701430350544\n",
      "LOSS train 1.0384184865629968 valid 3.6399474143981934\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.0336223292350769\n",
      "  batch 101 loss: 1.4923924567550422\n",
      "  batch 201 loss: 0.16046881895512344\n",
      "  batch 301 loss: 0.3380615462455899\n",
      "  batch 401 loss: 1.160810773782432\n",
      "LOSS train 0.9389951392928344 valid 0.352827787399292\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ") 0.2 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00016704199835658074\n",
      "  batch 101 loss: 46.153470570147036\n",
      "  batch 201 loss: 0.11175330284982919\n",
      "  batch 301 loss: 0.027196979131549597\n",
      "  batch 401 loss: 0.0047465625521726906\n",
      "LOSS train 10.451017405882054 valid 0.00030659392359666526\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.1777183283120392e-05\n",
      "  batch 101 loss: 0.001260360280648456\n",
      "  batch 201 loss: 0.00013678052157047203\n",
      "  batch 301 loss: 7.712474112850031e-05\n",
      "  batch 401 loss: 3.850266037261463e-05\n",
      "LOSS train 0.0003504479858230435 valid 0.00013413000851869583\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5964466729201376e-06\n",
      "  batch 101 loss: 4.6388541486521715e-05\n",
      "  batch 201 loss: 2.728519948959729e-05\n",
      "  batch 301 loss: 2.5271599311054162e-05\n",
      "  batch 401 loss: 7.830534546883427e-05\n",
      "LOSS train 7.130750582004087e-05 valid 0.0020232496317476034\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.763460459187627e-05\n",
      "  batch 101 loss: 0.0009010485493490706\n",
      "  batch 201 loss: 7.069460851198528e-05\n",
      "  batch 301 loss: 4.711653870799637e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.0001742161551010213\n",
      "LOSS train 0.0003361622490297482 valid 0.0013080695644021034\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.4281949261203408e-05\n",
      "  batch 101 loss: 0.0037584718503057956\n",
      "  batch 201 loss: 3.297069108702126\n",
      "  batch 301 loss: 2.5840201649814842\n",
      "  batch 401 loss: 0.8256775644794107\n",
      "LOSS train 1.5372583545008873 valid 0.10459519177675247\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0017755924165248872\n",
      "  batch 101 loss: 0.14869044320657848\n",
      "  batch 201 loss: 0.01803798294160515\n",
      "  batch 301 loss: 0.9579494312754833\n",
      "  batch 401 loss: 3.529269043803215\n",
      "LOSS train 1.1549573179684436 valid 0.8973372578620911\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.013304710388183594\n",
      "  batch 101 loss: 0.6134716459736228\n",
      "  batch 201 loss: 0.11570018326863646\n",
      "  batch 301 loss: 0.06556728215422482\n",
      "  batch 401 loss: 1.910380638469942\n",
      "LOSS train 1.110203163475471 valid 4.065875053405762\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.03870770215988159\n",
      "  batch 101 loss: 5.409745552837848\n",
      "  batch 201 loss: 1.3604300659149886\n",
      "  batch 301 loss: 0.08288465287070722\n",
      "  batch 401 loss: 0.013627142315963283\n",
      "LOSS train 1.559101866497096 valid 0.00020133583166170865\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.2095325402915477e-05\n",
      "  batch 101 loss: 0.007652925222646445\n",
      "  batch 201 loss: 0.007613527330686338\n",
      "  batch 301 loss: 0.802492018446792\n",
      "  batch 401 loss: 5.837834563553333\n",
      "LOSS train 1.9810841334706464 valid 1.509352684020996\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.02466477394104004\n",
      "  batch 101 loss: 2.2053054919838906\n",
      "  batch 201 loss: 0.3280945663154125\n",
      "  batch 301 loss: 0.08039316739421337\n",
      "  batch 401 loss: 0.0049579203248140405\n",
      "LOSS train 0.5967842336738749 valid 0.0003795815573539585\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 8.074858924373984e-06\n",
      "  batch 101 loss: 0.0004778694639389869\n",
      "  batch 201 loss: 0.000559777134039905\n",
      "  batch 301 loss: 1.9189250228602033\n",
      "  batch 401 loss: 2.7201540216803552\n",
      "LOSS train 1.0802448958675004 valid 0.015489543788135052\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.0005011061578989029\n",
      "  batch 101 loss: 0.09576923908665776\n",
      "  batch 201 loss: 0.13175308682955802\n",
      "  batch 301 loss: 0.7692458432354033\n",
      "  batch 401 loss: 2.965199877023697\n",
      "LOSS train 1.0104706331385347 valid 4.411471843719482\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ") 0.4 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005720527470111847\n",
      "  batch 101 loss: 45.81903562605381\n",
      "  batch 201 loss: 0.15517039565369486\n",
      "  batch 301 loss: 0.009598075826652348\n",
      "  batch 401 loss: 0.001787213806528598\n",
      "LOSS train 10.38071065789662 valid 9.943557961378247e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.748822724446654e-05\n",
      "  batch 101 loss: 0.0009430247911222978\n",
      "  batch 201 loss: 7.113271181424352e-05\n",
      "  batch 301 loss: 5.473379488648788e-05\n",
      "  batch 401 loss: 6.250171868487087e-05\n",
      "LOSS train 0.0002671056327140323 valid 3.951178950956091e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.2393674296617975e-07\n",
      "  batch 101 loss: 5.79308178203064e-05\n",
      "  batch 201 loss: 6.227814946214493e-05\n",
      "  batch 301 loss: 5.7905616940843175e-05\n",
      "  batch 401 loss: 0.00035290040206746197\n",
      "LOSS train 0.0001508939238533201 valid 0.00015061013982631266\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.8080518930219115e-06\n",
      "  batch 101 loss: 0.0005575967384356772\n",
      "  batch 201 loss: 0.0006860819931171136\n",
      "  batch 301 loss: 0.001662209662317764\n",
      "  batch 401 loss: 0.0056492291809991\n",
      "LOSS train 0.008949143179856834 valid 0.036867644637823105\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.0006488610059022903\n",
      "  batch 101 loss: 0.7373456726968288\n",
      "  batch 201 loss: 15.366746101379395\n",
      "  batch 301 loss: 14.50628434896469\n",
      "  batch 401 loss: 0.47703885968774556\n",
      "LOSS train 7.023841309802645 valid 0.17939022183418274\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0009844394028186798\n",
      "  batch 101 loss: 0.1142769318819046\n",
      "  batch 201 loss: 0.028290003009606154\n",
      "  batch 301 loss: 0.01625184672418982\n",
      "  batch 401 loss: 0.01811808275175281\n",
      "LOSS train 0.04078952539670144 valid 0.0563603900372982\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0005942857265472412\n",
      "  batch 101 loss: 0.06412642282433807\n",
      "  batch 201 loss: 0.09766784873791039\n",
      "  batch 301 loss: 0.15448633331805467\n",
      "  batch 401 loss: 0.11983597043901682\n",
      "LOSS train 0.14360079485881005 valid 0.3666538596153259\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.007329096794128418\n",
      "  batch 101 loss: 2.1475304763019083\n",
      "  batch 201 loss: 2.6461662393808365\n",
      "  batch 301 loss: 1.453881483078003\n",
      "  batch 401 loss: 1.6742986270785332\n",
      "LOSS train 2.4694671430719626 valid 0.6377054452896118\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.021796855926513672\n",
      "  batch 101 loss: 12.77919270992279\n",
      "  batch 201 loss: 3.137810922861099\n",
      "  batch 301 loss: 0.5406638948619366\n",
      "  batch 401 loss: 0.3555363553762436\n",
      "LOSS train 3.839525669914067 valid 0.0680491179227829\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.0022539329528808594\n",
      "  batch 101 loss: 0.44043711803853514\n",
      "  batch 201 loss: 0.36912518568336966\n",
      "  batch 301 loss: 0.07350305772386491\n",
      "  batch 401 loss: 0.02431665069423616\n",
      "LOSS train 0.2083161308631957 valid 0.19377155601978302\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.001865471601486206\n",
      "  batch 101 loss: 0.8511061335355044\n",
      "  batch 201 loss: 6.491869630813599\n",
      "  batch 301 loss: 5.702981936335564\n",
      "  batch 401 loss: 1.3948802843689918\n",
      "LOSS train 3.6109421227135723 valid 1.952782392501831\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.023585429191589357\n",
      "  batch 101 loss: 5.28011568248272\n",
      "  batch 201 loss: 0.14780766524374486\n",
      "  batch 301 loss: 0.015465915894601494\n",
      "  batch 401 loss: 0.007060906949918717\n",
      "LOSS train 1.2368745284215978 valid 0.003674174891784787\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00032513841986656186\n",
      "  batch 101 loss: 34.09157221708447\n",
      "  batch 201 loss: 0.0026132773640790675\n",
      "  batch 301 loss: 7.525673786403785e-05\n",
      "  batch 401 loss: 7.818745622444112e-05\n",
      "LOSS train 7.696320109460433 valid 7.002817437751219e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.546733493451029e-07\n",
      "  batch 101 loss: 8.302532046400302e-05\n",
      "  batch 201 loss: 8.627800777958328e-05\n",
      "  batch 301 loss: 8.951060324079663e-05\n",
      "  batch 401 loss: 9.358389711451309e-05\n",
      "LOSS train 8.805011415179929e-05 valid 9.603910439182073e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.2156354932812975e-06\n",
      "  batch 101 loss: 9.823452473824546e-05\n",
      "  batch 201 loss: 0.00010131989391993556\n",
      "  batch 301 loss: 0.00010385667696141354\n",
      "  batch 401 loss: 0.00010667530763328159\n",
      "LOSS train 0.00010122722241930088 valid 0.00015230930875986814\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.149584761355072e-06\n",
      "  batch 101 loss: 0.0001089786777208701\n",
      "  batch 201 loss: 0.0001103719423451821\n",
      "  batch 301 loss: 0.00010920817316048214\n",
      "  batch 401 loss: 0.00010759153798744591\n",
      "LOSS train 0.00010643857326348887 valid 0.00021404550352599472\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.0536699341610075e-06\n",
      "  batch 101 loss: 0.0001048281607984336\n",
      "  batch 201 loss: 0.00010237760655911642\n",
      "  batch 301 loss: 9.626895045300899e-05\n",
      "  batch 401 loss: 9.076268485330274e-05\n",
      "LOSS train 9.586988567253493e-05 valid 0.00021629409457091242\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.0853724456392228e-06\n",
      "  batch 101 loss: 8.58926213192035e-05\n",
      "  batch 201 loss: 8.185599527394061e-05\n",
      "  batch 301 loss: 7.540329644598387e-05\n",
      "  batch 401 loss: 7.086877275128245e-05\n",
      "LOSS train 7.673419015419943e-05 valid 0.0001815176656236872\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.58656800724566e-06\n",
      "  batch 101 loss: 6.861794946985355e-05\n",
      "  batch 201 loss: 6.585978285443161e-05\n",
      "  batch 301 loss: 6.182294896234453e-05\n",
      "  batch 401 loss: 5.978381375228992e-05\n",
      "LOSS train 6.309219405646676e-05 valid 0.00016018144378904253\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2694094514008612e-06\n",
      "  batch 101 loss: 6.0797985541967135e-05\n",
      "  batch 201 loss: 5.963505362799992e-05\n",
      "  batch 301 loss: 5.765804202241043e-05\n",
      "  batch 401 loss: 5.760770418305583e-05\n",
      "LOSS train 5.859685675614639e-05 valid 0.00015677054761908948\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.2176996571943164e-06\n",
      "  batch 101 loss: 6.0760312613297175e-05\n",
      "  batch 201 loss: 6.0364850847918204e-05\n",
      "  batch 301 loss: 5.9181487889645725e-05\n",
      "  batch 401 loss: 5.9791127206381134e-05\n",
      "LOSS train 6.0098325498952376e-05 valid 0.0001386111689498648\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.9365739717613907e-06\n",
      "  batch 101 loss: 6.0302362315383106e-05\n",
      "  batch 201 loss: 5.864724567572921e-05\n",
      "  batch 301 loss: 5.638967108325232e-05\n",
      "  batch 401 loss: 5.5226023403065486e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 5.7501542393255523e-05 valid 9.024244354804978e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.105220871977508e-06\n",
      "  batch 101 loss: 5.068766683393733e-05\n",
      "  batch 201 loss: 4.904762953856334e-05\n",
      "  batch 301 loss: 4.7502538182015995e-05\n",
      "  batch 401 loss: 4.6948592660100986e-05\n",
      "LOSS train 4.8435023733198674e-05 valid 7.345936319325119e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.437801104970276e-07\n",
      "  batch 101 loss: 4.4617914206526165e-05\n",
      "  batch 201 loss: 4.4730149148222154e-05\n",
      "  batch 301 loss: 4.548709283142216e-05\n",
      "  batch 401 loss: 4.7065637907053316e-05\n",
      "LOSS train 4.5655420020319105e-05 valid 6.990473048062995e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.1 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001935490407049656\n",
      "  batch 101 loss: 18.952550369350938\n",
      "  batch 201 loss: 0.0001091480774235265\n",
      "  batch 301 loss: 7.935237241326831e-05\n",
      "  batch 401 loss: 8.553105724331544e-05\n",
      "LOSS train 5.5439052685770545 valid 0.48100849986076355\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.004739288091659546\n",
      "  batch 101 loss: 0.026455908646254328\n",
      "  batch 201 loss: 9.10073008890322e-05\n",
      "  batch 301 loss: 9.444406499369507e-05\n",
      "  batch 401 loss: 9.877789557890537e-05\n",
      "LOSS train 0.007114422426539494 valid 0.00011355305468896404\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.5267742855940015e-06\n",
      "  batch 101 loss: 0.00010337038954048694\n",
      "  batch 201 loss: 0.00010637287700433262\n",
      "  batch 301 loss: 0.00010800850572195486\n",
      "  batch 401 loss: 0.00010934329902113404\n",
      "LOSS train 0.000104831121844199 valid 0.0001846599334385246\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.6324696955271066e-06\n",
      "  batch 101 loss: 0.00010941298841117942\n",
      "  batch 201 loss: 0.00010898630700467037\n",
      "  batch 301 loss: 0.00010498229573784101\n",
      "  batch 401 loss: 0.00010067366697001034\n",
      "LOSS train 0.00010314269928370106 valid 0.0002240156172774732\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.1937335734255614e-06\n",
      "  batch 101 loss: 9.581148438741139e-05\n",
      "  batch 201 loss: 9.181414661597386e-05\n",
      "  batch 301 loss: 8.470266187600828e-05\n",
      "  batch 401 loss: 7.911369108001055e-05\n",
      "LOSS train 8.560674634583307e-05 valid 0.0001972621539607644\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.81478394754231e-06\n",
      "  batch 101 loss: 7.524020060998283e-05\n",
      "  batch 201 loss: 7.167332642325163e-05\n",
      "  batch 301 loss: 6.645416940045834e-05\n",
      "  batch 401 loss: 6.330775486731e-05\n",
      "LOSS train 6.790711249528884e-05 valid 0.00016650630277581513\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.3645078181289135e-06\n",
      "  batch 101 loss: 6.29558669655239e-05\n",
      "  batch 201 loss: 6.114166433803802e-05\n",
      "  batch 301 loss: 5.8396291614712935e-05\n",
      "  batch 401 loss: 5.7602639304832334e-05\n",
      "LOSS train 5.946328501280143e-05 valid 0.00015709073340985924\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2225665452424435e-06\n",
      "  batch 101 loss: 6.013596023194623e-05\n",
      "  batch 201 loss: 5.961317983974368e-05\n",
      "  batch 301 loss: 5.833944078915465e-05\n",
      "  batch 401 loss: 5.8970469100358967e-05\n",
      "LOSS train 5.922859042821218e-05 valid 0.0001513282477390021\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.1345281857065857e-06\n",
      "  batch 101 loss: 6.147371527475797e-05\n",
      "  batch 201 loss: 6.051754262131226e-05\n",
      "  batch 301 loss: 5.876989753005546e-05\n",
      "  batch 401 loss: 5.839916892114161e-05\n",
      "LOSS train 5.9784738232235334e-05 valid 0.00010779224248835817\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.4273938722908496e-06\n",
      "  batch 101 loss: 5.5009417909559485e-05\n",
      "  batch 201 loss: 5.2937883899630834e-05\n",
      "  batch 301 loss: 5.071714105383762e-05\n",
      "  batch 401 loss: 4.9488299324593755e-05\n",
      "LOSS train 5.184587533290706e-05 valid 7.654682121938094e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 8.174902905011549e-07\n",
      "  batch 101 loss: 4.5911904971376314e-05\n",
      "  batch 201 loss: 4.531110745034539e-05\n",
      "  batch 301 loss: 4.5156091948399537e-05\n",
      "  batch 401 loss: 4.5955703655522483e-05\n",
      "LOSS train 4.565549383406479e-05 valid 7.183422712842003e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.027265382930636e-07\n",
      "  batch 101 loss: 4.512966652498562e-05\n",
      "  batch 201 loss: 4.633120593268813e-05\n",
      "  batch 301 loss: 4.8079791573059085e-05\n",
      "  batch 401 loss: 4.946907673740952e-05\n",
      "LOSS train 4.745380987911327e-05 valid 6.283284164965153e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.2 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.7745342813432216e-05\n",
      "  batch 101 loss: 30.339621691517532\n",
      "  batch 201 loss: 2.001954563853833\n",
      "  batch 301 loss: 0.02924108308116274\n",
      "  batch 401 loss: 8.35698871560453e-05\n",
      "LOSS train 7.307215156273748 valid 7.213997741928324e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.105916301952675e-07\n",
      "  batch 101 loss: 8.516044633324782e-05\n",
      "  batch 201 loss: 8.874678256233893e-05\n",
      "  batch 301 loss: 9.226697364283609e-05\n",
      "  batch 401 loss: 9.654366459926677e-05\n",
      "LOSS train 9.046261630637734e-05 valid 0.0001053059459081851\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.3837005826644598e-06\n",
      "  batch 101 loss: 0.00010123415755344922\n",
      "  batch 201 loss: 0.00010433435963705051\n",
      "  batch 301 loss: 0.00010647029328993085\n",
      "  batch 401 loss: 0.00010857087392594166\n",
      "LOSS train 0.00010349133029988246 valid 0.000170850136782974\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.4292664602398874e-06\n",
      "  batch 101 loss: 0.00010971707905071071\n",
      "  batch 201 loss: 0.00011016881300179193\n",
      "  batch 301 loss: 0.00010741695639069349\n",
      "  batch 401 loss: 0.00010420517258125983\n",
      "LOSS train 0.00010507534755075667 valid 0.0002222953044110909\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.1696571386419236e-06\n",
      "  batch 101 loss: 0.00010008265225451396\n",
      "  batch 201 loss: 9.661449422566194e-05\n",
      "  batch 301 loss: 8.972096344052716e-05\n",
      "  batch 401 loss: 8.398136572907334e-05\n",
      "LOSS train 9.013534150951815e-05 valid 0.00020596499962266535\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.939172263722867e-06\n",
      "  batch 101 loss: 7.953290845307493e-05\n",
      "  batch 201 loss: 7.568182581508154e-05\n",
      "  batch 301 loss: 6.987945813420993e-05\n",
      "  batch 401 loss: 6.612397466483344e-05\n",
      "LOSS train 7.133606622587352e-05 valid 0.0001720189320622012\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.446617872919887e-06\n",
      "  batch 101 loss: 6.496731764286778e-05\n",
      "  batch 201 loss: 6.275297046840933e-05\n",
      "  batch 301 loss: 5.948568740677729e-05\n",
      "  batch 401 loss: 5.8185577397296126e-05\n",
      "LOSS train 6.0637341125807766e-05 valid 0.00015775916108395904\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2327179613057523e-06\n",
      "  batch 101 loss: 6.012789099116844e-05\n",
      "  batch 201 loss: 5.937883101296393e-05\n",
      "  batch 301 loss: 5.787084262465214e-05\n",
      "  batch 401 loss: 5.8287475202405406e-05\n",
      "LOSS train 5.8766196714200396e-05 valid 0.00015508048818446696\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.1919618302490562e-06\n",
      "  batch 101 loss: 6.13729245898753e-05\n",
      "  batch 201 loss: 6.0769910297722164e-05\n",
      "  batch 301 loss: 5.93488367911732e-05\n",
      "  batch 401 loss: 5.951302828179905e-05\n",
      "LOSS train 6.0314729823532296e-05 valid 0.00012148548557888716\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.6599941591266542e-06\n",
      "  batch 101 loss: 5.766957555636054e-05\n",
      "  batch 201 loss: 5.560867107760714e-05\n",
      "  batch 301 loss: 5.320639280398609e-05\n",
      "  batch 401 loss: 5.179945651775597e-05\n",
      "LOSS train 5.4369915669765056e-05 valid 8.062993583735079e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 9.086659702006728e-07\n",
      "  batch 101 loss: 4.754636794586986e-05\n",
      "  batch 201 loss: 4.648623464504453e-05\n",
      "  batch 301 loss: 4.572304656221604e-05\n",
      "  batch 401 loss: 4.593613435190491e-05\n",
      "LOSS train 4.641442495754647e-05 valid 7.239318074425682e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.170540629886091e-07\n",
      "  batch 101 loss: 4.459355120104647e-05\n",
      "  batch 201 loss: 4.535157643317689e-05\n",
      "  batch 301 loss: 4.6817642033829546e-05\n",
      "  batch 401 loss: 4.86498420150383e-05\n",
      "LOSS train 4.658170556787427e-05 valid 6.600456981686875e-05\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0.4 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00023902487009763718\n",
      "  batch 101 loss: 26.536772302631288\n",
      "  batch 201 loss: 0.0009645387053842569\n",
      "  batch 301 loss: 7.685150117140438e-05\n",
      "  batch 401 loss: 8.207469087665232e-05\n",
      "LOSS train 5.99055752115103 valid 7.550488226115704e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.931567233754322e-07\n",
      "  batch 101 loss: 8.79930000337481e-05\n",
      "  batch 201 loss: 9.198327478543433e-05\n",
      "  batch 301 loss: 9.580215754112942e-05\n",
      "  batch 401 loss: 0.00010019418385013523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 9.351580623292165e-05 valid 0.00011944778088945895\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.6261314158327877e-06\n",
      "  batch 101 loss: 0.0001046516969267941\n",
      "  batch 201 loss: 0.00010752828225122358\n",
      "  batch 301 loss: 0.00010874153884685712\n",
      "  batch 401 loss: 0.0001094826044275976\n",
      "LOSS train 0.0001054813831797976 valid 0.00019305483147036284\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7542197494767607e-06\n",
      "  batch 101 loss: 0.00010879046534000735\n",
      "  batch 201 loss: 0.0001077490014824889\n",
      "  batch 301 loss: 0.00010296750774216434\n",
      "  batch 401 loss: 9.805481651142144e-05\n",
      "LOSS train 0.0001015138804464543 valid 0.00022320383868645877\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.1823772587813435e-06\n",
      "  batch 101 loss: 9.288100302001111e-05\n",
      "  batch 201 loss: 8.86666611182818e-05\n",
      "  batch 301 loss: 8.157039309935498e-05\n",
      "  batch 401 loss: 7.619159243688501e-05\n",
      "LOSS train 8.271809541079996e-05 valid 0.0001917105255415663\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.7348066214472057e-06\n",
      "  batch 101 loss: 7.276228300838739e-05\n",
      "  batch 201 loss: 6.941803854033423e-05\n",
      "  batch 301 loss: 6.458694546722654e-05\n",
      "  batch 401 loss: 6.18256981954346e-05\n",
      "LOSS train 6.600906937407013e-05 valid 0.00016371373203583062\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.322639775229618e-06\n",
      "  batch 101 loss: 6.197265067527269e-05\n",
      "  batch 201 loss: 6.040425570631669e-05\n",
      "  batch 301 loss: 5.796482727390639e-05\n",
      "  batch 401 loss: 5.747177841357143e-05\n",
      "LOSS train 5.8983945256744244e-05 valid 0.00015695429465267807\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2204939159564673e-06\n",
      "  batch 101 loss: 6.031290896430619e-05\n",
      "  batch 201 loss: 5.9882722608790574e-05\n",
      "  batch 301 loss: 5.8692769662229694e-05\n",
      "  batch 401 loss: 5.937806584114469e-05\n",
      "LOSS train 5.9588087348029674e-05 valid 0.0001473058364354074\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.072490460705012e-06\n",
      "  batch 101 loss: 6.123431661080758e-05\n",
      "  batch 201 loss: 5.99938770017161e-05\n",
      "  batch 301 loss: 5.8015784601934685e-05\n",
      "  batch 401 loss: 5.728256739587323e-05\n",
      "LOSS train 5.9068987178316235e-05 valid 9.982324263546616e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.2853597581852227e-06\n",
      "  batch 101 loss: 5.320290933468641e-05\n",
      "  batch 201 loss: 5.125214419990698e-05\n",
      "  batch 301 loss: 4.926134418781203e-05\n",
      "  batch 401 loss: 4.82669710436312e-05\n",
      "LOSS train 5.032902400553653e-05 valid 7.490474672522396e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 7.789031224092469e-07\n",
      "  batch 101 loss: 4.520056869587563e-05\n",
      "  batch 201 loss: 4.48944812700347e-05\n",
      "  batch 301 loss: 4.5125175178100107e-05\n",
      "  batch 401 loss: 4.628031699724033e-05\n",
      "LOSS train 4.5495808986162846e-05 valid 7.12788532837294e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 6.882523302920162e-07\n",
      "  batch 101 loss: 4.562125146634344e-05\n",
      "  batch 201 loss: 4.70472851844761e-05\n",
      "  batch 301 loss: 4.875638992984932e-05\n",
      "  batch 401 loss: 4.948690499759323e-05\n",
      "LOSS train 4.787832960649943e-05 valid 6.17034311289899e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0009872381389141082\n",
      "  batch 101 loss: 20.73513951664074\n",
      "  batch 201 loss: 0.0012292769195710207\n",
      "  batch 301 loss: 5.6774127138510266e-05\n",
      "  batch 401 loss: 5.841242675103331e-05\n",
      "LOSS train 4.68115014362645 valid 0.00013257797400001436\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.8406179151497782e-06\n",
      "  batch 101 loss: 5.7980929877885504e-05\n",
      "  batch 201 loss: 5.262087937239812e-05\n",
      "  batch 301 loss: 4.839842966305241e-05\n",
      "  batch 401 loss: 4.671282298886581e-05\n",
      "LOSS train 5.116113388027152e-05 valid 7.271415233844891e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.25180289009586e-07\n",
      "  batch 101 loss: 4.478080358353509e-05\n",
      "  batch 201 loss: 4.599825730451812e-05\n",
      "  batch 301 loss: 4.8265544671721726e-05\n",
      "  batch 401 loss: 4.9790502181394914e-05\n",
      "LOSS train 4.7440432742588115e-05 valid 6.172127177705988e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1753232178743927e-07\n",
      "  batch 101 loss: 4.485128501357849e-05\n",
      "  batch 201 loss: 4.4288026554681895e-05\n",
      "  batch 301 loss: 4.1606833062246554e-05\n",
      "  batch 401 loss: 3.870306607929308e-05\n",
      "LOSS train 4.2311817371321785e-05 valid 6.574411963811144e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.0806304999277927e-07\n",
      "  batch 101 loss: 3.6051654054176654e-05\n",
      "  batch 201 loss: 3.586811867009487e-05\n",
      "  batch 301 loss: 3.605223527983981e-05\n",
      "  batch 401 loss: 3.613088775921369e-05\n",
      "LOSS train 3.650601766374717e-05 valid 6.162610952742398e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.02386142720934e-07\n",
      "  batch 101 loss: 3.7417462415305636e-05\n",
      "  batch 201 loss: 3.793881180115477e-05\n",
      "  batch 301 loss: 4.053535218815796e-05\n",
      "  batch 401 loss: 4.3126379501359225e-05\n",
      "LOSS train 4.048321558472387e-05 valid 8.667654037708417e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0346218186896294e-06\n",
      "  batch 101 loss: 5.06438183194291e-05\n",
      "  batch 201 loss: 5.382632305469315e-05\n",
      "  batch 301 loss: 6.149207944275759e-05\n",
      "  batch 401 loss: 6.249568370918724e-05\n",
      "LOSS train 5.7398542533634475e-05 valid 9.925918857334182e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2750696623697876e-06\n",
      "  batch 101 loss: 5.6339621237384564e-05\n",
      "  batch 201 loss: 5.4879339500644165e-05\n",
      "  batch 301 loss: 5.1866157128301894e-05\n",
      "  batch 401 loss: 5.333408158264774e-05\n",
      "LOSS train 5.416080448548699e-05 valid 6.30792710580863e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.1493436583550645e-07\n",
      "  batch 101 loss: 5.4542558868604376e-05\n",
      "  batch 201 loss: 6.0226970990697734e-05\n",
      "  batch 301 loss: 5.678907267849809e-05\n",
      "  batch 401 loss: 5.1109340420225634e-05\n",
      "LOSS train 5.522743548134442e-05 valid 6.560233305208385e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.1019369594578166e-07\n",
      "  batch 101 loss: 4.210516501359507e-05\n",
      "  batch 201 loss: 4.281829677438509e-05\n",
      "  batch 301 loss: 4.412708545373789e-05\n",
      "  batch 401 loss: 4.5574897478672936e-05\n",
      "LOSS train 4.3775808451863994e-05 valid 6.786267476854846e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 5.924493962083943e-07\n",
      "  batch 101 loss: 4.487148910740757e-05\n",
      "  batch 201 loss: 4.791910358534324e-05\n",
      "  batch 301 loss: 5.490290774730511e-05\n",
      "  batch 401 loss: 6.454443864100767e-05\n",
      "LOSS train 5.504265421995536e-05 valid 6.621012289542705e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.0146537533728406e-07\n",
      "  batch 101 loss: 6.147794145448415e-05\n",
      "  batch 201 loss: 6.518847533214966e-05\n",
      "  batch 301 loss: 7.82668229896899e-05\n",
      "  batch 401 loss: 8.543245931235787e-05\n",
      "LOSS train 7.432560074687624e-05 valid 0.00012358700041659176\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.1 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004960381239652634\n",
      "  batch 101 loss: 0.0017420382263151168\n",
      "  batch 201 loss: 5.677318279509791e-05\n",
      "  batch 301 loss: 5.291864675228908e-05\n",
      "  batch 401 loss: 4.501825089334943e-05\n",
      "LOSS train 0.0005445451578710432 valid 6.469475192716345e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.856511804973707e-07\n",
      "  batch 101 loss: 5.4818849998810035e-05\n",
      "  batch 201 loss: 6.770890815516851e-05\n",
      "  batch 301 loss: 8.219892608508416e-05\n",
      "  batch 401 loss: 9.971555144147715e-05\n",
      "LOSS train 7.761060120386055e-05 valid 8.870781311998144e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.075121617759578e-06\n",
      "  batch 101 loss: 8.415302861635609e-05\n",
      "  batch 201 loss: 9.947121791753944e-05\n",
      "  batch 301 loss: 0.00013777053457829423\n",
      "  batch 401 loss: 0.00014389512731895592\n",
      "LOSS train 0.00011668392103395144 valid 0.0001534801413072273\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.193236105085816e-07\n",
      "  batch 101 loss: 0.0001106644605800966\n",
      "  batch 201 loss: 0.00011830519986858689\n",
      "  batch 301 loss: 0.00011966105785631953\n",
      "  batch 401 loss: 0.00010070674056748885\n",
      "LOSS train 0.00010889560352032576 valid 0.00017781127826310694\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.4128323022741825e-07\n",
      "  batch 101 loss: 6.863970087010785e-05\n",
      "  batch 201 loss: 6.670555860068816e-05\n",
      "  batch 301 loss: 7.109773593043656e-05\n",
      "  batch 401 loss: 6.619110246845138e-05\n",
      "LOSS train 6.771174265852587e-05 valid 6.924023182364181e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.933145868970314e-08\n",
      "  batch 101 loss: 5.4369356909376166e-05\n",
      "  batch 201 loss: 5.7169602541193854e-05\n",
      "  batch 301 loss: 5.7394095568952254e-05\n",
      "  batch 401 loss: 4.9970476952694297e-05\n",
      "LOSS train 5.576768571626947e-05 valid 7.877607276896015e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.680143946548924e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 6.4572549147357e-05\n",
      "  batch 201 loss: 5.9338781735931433e-05\n",
      "  batch 301 loss: 7.187034385879088e-05\n",
      "  batch 401 loss: 6.233423715855224e-05\n",
      "LOSS train 6.535029600881303e-05 valid 0.00013735330139752477\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.9166863057762383e-06\n",
      "  batch 101 loss: 6.98206557650849e-05\n",
      "  batch 201 loss: 6.52168628765537e-05\n",
      "  batch 301 loss: 6.527587580507088e-05\n",
      "  batch 401 loss: 5.575926810877263e-05\n",
      "LOSS train 6.610658157696335e-05 valid 0.00011471202014945447\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.5464828175026924e-06\n",
      "  batch 101 loss: 7.89872504951461e-05\n",
      "  batch 201 loss: 7.243385874005525e-05\n",
      "  batch 301 loss: 6.617804057668764e-05\n",
      "  batch 401 loss: 5.756003730141401e-05\n",
      "LOSS train 7.104946635354743e-05 valid 9.807194874156266e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.2532967957668006e-06\n",
      "  batch 101 loss: 8.402170810143162e-05\n",
      "  batch 201 loss: 7.553375066919443e-05\n",
      "  batch 301 loss: 6.748925632066972e-05\n",
      "  batch 401 loss: 5.829978333906638e-05\n",
      "LOSS train 7.33997485012732e-05 valid 9.542944462737069e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.2042422167724e-06\n",
      "  batch 101 loss: 8.477565145767585e-05\n",
      "  batch 201 loss: 7.599246989428821e-05\n",
      "  batch 301 loss: 6.768696597532653e-05\n",
      "  batch 401 loss: 5.838691744770585e-05\n",
      "LOSS train 7.373905702401774e-05 valid 9.512128599453717e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.1984637967543675e-06\n",
      "  batch 101 loss: 8.486637662741714e-05\n",
      "  batch 201 loss: 7.605580170206849e-05\n",
      "  batch 301 loss: 6.771537018806839e-05\n",
      "  batch 401 loss: 5.839051135865247e-05\n",
      "LOSS train 7.37800251286205e-05 valid 9.508868970442563e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.2 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0008527353405952453\n",
      "  batch 101 loss: 159.87915075436234\n",
      "  batch 201 loss: 0.053137028510682284\n",
      "  batch 301 loss: 0.0005536697275056213\n",
      "  batch 401 loss: 6.693397062917938e-05\n",
      "LOSS train 36.102436126371764 valid 6.283367838477716e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.019392144982703e-07\n",
      "  batch 101 loss: 7.053215591440676e-05\n",
      "  batch 201 loss: 7.179340112998034e-05\n",
      "  batch 301 loss: 7.295943744793476e-05\n",
      "  batch 401 loss: 7.486066584533546e-05\n",
      "LOSS train 7.299782247639464e-05 valid 6.575648876605555e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.244845669949427e-07\n",
      "  batch 101 loss: 7.70313338489359e-05\n",
      "  batch 201 loss: 7.839870238058211e-05\n",
      "  batch 301 loss: 7.979343401530059e-05\n",
      "  batch 401 loss: 8.206306807096553e-05\n",
      "LOSS train 7.958545330165884e-05 valid 7.22535332897678e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.134964835131541e-07\n",
      "  batch 101 loss: 8.474344319893134e-05\n",
      "  batch 201 loss: 8.639891138955136e-05\n",
      "  batch 301 loss: 8.814884525691013e-05\n",
      "  batch 401 loss: 9.083837833031793e-05\n",
      "LOSS train 8.745236200267743e-05 valid 8.655206329422072e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.032115178531967e-06\n",
      "  batch 101 loss: 9.402962799185843e-05\n",
      "  batch 201 loss: 9.602361991710495e-05\n",
      "  batch 301 loss: 9.797973323657061e-05\n",
      "  batch 401 loss: 0.00010075441925891938\n",
      "LOSS train 9.648709624215295e-05 valid 0.00011634053225861862\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.574028719915077e-06\n",
      "  batch 101 loss: 0.00010381856026356218\n",
      "  batch 201 loss: 0.00010584723601823498\n",
      "  batch 301 loss: 0.00010701882144701358\n",
      "  batch 401 loss: 0.00010856353550252607\n",
      "LOSS train 0.00010461956021988145 valid 0.000166917045135051\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.3706478532403706e-06\n",
      "  batch 101 loss: 0.00010971075542499875\n",
      "  batch 201 loss: 0.00011046332425081573\n",
      "  batch 301 loss: 0.00010866471888846264\n",
      "  batch 401 loss: 0.00010678967982812538\n",
      "LOSS train 0.00010629620229779547 valid 0.00021581962937489152\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.0786881688982248e-06\n",
      "  batch 101 loss: 0.0001042453656549469\n",
      "  batch 201 loss: 0.0001021289437193218\n",
      "  batch 301 loss: 9.650106003562087e-05\n",
      "  batch 401 loss: 9.152581377350088e-05\n",
      "LOSS train 9.597475007700458e-05 valid 0.0002176286798203364\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.104155766777694e-06\n",
      "  batch 101 loss: 8.719220230716473e-05\n",
      "  batch 201 loss: 8.353671814234075e-05\n",
      "  batch 301 loss: 7.727922794458663e-05\n",
      "  batch 401 loss: 7.278416127178389e-05\n",
      "LOSS train 7.836554979580868e-05 valid 0.00018569664098322392\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.6475716731511055e-06\n",
      "  batch 101 loss: 7.039008623337395e-05\n",
      "  batch 201 loss: 6.756495394142803e-05\n",
      "  batch 301 loss: 6.328090382567098e-05\n",
      "  batch 401 loss: 6.094943252605844e-05\n",
      "LOSS train 6.451582628730708e-05 valid 0.000162311305757612\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.3015432816464455e-06\n",
      "  batch 101 loss: 6.151671735892705e-05\n",
      "  batch 201 loss: 6.01292369441353e-05\n",
      "  batch 301 loss: 5.785254511408766e-05\n",
      "  batch 401 loss: 5.7479016991521806e-05\n",
      "LOSS train 5.880788223503742e-05 valid 0.00015697565686423331\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 2.2208171139936896e-06\n",
      "  batch 101 loss: 6.038401500063628e-05\n",
      "  batch 201 loss: 5.996039823685351e-05\n",
      "  batch 301 loss: 5.8769024520870515e-05\n",
      "  batch 401 loss: 5.9441125591774836e-05\n",
      "LOSS train 5.966254616973743e-05 valid 0.00014682630717288703\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0.4 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 4.717340692877769e-05\n",
      "  batch 101 loss: 0.5244841757711947\n",
      "  batch 201 loss: 6.209067653799138e-05\n",
      "  batch 301 loss: 5.681276966882365e-05\n",
      "  batch 401 loss: 5.838453146651545e-05\n",
      "LOSS train 0.1184497981647621 valid 0.00012576965673360974\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7304634093306958e-06\n",
      "  batch 101 loss: 5.66741313446073e-05\n",
      "  batch 201 loss: 5.126299703874793e-05\n",
      "  batch 301 loss: 4.7400874501590805e-05\n",
      "  batch 401 loss: 4.6164333909928245e-05\n",
      "LOSS train 5.016650595119062e-05 valid 7.187877781689167e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.038766489131377e-07\n",
      "  batch 101 loss: 4.473151428555866e-05\n",
      "  batch 201 loss: 4.621683666186982e-05\n",
      "  batch 301 loss: 4.84975401536758e-05\n",
      "  batch 401 loss: 4.934994073380494e-05\n",
      "LOSS train 4.739016378440821e-05 valid 6.160960765555501e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.504981239326298e-07\n",
      "  batch 101 loss: 4.353624138587975e-05\n",
      "  batch 201 loss: 4.27170351790096e-05\n",
      "  batch 301 loss: 4.0200381994282e-05\n",
      "  batch 401 loss: 3.7763541871669304e-05\n",
      "LOSS train 4.108122536737099e-05 valid 6.513045082101598e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1775568054872565e-07\n",
      "  batch 101 loss: 3.583136533350739e-05\n",
      "  batch 201 loss: 3.568938351421025e-05\n",
      "  batch 301 loss: 3.612428900737541e-05\n",
      "  batch 401 loss: 3.647387892101506e-05\n",
      "LOSS train 3.652712110535789e-05 valid 6.244050018722191e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.7874509871471675e-07\n",
      "  batch 101 loss: 3.8259800282389735e-05\n",
      "  batch 201 loss: 3.898436105146175e-05\n",
      "  batch 301 loss: 4.221626682806345e-05\n",
      "  batch 401 loss: 4.5486961625442745e-05\n",
      "LOSS train 4.2030701648802165e-05 valid 9.647679689805955e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.2237860937602818e-06\n",
      "  batch 101 loss: 5.439620075634366e-05\n",
      "  batch 201 loss: 5.7834977117181555e-05\n",
      "  batch 301 loss: 6.447864083327204e-05\n",
      "  batch 401 loss: 6.212284749267383e-05\n",
      "LOSS train 5.9563530688932725e-05 valid 8.328659896505997e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.651228901930154e-07\n",
      "  batch 101 loss: 5.294164658494083e-05\n",
      "  batch 201 loss: 5.286416872081645e-05\n",
      "  batch 301 loss: 5.166100632720827e-05\n",
      "  batch 401 loss: 5.5394646259969705e-05\n",
      "LOSS train 5.3570395680023885e-05 valid 6.166366074467078e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.4142187612596897e-07\n",
      "  batch 101 loss: 5.4840956621191594e-05\n",
      "  batch 201 loss: 5.747611606125247e-05\n",
      "  batch 301 loss: 5.125725109223822e-05\n",
      "  batch 401 loss: 4.613246332837661e-05\n",
      "LOSS train 5.201956691972495e-05 valid 6.171319546410814e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.350586510146968e-07\n",
      "  batch 101 loss: 4.224347083777502e-05\n",
      "  batch 201 loss: 4.348288612931128e-05\n",
      "  batch 301 loss: 4.502147949324353e-05\n",
      "  batch 401 loss: 4.654514725331183e-05\n",
      "LOSS train 4.451611457076217e-05 valid 6.577624299097806e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 5.251714901532978e-07\n",
      "  batch 101 loss: 4.76037067727475e-05\n",
      "  batch 201 loss: 5.256936949706414e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 301 loss: 6.066767046775112e-05\n",
      "  batch 401 loss: 6.662639375463187e-05\n",
      "LOSS train 5.856616457072718e-05 valid 6.357180245686322e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.4997551261330955e-07\n",
      "  batch 101 loss: 6.197837299112053e-05\n",
      "  batch 201 loss: 7.195442999886836e-05\n",
      "  batch 301 loss: 9.475367164412773e-05\n",
      "  batch 401 loss: 9.905219628421947e-05\n",
      "LOSS train 8.311235076169439e-05 valid 0.00011548292968655005\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.000654267892241478\n",
      "  batch 101 loss: 15692.381910663877\n",
      "  batch 201 loss: 0.7106524324417114\n",
      "  batch 301 loss: 0.4845388248562813\n",
      "  batch 401 loss: 0.2983038254082203\n",
      "LOSS train 3542.6545289218257 valid 0.17317678034305573\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.001689155697822571\n",
      "  batch 101 loss: 0.12677618965506554\n",
      "  batch 201 loss: 0.06264772199094296\n",
      "  batch 301 loss: 0.028377020638436078\n",
      "  batch 401 loss: 0.011804972034879029\n",
      "LOSS train 0.05277998865167894 valid 0.004694050177931786\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.968419972807169e-05\n",
      "  batch 101 loss: 0.0028867824922781437\n",
      "  batch 201 loss: 0.000997724193148315\n",
      "  batch 301 loss: 0.00034092657089786373\n",
      "  batch 401 loss: 0.0001374741704017879\n",
      "LOSS train 0.001003671381219737 valid 8.033496123971418e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.1664200176019222e-08\n",
      "  batch 101 loss: 6.939190840967058e-05\n",
      "  batch 201 loss: 6.371233512254548e-05\n",
      "  batch 301 loss: 6.265020419050415e-05\n",
      "  batch 401 loss: 6.317467162261892e-05\n",
      "LOSS train 6.499305435359563e-05 valid 6.15829267189838e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.9208251362433657e-07\n",
      "  batch 101 loss: 6.329849016765365e-05\n",
      "  batch 201 loss: 6.35905979652307e-05\n",
      "  batch 301 loss: 6.349663601213252e-05\n",
      "  batch 401 loss: 6.399202986358432e-05\n",
      "LOSS train 6.406633102593685e-05 valid 6.16869074292481e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.1269817554857584e-07\n",
      "  batch 101 loss: 6.413483600226755e-05\n",
      "  batch 201 loss: 6.436012914491584e-05\n",
      "  batch 301 loss: 6.428604470784194e-05\n",
      "  batch 401 loss: 6.481872802396537e-05\n",
      "LOSS train 6.486495298361847e-05 valid 6.177958857733756e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.247977656428702e-07\n",
      "  batch 101 loss: 6.504593167846906e-05\n",
      "  batch 201 loss: 6.528178183543786e-05\n",
      "  batch 301 loss: 6.52647819788399e-05\n",
      "  batch 401 loss: 6.585900001482514e-05\n",
      "LOSS train 6.582035913396759e-05 valid 6.193198350956663e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.4049953683279456e-07\n",
      "  batch 101 loss: 6.619484811380971e-05\n",
      "  batch 201 loss: 6.645218717494572e-05\n",
      "  batch 301 loss: 6.651065212281537e-05\n",
      "  batch 401 loss: 6.71890468311176e-05\n",
      "LOSS train 6.703331470260548e-05 valid 6.218903581611812e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.6154920962871983e-07\n",
      "  batch 101 loss: 6.766227504158451e-05\n",
      "  batch 201 loss: 6.795896793846622e-05\n",
      "  batch 301 loss: 6.81144052214222e-05\n",
      "  batch 401 loss: 6.890827281495148e-05\n",
      "LOSS train 6.85908921192961e-05 valid 6.263385148486122e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.9059003029251474e-07\n",
      "  batch 101 loss: 6.955752975954965e-05\n",
      "  batch 201 loss: 6.992185872150003e-05\n",
      "  batch 301 loss: 7.020059120804945e-05\n",
      "  batch 401 loss: 7.11530797798332e-05\n",
      "LOSS train 7.061129911462004e-05 valid 6.342575215967372e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 4.3191666918573903e-07\n",
      "  batch 101 loss: 7.20318990170199e-05\n",
      "  batch 201 loss: 7.250499236761244e-05\n",
      "  batch 301 loss: 7.294263852600125e-05\n",
      "  batch 401 loss: 7.411114427668508e-05\n",
      "LOSS train 7.325617740929903e-05 valid 6.488049984909594e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.927554982714355e-07\n",
      "  batch 101 loss: 7.529403917487798e-05\n",
      "  batch 201 loss: 7.59312403988588e-05\n",
      "  batch 301 loss: 7.657998078684613e-05\n",
      "  batch 401 loss: 7.803605577009875e-05\n",
      "LOSS train 7.674274171123805e-05 valid 6.764417048543692e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.1 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00021276867017149925\n",
      "  batch 101 loss: 79440.61508860518\n",
      "  batch 201 loss: 34.63527430016548\n",
      "  batch 301 loss: 0.7126132738590241\n",
      "  batch 401 loss: 3.338313572406769\n",
      "LOSS train 17941.193041728086 valid 0.41116756200790405\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.004046193361282349\n",
      "  batch 101 loss: 0.35660720825195313\n",
      "  batch 201 loss: 0.2610557781159878\n",
      "  batch 301 loss: 0.18390834733843803\n",
      "  batch 401 loss: 0.12482681892812252\n",
      "LOSS train 0.21916877892095402 valid 0.08376626670360565\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0008079271018505096\n",
      "  batch 101 loss: 0.0670156442373991\n",
      "  batch 201 loss: 0.04146423539146781\n",
      "  batch 301 loss: 0.02465160921216011\n",
      "  batch 401 loss: 0.014089537914842368\n",
      "LOSS train 0.03430118737216728 valid 0.00799474772065878\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.055702619254589e-05\n",
      "  batch 101 loss: 0.00594803822459653\n",
      "  batch 201 loss: 0.003036714857444167\n",
      "  batch 301 loss: 0.0015149992081569508\n",
      "  batch 401 loss: 0.0008555039361817763\n",
      "LOSS train 0.002624710032809158 valid 0.00037293857894837856\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.5952158719301223e-06\n",
      "  batch 101 loss: 0.00026129096891963855\n",
      "  batch 201 loss: 0.000143056877369645\n",
      "  batch 301 loss: 9.222810904475409e-05\n",
      "  batch 401 loss: 7.306395499881546e-05\n",
      "LOSS train 0.00013628833892609206 valid 6.53850074741058e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.1358208212186583e-07\n",
      "  batch 101 loss: 6.339007193219004e-05\n",
      "  batch 201 loss: 6.2640238484164e-05\n",
      "  batch 301 loss: 6.222575038918876e-05\n",
      "  batch 401 loss: 6.267801009016694e-05\n",
      "LOSS train 6.317037546571598e-05 valid 6.155790470074862e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.814514846249949e-07\n",
      "  batch 101 loss: 6.265296028686861e-05\n",
      "  batch 201 loss: 6.290991449532157e-05\n",
      "  batch 301 loss: 6.276253498072037e-05\n",
      "  batch 401 loss: 6.320425889498438e-05\n",
      "LOSS train 6.335650757319766e-05 valid 6.161905912449583e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.0095934562268666e-07\n",
      "  batch 101 loss: 6.323858433461282e-05\n",
      "  batch 201 loss: 0.0006770901277377561\n",
      "  batch 301 loss: 6.349711390612355e-05\n",
      "  batch 401 loss: 6.380791481205961e-05\n",
      "LOSS train 0.00020247882042706192 valid 6.167451647343114e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.1081119232112543e-07\n",
      "  batch 101 loss: 6.386332805050188e-05\n",
      "  batch 201 loss: 6.405539606021194e-05\n",
      "  batch 301 loss: 6.39442101601162e-05\n",
      "  batch 401 loss: 6.444072535487066e-05\n",
      "LOSS train 6.454071646974288e-05 valid 6.1732716858387e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.1903629860607907e-07\n",
      "  batch 101 loss: 6.460804302150791e-05\n",
      "  batch 201 loss: 6.482441443040444e-05\n",
      "  batch 301 loss: 6.476594265222957e-05\n",
      "  batch 401 loss: 6.531586409437295e-05\n",
      "LOSS train 6.53379156389601e-05 valid 6.184555968502536e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.3205094950972125e-07\n",
      "  batch 101 loss: 6.557869652169757e-05\n",
      "  batch 201 loss: 6.581236597412499e-05\n",
      "  batch 301 loss: 6.58189958994626e-05\n",
      "  batch 401 loss: 6.643975980296091e-05\n",
      "LOSS train 6.636365269842985e-05 valid 6.203355587786064e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.493971235002391e-07\n",
      "  batch 101 loss: 6.682225724034651e-05\n",
      "  batch 201 loss: 6.708562214953417e-05\n",
      "  batch 301 loss: 6.717637899782857e-05\n",
      "  batch 401 loss: 6.789333246160822e-05\n",
      "LOSS train 6.768313704684162e-05 valid 6.235329783521593e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.2 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005498850345611573\n",
      "  batch 101 loss: 104168.99719521031\n",
      "  batch 201 loss: 1.680140934586525\n",
      "  batch 301 loss: 0.6729076105356216\n",
      "  batch 401 loss: 0.5551988333463669\n",
      "LOSS train 23515.147671039063 valid 0.4494347870349884\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.004562108814716339\n",
      "  batch 101 loss: 0.3963954174518585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 0.30035987675189973\n",
      "  batch 301 loss: 0.22017057359218598\n",
      "  batch 401 loss: 0.15608315289020538\n",
      "LOSS train 0.25448668859142215 valid 0.10962988436222076\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0011295874416828156\n",
      "  batch 101 loss: 0.09014573127031326\n",
      "  batch 201 loss: 0.05889825716614723\n",
      "  batch 301 loss: 0.037136529982089994\n",
      "  batch 401 loss: 0.02259988710284233\n",
      "LOSS train 0.04882105195613128 valid 0.013656070455908775\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 0.00014806509017944335\n",
      "  batch 101 loss: 0.01046860294882208\n",
      "  batch 201 loss: 0.005829620296135545\n",
      "  batch 301 loss: 0.0031365131004713474\n",
      "  batch 401 loss: 0.001631229431659449\n",
      "LOSS train 0.0048774845723265625 valid 0.0008521203999407589\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1036416981369258e-05\n",
      "  batch 101 loss: 0.0006290308661118615\n",
      "  batch 201 loss: 0.00032252127195533833\n",
      "  batch 301 loss: 0.00023165167920524254\n",
      "  batch 401 loss: 0.00010983210308040725\n",
      "LOSS train 0.0003021899191327819 valid 8.07132528279908e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.104662603931501e-07\n",
      "  batch 101 loss: 7.673802321733091e-05\n",
      "  batch 201 loss: 0.0001082709487491229\n",
      "  batch 301 loss: 6.435317147406749e-05\n",
      "  batch 401 loss: 6.340159742649121e-05\n",
      "LOSS train 7.722949897143812e-05 valid 6.184362428029999e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.3184245694428684e-07\n",
      "  batch 101 loss: 6.29383620525914e-05\n",
      "  batch 201 loss: 6.29001440211141e-05\n",
      "  batch 301 loss: 6.263190648496675e-05\n",
      "  batch 401 loss: 6.300818128693208e-05\n",
      "LOSS train 6.33349241409142e-05 valid 6.161336932564154e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9975586585351266e-07\n",
      "  batch 101 loss: 6.300176036347694e-05\n",
      "  batch 201 loss: 6.318609214758908e-05\n",
      "  batch 301 loss: 6.302179469457769e-05\n",
      "  batch 401 loss: 6.346187585222651e-05\n",
      "LOSS train 6.363834834261916e-05 valid 6.163938087411225e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.0490868084598334e-07\n",
      "  batch 101 loss: 6.351880877446092e-05\n",
      "  batch 201 loss: 6.371921441314043e-05\n",
      "  batch 301 loss: 6.358997753522999e-05\n",
      "  batch 401 loss: 8.912619337024808e-05\n",
      "LOSS train 6.984974152351909e-05 valid 6.167282117530704e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.105422729277052e-07\n",
      "  batch 101 loss: 6.415738758732914e-05\n",
      "  batch 201 loss: 6.439239618885039e-05\n",
      "  batch 301 loss: 6.431111797610356e-05\n",
      "  batch 401 loss: 6.483218863650109e-05\n",
      "LOSS train 6.488598286006119e-05 valid 6.17796104052104e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.247995482524857e-07\n",
      "  batch 101 loss: 6.504187208520307e-05\n",
      "  batch 201 loss: 6.526479027343158e-05\n",
      "  batch 301 loss: 6.523458115225367e-05\n",
      "  batch 401 loss: 6.581498852028744e-05\n",
      "LOSS train 6.579482852342093e-05 valid 6.192282307893038e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.3965196053031835e-07\n",
      "  batch 101 loss: 6.613050408304843e-05\n",
      "  batch 201 loss: 6.637571709688928e-05\n",
      "  batch 301 loss: 0.00014699762095915503\n",
      "  batch 401 loss: 6.700241342514346e-05\n",
      "LOSS train 8.511864502732466e-05 valid 6.216200563358143e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0.4 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002522932551801205\n",
      "  batch 101 loss: 96066.1121562487\n",
      "  batch 201 loss: 83.11888537943364\n",
      "  batch 301 loss: 0.8343732604384422\n",
      "  batch 401 loss: 0.4416131392121315\n",
      "LOSS train 21704.43955685368 valid 0.35346898436546326\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0034739512205123903\n",
      "  batch 101 loss: 0.30935026735067367\n",
      "  batch 201 loss: 0.23108133018016816\n",
      "  batch 301 loss: 0.1666193413734436\n",
      "  batch 401 loss: 0.11601433917880058\n",
      "LOSS train 0.19680685561747369 valid 0.08018603920936584\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0007727613300085068\n",
      "  batch 101 loss: 0.06621840488165617\n",
      "  batch 201 loss: 0.041890552435070276\n",
      "  batch 301 loss: 0.025511461440473795\n",
      "  batch 401 loss: 0.01513321184553206\n",
      "LOSS train 0.03473213450608956 valid 0.008922278881072998\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.931801490485669e-05\n",
      "  batch 101 loss: 0.006729018823243678\n",
      "  batch 201 loss: 0.0036067359219305217\n",
      "  batch 301 loss: 0.0022981941484613343\n",
      "  batch 401 loss: 0.0009491364532732405\n",
      "LOSS train 0.0032155832093635796 valid 0.0004920087521895766\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.4708351702429354e-06\n",
      "  batch 101 loss: 0.000400186972838128\n",
      "  batch 201 loss: 0.00027306411660902083\n",
      "  batch 301 loss: 0.00018949027519880702\n",
      "  batch 401 loss: 8.091249874325968e-05\n",
      "LOSS train 0.0002214116701363518 valid 6.853629020042717e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.549877864221344e-08\n",
      "  batch 101 loss: 6.497613836927486e-05\n",
      "  batch 201 loss: 7.824031631571415e-05\n",
      "  batch 301 loss: 6.224126724191592e-05\n",
      "  batch 401 loss: 0.0005279104758847097\n",
      "LOSS train 0.00017202554731400178 valid 6.157973257359117e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.911009869421832e-07\n",
      "  batch 101 loss: 7.894079719335423e-05\n",
      "  batch 201 loss: 6.285026707701035e-05\n",
      "  batch 301 loss: 0.0006399720401259402\n",
      "  batch 401 loss: 6.33036209455895e-05\n",
      "LOSS train 0.0002839536548646153 valid 6.210192805156112e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.549224493326619e-07\n",
      "  batch 101 loss: 7.794418690536985e-05\n",
      "  batch 201 loss: 6.352659460389986e-05\n",
      "  batch 301 loss: 6.321399599073629e-05\n",
      "  batch 401 loss: 6.360602500535606e-05\n",
      "LOSS train 6.718564799244533e-05 valid 6.165430386317894e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.07526788674295e-07\n",
      "  batch 101 loss: 6.366273249113874e-05\n",
      "  batch 201 loss: 7.521427213760035e-05\n",
      "  batch 301 loss: 6.378038291586563e-05\n",
      "  batch 401 loss: 6.423252695640258e-05\n",
      "LOSS train 6.691153613568897e-05 valid 6.171060522319749e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.1608473364030945e-07\n",
      "  batch 101 loss: 6.43653873976291e-05\n",
      "  batch 201 loss: 6.457516759837745e-05\n",
      "  batch 301 loss: 6.449961276302929e-05\n",
      "  batch 401 loss: 6.50317097097286e-05\n",
      "LOSS train 6.507910585605304e-05 valid 6.18057674728334e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.277755240560509e-07\n",
      "  batch 101 loss: 6.76838690196746e-05\n",
      "  batch 201 loss: 6.899664695993124e-05\n",
      "  batch 301 loss: 6.552502365593682e-05\n",
      "  batch 401 loss: 6.60779926511168e-05\n",
      "LOSS train 6.737968270247027e-05 valid 6.196682079462335e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 3.4365919418632987e-07\n",
      "  batch 101 loss: 6.641574370405578e-05\n",
      "  batch 201 loss: 6.666784452136199e-05\n",
      "  batch 301 loss: 6.673080856671732e-05\n",
      "  batch 401 loss: 8.719889457097452e-05\n",
      "LOSS train 9.162012379142888e-05 valid 6.242113158805296e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(3)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [4,8,12]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,1,2,3]\n",
    "dors = [0,0.1,0.2,0.4]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w\"\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            for dor in dors:\n",
    "                m = create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor)\n",
    "                m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor\"\n",
    "                optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                saved_models = dict()\n",
    "\n",
    "                for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                    path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                    model = m\n",
    "                    m.load_state_dict(torch.load(path))\n",
    "                    m.eval()\n",
    "\n",
    "                    test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                    test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "\n",
    "                    train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                    train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "\n",
    "                    validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                    validation_loss = loss_fn(validation_prediction,d_ft_out[\"val\"])\n",
    "\n",
    "                    if mt == \"min_val\": \n",
    "                        min_val = True\n",
    "                    else: \n",
    "                        min_val = False\n",
    "\n",
    "                    r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                      \"Min_val\":min_val,\n",
    "                                      \"Epochs\": nb_e,\n",
    "                                      \"Lr\":lr,\n",
    "                                      \"Dor\": dor,\n",
    "                                      \"Tr_l\":train_loss.item(),\n",
    "                                      \"Te_l\":test_loss.item(),\n",
    "                                      \"V_l\": validation_loss.item()}\n",
    "                                     ,index = [i]\n",
    "                    )\n",
    "                    i+=1\n",
    "                    results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45832a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (results.Model_type == 1) &  (results.Min_val ==True) & (results.Epochs !=8)\n",
    "results[f].boxplot(column = [\"Te_l\", \"Tr_l\",\"V_l\"],by = [\"Dor\"],layout = (3,1),sharey = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900a113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/10_exec_Hyperparam_multi_dor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63ca3e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epochs', ylabel='Te_l'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = (results.Dor ==0) &(results.Model_type ==2)  #& (results.Min_val == True)  \n",
    "sns.boxplot(x = \"Epochs\", y = \"Te_l\",data=results[f],hue = \"Min_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "837e6be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epochs', ylabel='Te_l'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = (results.Lr ==0.01)#(results.Dor ==0) #& (results.Min_val == True)  \n",
    "sns.boxplot(x = \"Epochs\", y = \"Te_l\",data=results[f],hue = \"Model_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbb2ea",
   "metadata": {},
   "source": [
    "## Old code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01*4**i for i in range(3)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [2,4]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,1,2,3]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            m = create_model(nb_hidden,input_size=d_ft_in['train'].shape[1])\n",
    "            m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "            \n",
    "            saved_models = dict()\n",
    "            \n",
    "            for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                path = f\"trained_models/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "                \n",
    "                model = m\n",
    "                m.load_state_dict(torch.load(path))\n",
    "                m.eval()\n",
    "\n",
    "                test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "                \n",
    "                train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "                \n",
    "                validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                validation_loss = loss_fn(validation_prediction,d_ft_out[\"val\"])\n",
    "\n",
    "                if mt == \"min_val\": \n",
    "                    min_val = True\n",
    "                else: \n",
    "                    min_val = False\n",
    "\n",
    "                r = pd.DataFrame({\"Model_type\": nb_hidden,\"Min_val\":min_val,\"Epochs\": nb_e,\"Lr\":lr, \"Tr_l\":train_loss.item(),\"Te_l\":test_loss.item(),\"V_l\": validation_loss.item()},index = [i]\n",
    "                )\n",
    "                i+=1\n",
    "                results = pd.concat([results,r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
