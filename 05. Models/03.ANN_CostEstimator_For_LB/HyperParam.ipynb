{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b42ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NN_classes\n",
    "from torchvision import datasets, transforms\n",
    "import training_methods\n",
    "import DataLoading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cca8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_101_N_105_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_102_N_106_cac1\",\"Network_Line_Out_N_103_N_109_cac1\"]\n",
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\"]\n",
    "#executions = [\"Network_Full_Generation_Full\",\"Network_Line_In_N_101_N_102_cac1\",\"Network_Line_In_N_101_N_103_cac1\",\"Network_Line_In_N_101_N_105_cac1\"]\n",
    "\n",
    "executions = [\"Network_Full_Generation_Full\"]\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"\"\n",
    "te_s = 0.1\n",
    "val_s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f16d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/input_f_sc01_Network_Full_Generation_Full_2030.csv\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te(dfs_in,dfs_out,executions,te_s,val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4032fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out = DataLoading.concat_and_normalize(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da40d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "training_loader = DataLoader(train,batch_size=32)\n",
    "validation_loader = DataLoader(train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b1e7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6289, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ft_in['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fb281c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.021659431457519532\n",
      "  batch 101 loss: 0.06365786358175683\n",
      "LOSS train 0.043642487576075355 valid 0.001716108527034521\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.6842046584934e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 0.0009185276090283878\n",
      "LOSS train 0.0006813842110435696 valid 0.0006692394381389022\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.761367941275239e-06\n",
      "  batch 101 loss: 0.0005948823177459417\n",
      "LOSS train 0.0004200123652213355 valid 0.0003975080617237836\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.4073300301097332e-06\n",
      "  batch 101 loss: 0.00038175742476596495\n",
      "LOSS train 0.000280763679192145 valid 0.0002890569157898426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([874])) that is different to the input size (torch.Size([874, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([6289])) that is different to the input size (torch.Size([6289, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 4.071578165525391e-06\n",
      "LOSS train 2.080302937462668e-06 valid 1.3629220063648972e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.112736711661455e-09\n",
      "  batch 101 loss: 5.769629255469866e-08\n",
      "LOSS train 4.336244556994104e-08 valid 1.1700075219778228e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.346970841761505e-10\n",
      "  batch 101 loss: 6.580151300061843e-08\n",
      "LOSS train 4.3894099671394986e-08 valid 1.4067407683171496e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.6401305847941784e-11\n",
      "  batch 101 loss: 1.2252791978584198e-07\n",
      "LOSS train 9.0020249064378e-08 valid 4.244759921334662e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002817269042134285\n",
      "  batch 101 loss: 0.019698464205139318\n",
      "LOSS train 0.010359330110225017 valid 0.00036127990460954607\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.844807554036379e-06\n",
      "  batch 101 loss: 0.00019741035894185188\n",
      "LOSS train 0.00014646924443746537 valid 7.013043796177953e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.665174103341997e-07\n",
      "  batch 101 loss: 3.353691118917368e-05\n",
      "LOSS train 2.6208709880422e-05 valid 8.87699206941761e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.914690624806098e-08\n",
      "  batch 101 loss: 8.444375604312881e-06\n",
      "LOSS train 1.4866638956370623e-05 valid 1.7343249055556953e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 6.715133140729357e-05\n",
      "LOSS train 3.4103742967940915e-05 valid 1.3678541677109024e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1173042935297417e-09\n",
      "  batch 101 loss: 5.507985941455296e-08\n",
      "LOSS train 4.174572949364256e-08 valid 1.1668080901472422e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.317551530330093e-10\n",
      "  batch 101 loss: 6.632424412789106e-08\n",
      "LOSS train 4.424051783683992e-08 valid 1.4546422733019426e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7629430116890036e-11\n",
      "  batch 101 loss: 1.210667685991318e-07\n",
      "LOSS train 8.871588102834877e-08 valid 6.930258678039536e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.855524891056121e-06\n",
      "  batch 101 loss: 0.4843986222043168\n",
      "LOSS train 0.2474948336586869 valid 0.005015888717025518\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.79993431083858e-05\n",
      "  batch 101 loss: 0.0008575227926939988\n",
      "LOSS train 0.0005195673546751756 valid 0.00011523092689458281\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.586779247503727e-07\n",
      "  batch 101 loss: 7.351050464649234e-05\n",
      "LOSS train 7.44414947274692e-05 valid 5.1358445489313453e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.012242061435245e-07\n",
      "  batch 101 loss: 0.00015404007277311393\n",
      "LOSS train 0.0002646726642914413 valid 0.00024337512149941176\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.001084015491099155\n",
      "LOSS train 0.0005503015787695451 valid 1.360140799988585e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1101608521357775e-09\n",
      "  batch 101 loss: 5.4018645948561425e-08\n",
      "LOSS train 4.094502018260467e-08 valid 1.1686979206615433e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.334921458048484e-10\n",
      "  batch 101 loss: 6.566149838294066e-08\n",
      "LOSS train 4.385277669476959e-08 valid 1.441576369387576e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7228097816164337e-11\n",
      "  batch 101 loss: 1.2125998162471062e-07\n",
      "LOSS train 8.903998209807574e-08 valid 6.930692819651085e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002313319444656372\n",
      "  batch 101 loss: 7.356820426289923\n",
      "LOSS train 3.749775364155719 valid 0.052875060588121414\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.00044665876775979997\n",
      "  batch 101 loss: 0.012680398903758033\n",
      "LOSS train 0.006914132243358199 valid 0.0001306407357333228\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.106925189262256e-07\n",
      "  batch 101 loss: 0.0004160117021820042\n",
      "LOSS train 0.0007446574785588262 valid 0.0011263735359534621\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.6056679780595007e-06\n",
      "  batch 101 loss: 0.003934022359244409\n",
      "LOSS train 0.005854242281368662 valid 0.004196881782263517\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.017384287001573354\n",
      "LOSS train 0.008824835846800078 valid 1.3303362322858447e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0825767304822876e-09\n",
      "  batch 101 loss: 5.37126554911449e-08\n",
      "LOSS train 4.0701755731156736e-08 valid 1.1689171941497989e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.336942241588986e-10\n",
      "  batch 101 loss: 6.544446227385592e-08\n",
      "LOSS train 4.372817862179368e-08 valid 1.4368113809837268e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.709266392983636e-11\n",
      "  batch 101 loss: 1.2135769222210158e-07\n",
      "LOSS train 8.919380558276444e-08 valid 6.92489194875634e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0007473932206630707\n",
      "  batch 101 loss: 0.0030073308964347232\n",
      "LOSS train 0.0020487874794041096 valid 0.00018845259910449386\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.579318385571242e-06\n",
      "  batch 101 loss: 0.0002045418640364005\n",
      "LOSS train 0.0001429457706093635 valid 2.2902979253558442e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.115063246805221e-07\n",
      "  batch 101 loss: 4.735198453772682e-05\n",
      "LOSS train 2.930174540517263e-05 valid 2.003491772484267e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.1163084536747192e-08\n",
      "  batch 101 loss: 2.6759222597760868e-06\n",
      "LOSS train 1.6797346025288943e-06 valid 4.0359122976951767e-07\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.1281270790041163e-09\n",
      "  batch 101 loss: 3.0807414217903785e-07\n",
      "LOSS train 3.8341338843827896e-07 valid 4.6757642735428817e-07\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.63690638400294e-10\n",
      "  batch 101 loss: 1.5271769535019076e-06\n",
      "LOSS train 3.531212767283421e-06 valid 7.336140242841793e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.9739704839594196e-08\n",
      "  batch 101 loss: 2.0942228455851364e-05\n",
      "LOSS train 3.860412025050657e-05 valid 3.631960498751141e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.980670488905162e-07\n",
      "  batch 101 loss: 0.0001715038295742488\n",
      "LOSS train 0.0005193508251936068 valid 0.0032875363249331713\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 4.071578165525391e-06\n",
      "LOSS train 2.080302937462668e-06 valid 1.3629220063648972e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.112736711661455e-09\n",
      "  batch 101 loss: 5.769629255469866e-08\n",
      "LOSS train 4.336244556994104e-08 valid 1.1700075219778228e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.346970841761505e-10\n",
      "  batch 101 loss: 6.580151300061843e-08\n",
      "LOSS train 4.3894099671394986e-08 valid 1.4067407683171496e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.6401305847941784e-11\n",
      "  batch 101 loss: 1.2252791978584198e-07\n",
      "LOSS train 9.0020249064378e-08 valid 4.244759921334662e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.675644772409669e-10\n",
      "  batch 101 loss: 6.82364773885169e-08\n",
      "LOSS train 5.129339161535512e-08 valid 1.3101491447287117e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.048133105451825e-11\n",
      "  batch 101 loss: 6.828989873919333e-08\n",
      "LOSS train 6.747988179119873e-08 valid 7.321183659314556e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.368648103853957e-10\n",
      "  batch 101 loss: 1.0319820639370647e-07\n",
      "LOSS train 8.503938902185924e-08 valid 1.4205116194432321e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.667973645984148e-11\n",
      "  batch 101 loss: 1.7175381556322477e-07\n",
      "LOSS train 1.22183857119915e-07 valid 2.7180444561736294e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.005692585706710816\n",
      "  batch 101 loss: 0.0164904689508694\n",
      "LOSS train 0.011320324793228182 valid 7.418775203404948e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0370652307756245e-06\n",
      "  batch 101 loss: 5.864517458576301e-05\n",
      "LOSS train 3.4213020448306696e-05 valid 6.596520051971311e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.1443699956289494e-08\n",
      "  batch 101 loss: 2.8615103109075335e-06\n",
      "LOSS train 2.666934327694469e-06 valid 1.5050073898237315e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3444114301819354e-08\n",
      "  batch 101 loss: 5.725090307464598e-06\n",
      "LOSS train 7.739542573851169e-06 valid 8.311857527587563e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.4993813238106668e-07\n",
      "  batch 101 loss: 2.008104845458547e-05\n",
      "LOSS train 6.003471516029039e-05 valid 4.47850143245887e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.527196764072869e-07\n",
      "  batch 101 loss: 0.00010396350088285544\n",
      "LOSS train 8.866144810961458e-05 valid 5.2037747082067654e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.4851875423337334e-07\n",
      "  batch 101 loss: 0.0020224543610993352\n",
      "LOSS train 0.003559418043544052 valid 0.00013774078979622573\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.3461314160376786e-06\n",
      "  batch 101 loss: 0.0001423329815531815\n",
      "LOSS train 0.00012047615735939082 valid 0.0002911327173933387\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 6.715133140729357e-05\n",
      "LOSS train 3.4103742967940915e-05 valid 1.3678541677109024e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1173042935297417e-09\n",
      "  batch 101 loss: 5.507985941455296e-08\n",
      "LOSS train 4.174572949364256e-08 valid 1.1668080901472422e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.317551530330093e-10\n",
      "  batch 101 loss: 6.632424412789106e-08\n",
      "LOSS train 4.424051783683992e-08 valid 1.4546422733019426e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7629430116890036e-11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 1.210667685991318e-07\n",
      "LOSS train 8.871588102834877e-08 valid 6.930258678039536e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.019805726647064e-10\n",
      "  batch 101 loss: 5.99706881687645e-08\n",
      "LOSS train 5.3144307435290316e-08 valid 1.781739733530685e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4599928377379e-10\n",
      "  batch 101 loss: 8.701891412887797e-08\n",
      "LOSS train 7.554065746702076e-08 valid 1.3896993777962052e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.591834799176468e-11\n",
      "  batch 101 loss: 1.5618407859463445e-07\n",
      "LOSS train 1.125280501661395e-07 valid 1.3739922088973344e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.6096900458157963e-11\n",
      "  batch 101 loss: 1.589693437931139e-07\n",
      "LOSS train 2.0151165553013887e-07 valid 8.689749364521049e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.005694522857666016\n",
      "  batch 101 loss: 0.28335835190817305\n",
      "LOSS train 0.1467900693252914 valid 0.00022956583416089416\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7252274847123771e-06\n",
      "  batch 101 loss: 0.00012038658342135022\n",
      "LOSS train 9.849481825929311e-05 valid 7.388478115899488e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.192742097890005e-07\n",
      "  batch 101 loss: 0.0001506175869371873\n",
      "LOSS train 0.00020018614659512898 valid 0.0001646692689973861\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.340642528608441e-06\n",
      "  batch 101 loss: 0.0003965091972168011\n",
      "LOSS train 0.000608946458475477 valid 0.0006467606290243566\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.291112672537565e-05\n",
      "  batch 101 loss: 0.0023492493976664266\n",
      "LOSS train 0.009240472706629857 valid 0.03145155310630798\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.00021100277081131935\n",
      "  batch 101 loss: 0.030499252855370286\n",
      "LOSS train 0.046759541368946375 valid 0.02089310623705387\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.0003501731529831886\n",
      "  batch 101 loss: 0.00859246300002269\n",
      "LOSS train 0.007911333265833423 valid 0.007154527120292187\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.1465625613927842e-05\n",
      "  batch 101 loss: 0.016712971859378742\n",
      "LOSS train 0.01549953700065126 valid 0.005170992109924555\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.001084015491099155\n",
      "LOSS train 0.0005503015787695451 valid 1.360140799988585e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1101608521357775e-09\n",
      "  batch 101 loss: 5.4018645948561425e-08\n",
      "LOSS train 4.094502018260467e-08 valid 1.1686979206615433e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.334921458048484e-10\n",
      "  batch 101 loss: 6.566149838294066e-08\n",
      "LOSS train 4.385277669476959e-08 valid 1.441576369387576e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7228097816164337e-11\n",
      "  batch 101 loss: 1.2125998162471062e-07\n",
      "LOSS train 8.903998209807574e-08 valid 6.930692819651085e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.020190130267111e-10\n",
      "  batch 101 loss: 5.994840561074355e-08\n",
      "LOSS train 5.321237823245624e-08 valid 1.7466016188905087e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.411350769586761e-10\n",
      "  batch 101 loss: 8.797843713770348e-08\n",
      "LOSS train 7.603583367501082e-08 valid 1.5573936806845268e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1376199537949105e-10\n",
      "  batch 101 loss: 1.5745378360576544e-07\n",
      "LOSS train 1.1319093942907927e-07 valid 1.5055446667133765e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0572723141422102e-10\n",
      "  batch 101 loss: 1.7451650911493032e-07\n",
      "LOSS train 2.0400749962667772e-07 valid 4.2583229742376716e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0029157134890556336\n",
      "  batch 101 loss: 7.250655517224223\n",
      "LOSS train 3.692511412421883 valid 0.04265763983130455\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0003316214308142662\n",
      "  batch 101 loss: 0.009840138495364954\n",
      "LOSS train 0.0052367355942483 valid 5.24691968166735e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.3835560568841175e-07\n",
      "  batch 101 loss: 1.732170438799585e-05\n",
      "LOSS train 2.5021207397566548e-05 valid 2.1192450731177814e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.6133219105540774e-07\n",
      "  batch 101 loss: 5.238154722746913e-05\n",
      "LOSS train 8.213259576190891e-05 valid 6.851276702946052e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.9857891311403366e-06\n",
      "  batch 101 loss: 0.00021961550871310464\n",
      "LOSS train 0.0003187543868315409 valid 0.00032016861950978637\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.706761243753135e-06\n",
      "  batch 101 loss: 0.0008483459112176206\n",
      "LOSS train 0.0013123574675690751 valid 0.0033300467766821384\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0411323746666312e-05\n",
      "  batch 101 loss: 0.005296231046741013\n",
      "LOSS train 0.004777182824134077 valid 0.002299524610862136\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.560546997003257e-06\n",
      "  batch 101 loss: 0.004325176302736509\n",
      "LOSS train 0.004042291448271046 valid 0.0007058906485326588\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.017384287001573354\n",
      "LOSS train 0.008824835846800078 valid 1.3303362322858447e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0825767304822876e-09\n",
      "  batch 101 loss: 5.37126554911449e-08\n",
      "LOSS train 4.0701755731156736e-08 valid 1.1689171941497989e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.336942241588986e-10\n",
      "  batch 101 loss: 6.544446227385592e-08\n",
      "LOSS train 4.372817862179368e-08 valid 1.4368113809837268e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.709266392983636e-11\n",
      "  batch 101 loss: 1.2135769222210158e-07\n",
      "LOSS train 8.919380558276444e-08 valid 6.92489194875634e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.015024484578135e-10\n",
      "  batch 101 loss: 5.989512211312942e-08\n",
      "LOSS train 5.304861540963891e-08 valid 1.7768449822597177e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4532463232796999e-10\n",
      "  batch 101 loss: 8.758077667203601e-08\n",
      "LOSS train 7.581879443958313e-08 valid 1.5220951610217526e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0832870600552269e-10\n",
      "  batch 101 loss: 1.5759172672047406e-07\n",
      "LOSS train 1.1323905478341966e-07 valid 1.4826023964076285e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0205297051868455e-10\n",
      "  batch 101 loss: 1.7420755701991197e-07\n",
      "LOSS train 2.0363004762357855e-07 valid 4.325113991399121e-07\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(4)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,3]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            if nb_hidden == 0: \n",
    "                m = NN_classes.ObjectiveEstimator_ANN_Single_layer(input_size=d_ft_in['train'].shape[1],output_size=1)\n",
    "                \n",
    "            elif nb_hidden == 3: \n",
    "                m = NN_classes.ObjectiveEstimator_ANN_3hidden_layer(input_size=d_ft_in['train'].shape[1],hidden_size1=int(d_ft_in['train'].shape[1]/4),hidden_size2=int(d_ft_in['train'].shape[1]/16),hidden_size3=int(d_ft_in['train'].shape[1]/64),output_size=1)\n",
    "            m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "            \n",
    "            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "            test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "            train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "            \n",
    "            r = pd.DataFrame({\"Model_type\": m_name,\"Epochs\": nb_e,\"Lr\":lr, \"Tr_l\":train_loss.item(),\"Te_l\":test_loss.item()},index = [i]\n",
    "            )\n",
    "            i+=1\n",
    "            results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450d1808",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.995044e-02</td>\n",
       "      <td>3.113820e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.880393e-08</td>\n",
       "      <td>5.301107e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.364681e-04</td>\n",
       "      <td>2.971305e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.932472e-08</td>\n",
       "      <td>5.343779e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.749902e-05</td>\n",
       "      <td>2.335273e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>6.936998e-08</td>\n",
       "      <td>5.347490e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.291427e-04</td>\n",
       "      <td>1.987444e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>6.930760e-08</td>\n",
       "      <td>5.342376e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.233874e-06</td>\n",
       "      <td>1.735164e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.290619e-08</td>\n",
       "      <td>3.574458e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.249689e-06</td>\n",
       "      <td>9.934462e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.761602e-08</td>\n",
       "      <td>8.002224e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.945337e-03</td>\n",
       "      <td>3.184862e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.236949e-07</td>\n",
       "      <td>4.828290e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.091165e-01</td>\n",
       "      <td>7.309395e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4.441067e-07</td>\n",
       "      <td>5.045854e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model_type  Epochs    Lr          Tr_l          Te_l\n",
       "0          sl       4  0.01  1.995044e-02  3.113820e-02\n",
       "1          3h       4  0.01  6.880393e-08  5.301107e-08\n",
       "2          sl       4  0.04  2.364681e-04  2.971305e-04\n",
       "3          3h       4  0.04  6.932472e-08  5.343779e-08\n",
       "4          sl       4  0.16  2.749902e-05  2.335273e-05\n",
       "5          3h       4  0.16  6.936998e-08  5.347490e-08\n",
       "6          sl       4  0.64  2.291427e-04  1.987444e-04\n",
       "7          3h       4  0.64  6.930760e-08  5.342376e-08\n",
       "8          sl       8  0.01  2.233874e-06  1.735164e-06\n",
       "9          3h       8  0.01  2.290619e-08  3.574458e-08\n",
       "10         sl       8  0.04  6.249689e-06  9.934462e-06\n",
       "11         3h       8  0.04  5.761602e-08  8.002224e-08\n",
       "12         sl       8  0.16  1.945337e-03  3.184862e-03\n",
       "13         3h       8  0.16  4.236949e-07  4.828290e-07\n",
       "14         sl       8  0.64  1.091165e-01  7.309395e-02\n",
       "15         3h       8  0.64  4.441067e-07  5.045854e-07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
