{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f6268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NN_classes\n",
    "from torchvision import datasets, transforms\n",
    "import training_methods\n",
    "import DataLoading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31ac245",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_101_N_105_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_102_N_106_cac1\",\"Network_Line_Out_N_103_N_109_cac1\"]\n",
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\"]\n",
    "#executions = [\"Network_Full_Generation_Full\",\"Network_Line_In_N_101_N_102_cac1\",\"Network_Line_In_N_101_N_103_cac1\",\"Network_Line_In_N_101_N_105_cac1\"]\n",
    "\n",
    "executions = [\"Network_Full_Generation_Full\"]\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"\"\n",
    "te_s = 0.1\n",
    "val_s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53556be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/input_f_sc01_Network_Full_Generation_Full_2030.csv\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f201c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te(dfs_in,dfs_out,executions,te_s,val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea61271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out = DataLoading.concat_and_normalize(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c6b25bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "training_loader = DataLoader(train,batch_size=32)\n",
    "validation_loader = DataLoader(train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e62777c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6289, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ft_in['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dabd7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.006295846700668335\n",
      "  batch 101 loss: 0.017080060772568687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.01212983794351106 valid 0.0007143783150240779\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.80630866251886e-06\n",
      "  batch 101 loss: 0.0003579086325044045\n",
      "LOSS train 0.0002421375833973844 valid 0.00010843825293704867\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.954517862643115e-07\n",
      "  batch 101 loss: 6.577696738531813e-05\n",
      "LOSS train 4.820274608128475e-05 valid 4.6465393097605556e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.481839510437567e-07\n",
      "  batch 101 loss: 1.9165457265444273e-05\n",
      "LOSS train 1.3372276907137873e-05 valid 1.8339544112677686e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([874])) that is different to the input size (torch.Size([874, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([6289])) that is different to the input size (torch.Size([6289, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 4.071578165525391e-06\n",
      "LOSS train 2.080302937462668e-06 valid 1.3629220063648972e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.112736711661455e-09\n",
      "  batch 101 loss: 5.769629255469866e-08\n",
      "LOSS train 4.336244556994104e-08 valid 1.1700075219778228e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.346970841761505e-10\n",
      "  batch 101 loss: 6.580151300061843e-08\n",
      "LOSS train 4.3894099671394986e-08 valid 1.4067407683171496e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.6401305847941784e-11\n",
      "  batch 101 loss: 1.2252791978584198e-07\n",
      "LOSS train 9.0020249064378e-08 valid 4.244759921334662e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.2690004659816622e-05\n",
      "  batch 101 loss: 0.028640668265434214\n",
      "LOSS train 0.014704008436254917 valid 0.0013121165102347732\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.068656363524496e-06\n",
      "  batch 101 loss: 0.0003297891774764139\n",
      "LOSS train 0.00022873046148140993 valid 7.623353303642944e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.422628990141675e-07\n",
      "  batch 101 loss: 6.410544303435017e-05\n",
      "LOSS train 9.103432678549432e-05 valid 8.753203292144462e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.4353323169634677e-07\n",
      "  batch 101 loss: 0.0003675513414782472\n",
      "LOSS train 0.0006675251787676869 valid 0.0007503345259465277\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 6.715133140729357e-05\n",
      "LOSS train 3.4103742967940915e-05 valid 1.3678541677109024e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1173042935297417e-09\n",
      "  batch 101 loss: 5.507985941455296e-08\n",
      "LOSS train 4.174572949364256e-08 valid 1.1668080901472422e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.317551530330093e-10\n",
      "  batch 101 loss: 6.632424412789106e-08\n",
      "LOSS train 4.424051783683992e-08 valid 1.4546422733019426e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7629430116890036e-11\n",
      "  batch 101 loss: 1.210667685991318e-07\n",
      "LOSS train 8.871588102834877e-08 valid 6.930258678039536e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004979961737990379\n",
      "  batch 101 loss: 0.42567390613578027\n",
      "LOSS train 0.21675203403032042 valid 0.003265007399022579\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.904314547777176e-05\n",
      "  batch 101 loss: 0.000944339373781986\n",
      "LOSS train 0.0005539122861208562 valid 0.00011646082566585392\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.497021680232137e-07\n",
      "  batch 101 loss: 0.00013100269972710521\n",
      "LOSS train 0.0002589486282904822 valid 0.0003931852988898754\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 9.472841338720173e-07\n",
      "  batch 101 loss: 0.0010520543294478557\n",
      "LOSS train 0.0016639286374354435 valid 0.001546315848827362\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.001084015491099155\n",
      "LOSS train 0.0005503015787695451 valid 1.360140799988585e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1101608521357775e-09\n",
      "  batch 101 loss: 5.4018645948561425e-08\n",
      "LOSS train 4.094502018260467e-08 valid 1.1686979206615433e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.334921458048484e-10\n",
      "  batch 101 loss: 6.566149838294066e-08\n",
      "LOSS train 4.385277669476959e-08 valid 1.441576369387576e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7228097816164337e-11\n",
      "  batch 101 loss: 1.2125998162471062e-07\n",
      "LOSS train 8.903998209807574e-08 valid 6.930692819651085e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0037169623374938964\n",
      "  batch 101 loss: 7.1753458371851595\n",
      "LOSS train 3.655350702690365 valid 0.04964975640177727\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0004341864213347435\n",
      "  batch 101 loss: 0.012177273556189902\n",
      "LOSS train 0.006638226703161386 valid 6.781108095310628e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.724768900312484e-07\n",
      "  batch 101 loss: 0.0001548085302420077\n",
      "LOSS train 0.0003095658039040379 valid 0.0005179497529752553\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.46593592944555e-06\n",
      "  batch 101 loss: 0.00167910009253319\n",
      "LOSS train 0.0023292086391889536 valid 0.001623768825083971\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.017384287001573354\n",
      "LOSS train 0.008824835846800078 valid 1.3303362322858447e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0825767304822876e-09\n",
      "  batch 101 loss: 5.37126554911449e-08\n",
      "LOSS train 4.0701755731156736e-08 valid 1.1689171941497989e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.336942241588986e-10\n",
      "  batch 101 loss: 6.544446227385592e-08\n",
      "LOSS train 4.372817862179368e-08 valid 1.4368113809837268e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.709266392983636e-11\n",
      "  batch 101 loss: 1.2135769222210158e-07\n",
      "LOSS train 8.919380558276444e-08 valid 6.92489194875634e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0074876976013183594\n",
      "  batch 101 loss: 0.01917592623649398\n",
      "LOSS train 0.013731064349587246 valid 0.000475908862426877\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.0664583472535015e-06\n",
      "  batch 101 loss: 0.00031058059888891873\n",
      "LOSS train 0.0002748574565354141 valid 0.0002965219027828425\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.694312519859523e-06\n",
      "  batch 101 loss: 0.000149067118527455\n",
      "LOSS train 0.00011747891429355509 valid 9.330245666205883e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.5544961206614972e-06\n",
      "  batch 101 loss: 4.9155994993270724e-05\n",
      "LOSS train 3.7251022311161226e-05 valid 3.919572554877959e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.646164547419176e-07\n",
      "  batch 101 loss: 1.2843622617992879e-05\n",
      "LOSS train 9.298955344928601e-06 valid 3.03444949167897e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.701427421125117e-08\n",
      "  batch 101 loss: 2.845266940454394e-06\n",
      "LOSS train 1.9340804416827517e-06 valid 5.534834599529859e-07\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.7034767552104312e-08\n",
      "  batch 101 loss: 6.009480980573301e-07\n",
      "LOSS train 3.890405796994617e-07 valid 1.2462378151667508e-07\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.3363335205649492e-09\n",
      "  batch 101 loss: 1.0491548806612627e-07\n",
      "LOSS train 7.507794065179034e-08 valid 3.695932093705778e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 4.071578165525391e-06\n",
      "LOSS train 2.080302937462668e-06 valid 1.3629220063648972e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.112736711661455e-09\n",
      "  batch 101 loss: 5.769629255469866e-08\n",
      "LOSS train 4.336244556994104e-08 valid 1.1700075219778228e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.346970841761505e-10\n",
      "  batch 101 loss: 6.580151300061843e-08\n",
      "LOSS train 4.3894099671394986e-08 valid 1.4067407683171496e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.6401305847941784e-11\n",
      "  batch 101 loss: 1.2252791978584198e-07\n",
      "LOSS train 9.0020249064378e-08 valid 4.244759921334662e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.675644772409669e-10\n",
      "  batch 101 loss: 6.82364773885169e-08\n",
      "LOSS train 5.129339161535512e-08 valid 1.3101491447287117e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.048133105451825e-11\n",
      "  batch 101 loss: 6.828989873919333e-08\n",
      "LOSS train 6.747988179119873e-08 valid 7.321183659314556e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.368648103853957e-10\n",
      "  batch 101 loss: 1.0319820639370647e-07\n",
      "LOSS train 8.503938902185924e-08 valid 1.4205116194432321e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.667973645984148e-11\n",
      "  batch 101 loss: 1.7175381556322477e-07\n",
      "LOSS train 1.22183857119915e-07 valid 2.7180444561736294e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.010852396488189697\n",
      "  batch 101 loss: 0.028767022493702825\n",
      "LOSS train 0.020503710296279353 valid 0.0007154995109885931\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.61701725423336e-06\n",
      "  batch 101 loss: 0.00023503068907302805\n",
      "LOSS train 0.0002027810573017078 valid 0.00015611978597007692\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.203740685014054e-06\n",
      "  batch 101 loss: 7.637281132247153e-05\n",
      "LOSS train 7.057655399788521e-05 valid 9.308070730185136e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2239233658183365e-07\n",
      "  batch 101 loss: 0.00030835111496571696\n",
      "LOSS train 0.0009075486448066928 valid 0.0024133771657943726\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.21648970991373e-06\n",
      "  batch 101 loss: 0.024720067596936134\n",
      "LOSS train 0.02240566845273695 valid 0.0012085356283932924\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.936348901130259e-06\n",
      "  batch 101 loss: 0.0010515697832124715\n",
      "LOSS train 0.0006315520416686977 valid 9.909275831887498e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0620419925544411e-06\n",
      "  batch 101 loss: 3.6706260389109956e-05\n",
      "LOSS train 5.940766273796804e-05 valid 0.00010339520667912439\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.578054954938125e-07\n",
      "  batch 101 loss: 0.00036428666608117054\n",
      "LOSS train 0.001113020730792349 valid 0.0067087216302752495\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 6.715133140729357e-05\n",
      "LOSS train 3.4103742967940915e-05 valid 1.3678541677109024e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1173042935297417e-09\n",
      "  batch 101 loss: 5.507985941455296e-08\n",
      "LOSS train 4.174572949364256e-08 valid 1.1668080901472422e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.317551530330093e-10\n",
      "  batch 101 loss: 6.632424412789106e-08\n",
      "LOSS train 4.424051783683992e-08 valid 1.4546422733019426e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7629430116890036e-11\n",
      "  batch 101 loss: 1.210667685991318e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 8.871588102834877e-08 valid 6.930258678039536e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.019805726647064e-10\n",
      "  batch 101 loss: 5.99706881687645e-08\n",
      "LOSS train 5.3144307435290316e-08 valid 1.781739733530685e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4599928377379e-10\n",
      "  batch 101 loss: 8.701891412887797e-08\n",
      "LOSS train 7.554065746702076e-08 valid 1.3896993777962052e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.591834799176468e-11\n",
      "  batch 101 loss: 1.5618407859463445e-07\n",
      "LOSS train 1.125280501661395e-07 valid 1.3739922088973344e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.6096900458157963e-11\n",
      "  batch 101 loss: 1.589693437931139e-07\n",
      "LOSS train 2.0151165553013887e-07 valid 8.689749364521049e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.004917735755443573\n",
      "  batch 101 loss: 0.2954124357126784\n",
      "LOSS train 0.15255148189069512 valid 7.662212738068774e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5257956692948938e-06\n",
      "  batch 101 loss: 0.0001645794152682356\n",
      "LOSS train 0.00010819698615813912 valid 6.722144462401047e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.128822522237897e-07\n",
      "  batch 101 loss: 5.3409408825473295e-05\n",
      "LOSS train 4.224637568938472e-05 valid 7.243686923175119e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.6515667741477955e-08\n",
      "  batch 101 loss: 4.695835396262282e-06\n",
      "LOSS train 7.881569230123325e-06 valid 5.331907232175581e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2259500838117676e-08\n",
      "  batch 101 loss: 8.175912604428959e-06\n",
      "LOSS train 8.821273101676723e-06 valid 6.5092172008007765e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.576334377925378e-08\n",
      "  batch 101 loss: 9.927846602408863e-05\n",
      "LOSS train 0.00018653314604305217 valid 4.15559952671174e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0427793313283474e-07\n",
      "  batch 101 loss: 4.828945253962047e-05\n",
      "LOSS train 4.373902846148478e-05 valid 1.0304108400305267e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.403392722451827e-08\n",
      "  batch 101 loss: 0.00012070256390700251\n",
      "LOSS train 0.01368376240783668 valid 0.004448330029845238\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.001084015491099155\n",
      "LOSS train 0.0005503015787695451 valid 1.360140799988585e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1101608521357775e-09\n",
      "  batch 101 loss: 5.4018645948561425e-08\n",
      "LOSS train 4.094502018260467e-08 valid 1.1686979206615433e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.334921458048484e-10\n",
      "  batch 101 loss: 6.566149838294066e-08\n",
      "LOSS train 4.385277669476959e-08 valid 1.441576369387576e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.7228097816164337e-11\n",
      "  batch 101 loss: 1.2125998162471062e-07\n",
      "LOSS train 8.903998209807574e-08 valid 6.930692819651085e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.020190130267111e-10\n",
      "  batch 101 loss: 5.994840561074355e-08\n",
      "LOSS train 5.321237823245624e-08 valid 1.7466016188905087e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.411350769586761e-10\n",
      "  batch 101 loss: 8.797843713770348e-08\n",
      "LOSS train 7.603583367501082e-08 valid 1.5573936806845268e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1376199537949105e-10\n",
      "  batch 101 loss: 1.5745378360576544e-07\n",
      "LOSS train 1.1319093942907927e-07 valid 1.5055446667133765e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0572723141422102e-10\n",
      "  batch 101 loss: 1.7451650911493032e-07\n",
      "LOSS train 2.0400749962667772e-07 valid 4.2583229742376716e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.001231733113527298\n",
      "  batch 101 loss: 7.522323031974956\n",
      "LOSS train 3.8354145112529583 valid 0.08259326964616776\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0006613750010728836\n",
      "  batch 101 loss: 0.016794075296602387\n",
      "LOSS train 0.009006179550842578 valid 6.395943637471646e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.8670565118081866e-07\n",
      "  batch 101 loss: 2.1174709283968697e-05\n",
      "LOSS train 3.46140239714587e-05 valid 8.441896352451295e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.657231304212474e-07\n",
      "  batch 101 loss: 0.00012811614555175766\n",
      "LOSS train 0.00012302446527294515 valid 3.234246469219215e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.000210512662306e-07\n",
      "  batch 101 loss: 0.00011312773436657153\n",
      "LOSS train 0.00015304865318686666 valid 0.0001953184837475419\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.7827595002017916e-06\n",
      "  batch 101 loss: 0.00032770562647783664\n",
      "LOSS train 0.0005361256376898677 valid 0.0006517103756777942\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.3668748326599597e-06\n",
      "  batch 101 loss: 0.0011094528919420555\n",
      "LOSS train 0.0010545322305924995 valid 0.0006500514573417604\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.0380734824575485e-06\n",
      "  batch 101 loss: 0.020779267340985827\n",
      "LOSS train 0.027820470875990697 valid 0.0054037426598370075\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.03668945334357e-10\n",
      "  batch 101 loss: 0.017384287001573354\n",
      "LOSS train 0.008824835846800078 valid 1.3303362322858447e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0825767304822876e-09\n",
      "  batch 101 loss: 5.37126554911449e-08\n",
      "LOSS train 4.0701755731156736e-08 valid 1.1689171941497989e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.336942241588986e-10\n",
      "  batch 101 loss: 6.544446227385592e-08\n",
      "LOSS train 4.372817862179368e-08 valid 1.4368113809837268e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.709266392983636e-11\n",
      "  batch 101 loss: 1.2135769222210158e-07\n",
      "LOSS train 8.919380558276444e-08 valid 6.92489194875634e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.015024484578135e-10\n",
      "  batch 101 loss: 5.989512211312942e-08\n",
      "LOSS train 5.304861540963891e-08 valid 1.7768449822597177e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.4532463232796999e-10\n",
      "  batch 101 loss: 8.758077667203601e-08\n",
      "LOSS train 7.581879443958313e-08 valid 1.5220951610217526e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0832870600552269e-10\n",
      "  batch 101 loss: 1.5759172672047406e-07\n",
      "LOSS train 1.1323905478341966e-07 valid 1.4826023964076285e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0205297051868455e-10\n",
      "  batch 101 loss: 1.7420755701991197e-07\n",
      "LOSS train 2.0363004762357855e-07 valid 4.325113991399121e-07\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(4)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,3]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            if nb_hidden == 0: \n",
    "                m = NN_classes.ObjectiveEstimator_ANN_Single_layer(input_size=d_ft_in['train'].shape[1],output_size=1)\n",
    "                \n",
    "            elif nb_hidden == 3: \n",
    "                m = NN_classes.ObjectiveEstimator_ANN_3hidden_layer(input_size=d_ft_in['train'].shape[1],hidden_size1=int(d_ft_in['train'].shape[1]/4),hidden_size2=int(d_ft_in['train'].shape[1]/16),hidden_size3=int(d_ft_in['train'].shape[1]/64),output_size=1)\n",
    "            m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "            \n",
    "            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "            test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "            train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "            \n",
    "            r = pd.DataFrame({\"Model_type\": m_name,\"Epochs\": nb_e,\"Lr\":lr, \"Tr_l\":train_loss.item(),\"Te_l\":test_loss.item()},index = [i]\n",
    "            )\n",
    "            i+=1\n",
    "            results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c322092e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.995044e-02</td>\n",
       "      <td>3.113820e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.880393e-08</td>\n",
       "      <td>5.301107e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.364681e-04</td>\n",
       "      <td>2.971305e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.932472e-08</td>\n",
       "      <td>5.343779e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.749902e-05</td>\n",
       "      <td>2.335273e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>6.936998e-08</td>\n",
       "      <td>5.347490e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.291427e-04</td>\n",
       "      <td>1.987444e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>6.930760e-08</td>\n",
       "      <td>5.342376e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.233874e-06</td>\n",
       "      <td>1.735164e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.290619e-08</td>\n",
       "      <td>3.574458e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.249689e-06</td>\n",
       "      <td>9.934462e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.761602e-08</td>\n",
       "      <td>8.002224e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.945337e-03</td>\n",
       "      <td>3.184862e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.236949e-07</td>\n",
       "      <td>4.828290e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.091165e-01</td>\n",
       "      <td>7.309395e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4.441067e-07</td>\n",
       "      <td>5.045854e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model_type  Epochs    Lr          Tr_l          Te_l\n",
       "0          sl       4  0.01  1.995044e-02  3.113820e-02\n",
       "1          3h       4  0.01  6.880393e-08  5.301107e-08\n",
       "2          sl       4  0.04  2.364681e-04  2.971305e-04\n",
       "3          3h       4  0.04  6.932472e-08  5.343779e-08\n",
       "4          sl       4  0.16  2.749902e-05  2.335273e-05\n",
       "5          3h       4  0.16  6.936998e-08  5.347490e-08\n",
       "6          sl       4  0.64  2.291427e-04  1.987444e-04\n",
       "7          3h       4  0.64  6.930760e-08  5.342376e-08\n",
       "8          sl       8  0.01  2.233874e-06  1.735164e-06\n",
       "9          3h       8  0.01  2.290619e-08  3.574458e-08\n",
       "10         sl       8  0.04  6.249689e-06  9.934462e-06\n",
       "11         3h       8  0.04  5.761602e-08  8.002224e-08\n",
       "12         sl       8  0.16  1.945337e-03  3.184862e-03\n",
       "13         3h       8  0.16  4.236949e-07  4.828290e-07\n",
       "14         sl       8  0.64  1.091165e-01  7.309395e-02\n",
       "15         3h       8  0.64  4.441067e-07  5.045854e-07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
