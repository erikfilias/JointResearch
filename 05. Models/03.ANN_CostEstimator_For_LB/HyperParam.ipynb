{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b42ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NN_classes\n",
    "from torchvision import datasets, transforms\n",
    "import training_methods\n",
    "import DataLoading\n",
    "import pivottablejs\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cca8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_101_N_105_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_102_N_106_cac1\",\"Network_Line_Out_N_103_N_109_cac1\"]\n",
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\"]\n",
    "#executions = [\"Network_Full_Generation_Full\",\"Network_Line_In_N_101_N_102_cac1\",\"Network_Line_In_N_101_N_103_cac1\",\"Network_Line_In_N_101_N_105_cac1\"]\n",
    "\n",
    "executions = [\"Network_Existing_Generation_Full\"]\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"Samples_3-bus_ACOPF\"\n",
    "te_s = 0.1\n",
    "val_s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f16d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te(dfs_in,dfs_out,executions,te_s,val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4032fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out = DataLoading.concat_and_normalize(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da40d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "training_loader = DataLoader(train,batch_size=32)\n",
    "validation_loader = DataLoader(train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59913ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nb_hidden,input_size,nb_neurons = (0,0),dor = None):\n",
    "    if dor is None: \n",
    "        if nb_hidden == 0: \n",
    "            m = NN_classes.ObjectiveEstimator_ANN_Single_layer(input_size=input_size,output_size=1)  \n",
    "        elif nb_hidden == 1: \n",
    "            hs1 = int(math.sqrt(d_ft_in['train'].shape[1]))\n",
    "            m = NN_classes.ObjectiveEstimator_ANN_1hidden_layer(input_size=input_size,hidden_size1=hs1,output_size=1)  \n",
    "        elif nb_hidden == 2: \n",
    "            hs1 = int(math.sqrt(d_ft_in['train'].shape[1]))\n",
    "            hs2 = int(math.sqrt(math.sqrt(d_ft_in['train'].shape[1])))\n",
    "            m = NN_classes.ObjectiveEstimator_ANN_2hidden_layer(input_size=input_size,hidden_size1=hs1,hidden_size2=hs2,output_size=1)\n",
    "        elif nb_hidden == 3:\n",
    "            hs1 = int(d_ft_in['train'].shape[1]/4)\n",
    "            hs2 = int(d_ft_in['train'].shape[1]/16)\n",
    "            hs3 = int(d_ft_in['train'].shape[1]/64)\n",
    "            m = NN_classes.ObjectiveEstimator_ANN_3hidden_layer(input_size=input_size,hidden_size1=hs1,hidden_size2=hs2,hidden_size3=hs3,output_size=1)\n",
    "    else: \n",
    "        if nb_hidden == 0: \n",
    "            m = NN_classes.ObjectiveEstimator_ANN_Single_layer_dropout(input_size=input_size,output_size=1,do_r=dor)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fb281c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 4.6571502462029454e-05\n",
      "  batch 101 loss: 0.0013790533083374611\n",
      "LOSS train 0.0009055268209646834 valid 0.00044891759171150625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([874])) that is different to the input size (torch.Size([874, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([6289])) that is different to the input size (torch.Size([6289, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1573])) that is different to the input size (torch.Size([1573, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0020991817116737367\n",
      "  batch 101 loss: 0.0050303048756723005\n",
      "LOSS train 0.003632892649771554 valid 3.5523600672604516e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.006295127868652344\n",
      "  batch 101 loss: 0.08536954214301659\n",
      "LOSS train 0.04653927889799926 valid 3.754870192551607e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 3.5267525583027036e-06\n",
      "LOSS train 1.8042565276598634e-06 valid 1.8270878854309558e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0010562753677368164\n",
      "  batch 101 loss: 0.014466171069943811\n",
      "LOSS train 0.008082097207892184 valid 0.00016070675337687135\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003597838431596756\n",
      "  batch 101 loss: 0.002460116074262686\n",
      "LOSS train 0.0014314965425196806 valid 1.7237061911146156e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002504109740257263\n",
      "  batch 101 loss: 0.00394940465926993\n",
      "LOSS train 0.0033241374578007387 valid 5.515299199032597e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 6.484566458188068e-05\n",
      "LOSS train 3.2936733340913424e-05 valid 1.6291983229166362e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002084341496229172\n",
      "  batch 101 loss: 0.35659139283001423\n",
      "LOSS train 0.18220902406612025 valid 0.00014486690633930266\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00016869259998202323\n",
      "  batch 101 loss: 0.0797567473975251\n",
      "LOSS train 0.04057172538651087 valid 2.801217213743712e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00048667598515748976\n",
      "  batch 101 loss: 0.0038080068785054876\n",
      "LOSS train 0.002180105968809365 valid 2.7112816880503487e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.0010746598370171335\n",
      "LOSS train 0.0005455558131975714 valid 1.551010342382142e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0053390759229660035\n",
      "  batch 101 loss: 0.014242091237538262\n",
      "LOSS train 0.010262296087584851 valid 0.0005825343541800976\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2906885240226984e-05\n",
      "  batch 101 loss: 0.000532906861481024\n",
      "LOSS train 0.0004570710815212732 valid 0.0004048874252475798\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002277916967868805\n",
      "  batch 101 loss: 0.019435057101435404\n",
      "LOSS train 0.011028091855243087 valid 1.3440850125334691e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.596151352918241e-07\n",
      "  batch 101 loss: 2.3774136621455e-05\n",
      "LOSS train 1.7308716377502018e-05 valid 2.489611506462097e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 6.358406972140074e-07\n",
      "  batch 101 loss: 0.0001731739367528462\n",
      "LOSS train 9.330182981505362e-05 valid 4.9031928028853144e-06\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.738184417074081e-08\n",
      "  batch 101 loss: 1.277394832641221e-05\n",
      "LOSS train 9.469057461208345e-06 valid 2.9182851903897244e-06\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 3.5267525583027036e-06\n",
      "LOSS train 1.8042565276598634e-06 valid 1.8270878854309558e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7451772293952671e-09\n",
      "  batch 101 loss: 1.1701225131055537e-07\n",
      "LOSS train 7.893608026330289e-08 valid 1.0667744732018036e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0024826616048812867\n",
      "  batch 101 loss: 0.009366609567914566\n",
      "LOSS train 0.006178939668986371 valid 0.00021825214207638055\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.835734090302139e-07\n",
      "  batch 101 loss: 2.6023654050675305e-05\n",
      "LOSS train 1.951801728109543e-05 valid 6.315550763247302e-06\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0008900261670351028\n",
      "  batch 101 loss: 0.0027663059251876467\n",
      "LOSS train 0.0018560573199334672 valid 2.6968770328039682e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.460723511736432e-11\n",
      "  batch 101 loss: 2.6027376225368216e-08\n",
      "LOSS train 2.3997117779640604e-08 valid 5.453294349422322e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002399335205554962\n",
      "  batch 101 loss: 0.010466455205582523\n",
      "LOSS train 0.006531032534079337 valid 2.822980604832992e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.828231479185434e-11\n",
      "  batch 101 loss: 2.694789179580326e-08\n",
      "LOSS train 2.098465466255511e-08 valid 2.734391912895262e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 6.484566458188068e-05\n",
      "LOSS train 3.2936733340913424e-05 valid 1.6291983229166362e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5391950114462816e-09\n",
      "  batch 101 loss: 8.724547989391596e-08\n",
      "LOSS train 6.037761759368691e-08 valid 1.6327339835697785e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004285329207777977\n",
      "  batch 101 loss: 0.434152137192068\n",
      "LOSS train 0.22130091203814167 valid 0.0010701254941523075\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5101504977792502e-05\n",
      "  batch 101 loss: 0.0007360575179518492\n",
      "LOSS train 0.00046553506497379397 valid 3.293675399618223e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0015680232644081115\n",
      "  batch 101 loss: 0.008604975749384352\n",
      "LOSS train 0.005164070293588869 valid 4.429264066629912e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.9673930040653432e-10\n",
      "  batch 101 loss: 2.2397975518728686e-08\n",
      "LOSS train 2.141891954492842e-08 valid 2.7010310432729057e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0008552813529968262\n",
      "  batch 101 loss: 0.0072317075247239695\n",
      "LOSS train 0.0041051325842176 valid 3.309306251253474e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0140118078538763e-10\n",
      "  batch 101 loss: 2.0582996329565616e-08\n",
      "LOSS train 2.0608902176463616e-08 valid 2.6984308121313916e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.0010746598370171335\n",
      "LOSS train 0.0005455558131975714 valid 1.551010342382142e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.457648011182755e-09\n",
      "  batch 101 loss: 7.921439272973174e-08\n",
      "LOSS train 5.5102622062501104e-08 valid 1.740781669923308e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0008376844227313996\n",
      "  batch 101 loss: 0.0026295517708604167\n",
      "LOSS train 0.0018395860573863164 valid 0.0006090929382480681\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.7147925468161703e-06\n",
      "  batch 101 loss: 0.00013248163364551146\n",
      "LOSS train 0.00010385814433219577 valid 0.00011843562970170751\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0093381570186466e-06\n",
      "  batch 101 loss: 8.957010594485837e-05\n",
      "LOSS train 6.534836623400848e-05 valid 4.224687290843576e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.899659375543706e-07\n",
      "  batch 101 loss: 2.152225877125602e-05\n",
      "LOSS train 1.96801456753248e-05 valid 2.365126601944212e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 4.600555170327425e-05\n",
      "  batch 101 loss: 0.0003188955787368286\n",
      "LOSS train 0.00021145111457275016 valid 3.1058898457558826e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.547021232312545e-07\n",
      "  batch 101 loss: 5.1463309264363486e-05\n",
      "LOSS train 3.7791208910098684e-05 valid 2.7814798158942722e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.625132275279612e-07\n",
      "  batch 101 loss: 2.3659054669451507e-05\n",
      "LOSS train 1.8338363121111443e-05 valid 7.928639206511434e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.7978138203034178e-07\n",
      "  batch 101 loss: 1.0895604021925465e-05\n",
      "LOSS train 8.518649951954015e-06 valid 4.125242412555963e-06\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00039792872965335843\n",
      "  batch 101 loss: 0.0017259487374576565\n",
      "LOSS train 0.0010816693189631247 valid 1.0638335879775696e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.595935363089666e-08\n",
      "  batch 101 loss: 8.549637618386896e-06\n",
      "LOSS train 6.199113680634301e-06 valid 7.146087227738462e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.391151610936504e-08\n",
      "  batch 101 loss: 4.8299997743583845e-06\n",
      "LOSS train 3.5695241201011786e-06 valid 4.199062459520064e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.029252189255203e-08\n",
      "  batch 101 loss: 2.7200934637505725e-06\n",
      "LOSS train 2.1281740369340466e-06 valid 1.711214054012089e-06\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 3.5267525583027036e-06\n",
      "LOSS train 1.8042565276598634e-06 valid 1.8270878854309558e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7451772293952671e-09\n",
      "  batch 101 loss: 1.1701225131055537e-07\n",
      "LOSS train 7.893608026330289e-08 valid 1.0667744732018036e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.498042174982402e-10\n",
      "  batch 101 loss: 9.928076639464934e-08\n",
      "LOSS train 6.448141113781532e-08 valid 4.4329382831165276e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7932307133937684e-10\n",
      "  batch 101 loss: 1.5157064648718687e-07\n",
      "LOSS train 1.2346435350678596e-07 valid 5.392543656057569e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.008410117030143738\n",
      "  batch 101 loss: 0.02328006440504396\n",
      "LOSS train 0.016351592814760178 valid 0.0006211733561940491\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.461337907239795e-06\n",
      "  batch 101 loss: 0.0003665568345059\n",
      "LOSS train 0.00036304666289578363 valid 0.000337980774929747\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.7440948178991677e-06\n",
      "  batch 101 loss: 0.0001972909665323641\n",
      "LOSS train 0.00016719996374215347 valid 0.00014034654304850847\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.3687666796613484e-06\n",
      "  batch 101 loss: 0.0003665110607016686\n",
      "LOSS train 0.0007079685144629084 valid 0.001279143849387765\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00023939665406942369\n",
      "  batch 101 loss: 0.0024926420884102463\n",
      "LOSS train 0.001386860661064746 valid 2.9665976342130307e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.53472306769254e-11\n",
      "  batch 101 loss: 2.4487351921731458e-08\n",
      "LOSS train 2.2291568645117458e-08 valid 4.7197005415000604e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.2247229836125371e-10\n",
      "  batch 101 loss: 2.4382940795319996e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 2.1518383832834224e-08 valid 4.3677069072600716e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.913203462322599e-10\n",
      "  batch 101 loss: 2.287202172990277e-08\n",
      "LOSS train 2.1557330750718996e-08 valid 3.2497965207767265e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0021690888702869415\n",
      "  batch 101 loss: 0.009300816231215023\n",
      "LOSS train 0.005822435271660535 valid 2.7139861913383356e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.038093663425116e-11\n",
      "  batch 101 loss: 2.7964256030799108e-08\n",
      "LOSS train 2.1826460987146117e-08 valid 2.7273092229052054e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.307580318638429e-11\n",
      "  batch 101 loss: 2.8143514234635347e-08\n",
      "LOSS train 2.4432591773786832e-08 valid 2.8482871172741397e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.758150039587463e-11\n",
      "  batch 101 loss: 2.573443075526427e-08\n",
      "LOSS train 2.4069375255447074e-08 valid 4.666154396204547e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 6.484566458188068e-05\n",
      "LOSS train 3.2936733340913424e-05 valid 1.6291983229166362e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5391950114462816e-09\n",
      "  batch 101 loss: 8.724547989391596e-08\n",
      "LOSS train 6.037761759368691e-08 valid 1.6327339835697785e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.542880454508122e-09\n",
      "  batch 101 loss: 9.643946196291608e-08\n",
      "LOSS train 6.188615216562161e-08 valid 4.311463186468245e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.657102271541589e-10\n",
      "  batch 101 loss: 1.542105683949302e-07\n",
      "LOSS train 1.2409942332979863e-07 valid 5.547791204207897e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0022042536735534667\n",
      "  batch 101 loss: 0.35597744769373096\n",
      "LOSS train 0.18198588491494438 valid 0.0006844876916147768\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.686966029927135e-06\n",
      "  batch 101 loss: 0.00031438840092050667\n",
      "LOSS train 0.00020410491444017496 valid 0.0001450029667466879\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.338756818673574e-07\n",
      "  batch 101 loss: 8.714634642046803e-05\n",
      "LOSS train 0.0001555989228083564 valid 0.00020871733431704342\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0744987957878038e-06\n",
      "  batch 101 loss: 0.0005581836938472407\n",
      "LOSS train 0.0007483553663359306 valid 0.0007410373073071241\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.70636198669672e-05\n",
      "  batch 101 loss: 0.00044370220676787753\n",
      "LOSS train 0.00026944441771569 valid 1.6504934308159136e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5613878190379182e-09\n",
      "  batch 101 loss: 9.344933804111833e-08\n",
      "LOSS train 6.259366556261654e-08 valid 3.2354083856489524e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.4134787562625206e-10\n",
      "  batch 101 loss: 8.078088657548222e-08\n",
      "LOSS train 7.015998902773476e-08 valid 2.9053065730977323e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 9.968120551206993e-11\n",
      "  batch 101 loss: 1.0704268000427674e-07\n",
      "LOSS train 1.0972825587012979e-07 valid 6.179671885320204e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001735646650195122\n",
      "  batch 101 loss: 0.04714788375378671\n",
      "LOSS train 0.024021459390879372 valid 3.536642623203079e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1995783921747715e-10\n",
      "  batch 101 loss: 2.384473922756314e-08\n",
      "LOSS train 2.1392097444831897e-08 valid 4.397497832542285e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.9394121864024782e-10\n",
      "  batch 101 loss: 2.3946963674248424e-08\n",
      "LOSS train 2.161900122993304e-08 valid 3.994137998120095e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.5875908587759113e-10\n",
      "  batch 101 loss: 2.185370240104234e-08\n",
      "LOSS train 2.1122559591993764e-08 valid 3.1142487699753474e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.0010746598370171335\n",
      "LOSS train 0.0005455558131975714 valid 1.551010342382142e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.457648011182755e-09\n",
      "  batch 101 loss: 7.921439272973174e-08\n",
      "LOSS train 5.5102622062501104e-08 valid 1.740781669923308e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.655409107570449e-09\n",
      "  batch 101 loss: 9.318774020883324e-08\n",
      "LOSS train 6.000231088637311e-08 valid 4.004807152568901e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.310797064808412e-10\n",
      "  batch 101 loss: 1.584751797389927e-07\n",
      "LOSS train 1.3084249057593987e-07 valid 5.937912561648773e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 6.299169617705047e-06\n",
      "  batch 101 loss: 0.002461259701212839\n",
      "LOSS train 0.0014650118015280323 valid 0.0002885175927076489\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0504194069653749e-05\n",
      "  batch 101 loss: 0.00036417280575733456\n",
      "LOSS train 0.00025630951933386213 valid 6.711381138302386e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.8355737847741693e-06\n",
      "  batch 101 loss: 0.00010222081163419717\n",
      "LOSS train 6.885756910341471e-05 valid 1.5500829249504022e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.6879372196854092e-07\n",
      "  batch 101 loss: 7.538014533565729e-06\n",
      "LOSS train 9.677540080487196e-06 valid 1.1595744581427425e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.507539875296061e-08\n",
      "  batch 101 loss: 5.031839626383316e-05\n",
      "LOSS train 0.000209223140063668 valid 0.001642158953472972\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.395825861953199e-06\n",
      "  batch 101 loss: 0.0014586678452542402\n",
      "LOSS train 0.0007676687318916482 valid 1.4334864317788742e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.411774585198145e-07\n",
      "  batch 101 loss: 9.596105012974476e-06\n",
      "LOSS train 8.400206773764837e-06 valid 4.519095455179922e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.549186547184945e-08\n",
      "  batch 101 loss: 6.767810697070331e-06\n",
      "LOSS train 2.0908664145015036e-05 valid 7.585510320495814e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002077178508043289\n",
      "  batch 101 loss: 0.006014156189428377\n",
      "LOSS train 0.004118460920965055 valid 3.7179081118665636e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.247248140629381e-07\n",
      "  batch 101 loss: 2.4996433970727595e-05\n",
      "LOSS train 1.6833016982022186e-05 valid 1.097827589546796e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.3344526198343374e-07\n",
      "  batch 101 loss: 7.327078593561964e-06\n",
      "LOSS train 5.13877207399004e-06 valid 2.420203372821561e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.383886789582903e-08\n",
      "  batch 101 loss: 2.2606323699392304e-06\n",
      "LOSS train 1.6537737219770057e-06 valid 9.942108363247826e-07\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.2827686077798717e-08\n",
      "  batch 101 loss: 1.0881212934066299e-06\n",
      "LOSS train 7.955892809076233e-07 valid 1.069123300112551e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.804341647788533e-09\n",
      "  batch 101 loss: 5.415907556738375e-07\n",
      "LOSS train 4.1594320102808176e-07 valid 4.052692474942887e-07\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.4820400906501164e-09\n",
      "  batch 101 loss: 2.781617732594555e-07\n",
      "LOSS train 2.2568622668833608e-07 valid 1.5597204594541836e-07\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.2222877760214033e-09\n",
      "  batch 101 loss: 1.485110506393994e-07\n",
      "LOSS train 1.210788381637125e-07 valid 7.782699640301871e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0038952356576919555\n",
      "  batch 101 loss: 0.02240313642150795\n",
      "LOSS train 0.013355779387978972 valid 1.1372061635483988e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.0906576537527143e-07\n",
      "  batch 101 loss: 9.46780004142056e-06\n",
      "LOSS train 8.327351605459376e-06 valid 1.2340576176939066e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.75406964647118e-07\n",
      "  batch 101 loss: 8.298923931988611e-06\n",
      "LOSS train 7.33338712216394e-06 valid 1.0042873327620327e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2685384465148673e-07\n",
      "  batch 101 loss: 6.990679476075457e-06\n",
      "LOSS train 6.256928311001138e-06 valid 8.903643902158365e-06\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.061133818642702e-07\n",
      "  batch 101 loss: 6.020051074528965e-06\n",
      "LOSS train 5.246978435717593e-06 valid 1.1267313311691396e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.4559798475820574e-07\n",
      "  batch 101 loss: 5.241629321517394e-06\n",
      "LOSS train 4.575234880705586e-06 valid 1.061441253114026e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2104726667748765e-07\n",
      "  batch 101 loss: 4.0684802286250484e-06\n",
      "LOSS train 3.646306717094602e-06 valid 1.3391416359809227e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.4635428417241203e-07\n",
      "  batch 101 loss: 3.3873720866495207e-06\n",
      "LOSS train 2.9548877372872545e-06 valid 1.0428086170577444e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 3.5267525583027036e-06\n",
      "LOSS train 1.8042565276598634e-06 valid 1.8270878854309558e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7451772293952671e-09\n",
      "  batch 101 loss: 1.1701225131055537e-07\n",
      "LOSS train 7.893608026330289e-08 valid 1.0667744732018036e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.498042174982402e-10\n",
      "  batch 101 loss: 9.928076639464934e-08\n",
      "LOSS train 6.448141113781532e-08 valid 4.4329382831165276e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7932307133937684e-10\n",
      "  batch 101 loss: 1.5157064648718687e-07\n",
      "LOSS train 1.2346435350678596e-07 valid 5.392543656057569e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.854053431950888e-10\n",
      "  batch 101 loss: 7.029986571982505e-08\n",
      "LOSS train 5.995282369256046e-08 valid 2.733861670378701e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.428877957238455e-11\n",
      "  batch 101 loss: 7.518641056192265e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 9.876818862380644e-08 valid 2.9021140335316886e-07\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.857171921277768e-09\n",
      "  batch 101 loss: 1.3596063913601242e-07\n",
      "LOSS train 1.4475787920074338e-07 valid 1.757144474368033e-07\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.6724354168218268e-09\n",
      "  batch 101 loss: 2.5508923273687145e-07\n",
      "LOSS train 2.0523051534132445e-07 valid 1.2188803566459683e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0035133010149002077\n",
      "  batch 101 loss: 0.013958273811804247\n",
      "LOSS train 0.009219342395946913 valid 0.0005746094393543899\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.502162242308259e-06\n",
      "  batch 101 loss: 0.0006112236017725082\n",
      "LOSS train 0.00034349810660729084 valid 4.918243575957604e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.892001015832647e-07\n",
      "  batch 101 loss: 0.0001185668522089145\n",
      "LOSS train 0.00018630168258403564 valid 0.00015554382116533816\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.088390851393342e-06\n",
      "  batch 101 loss: 0.0005243405637520482\n",
      "LOSS train 0.0007706215257371119 valid 0.0008931743213906884\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.60397298168391e-05\n",
      "  batch 101 loss: 0.0016866444577317452\n",
      "LOSS train 0.002629093281265235 valid 0.0029488203581422567\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.257059819996357e-05\n",
      "  batch 101 loss: 0.009368034522049129\n",
      "LOSS train 0.05114072725486464 valid 0.018389206379652023\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 5.759842693805694e-05\n",
      "  batch 101 loss: 0.014658884251839482\n",
      "LOSS train 0.0088166416364014 valid 0.0009289196459576488\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.251684596762061e-05\n",
      "  batch 101 loss: 0.0016040922460524598\n",
      "LOSS train 0.0010560024777336607 valid 0.00019048598187509924\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001370270736515522\n",
      "  batch 101 loss: 0.010774979868339765\n",
      "LOSS train 0.005539196194396397 valid 2.7364812638097646e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.475963847980438e-11\n",
      "  batch 101 loss: 2.8130518103797185e-08\n",
      "LOSS train 2.2094373898404197e-08 valid 2.7212703201939803e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.190167128356962e-11\n",
      "  batch 101 loss: 2.7954690036136042e-08\n",
      "LOSS train 2.453760197623806e-08 valid 2.955229128076553e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.45558015324832e-11\n",
      "  batch 101 loss: 2.542723007703529e-08\n",
      "LOSS train 2.3926771520647822e-08 valid 4.945496456798537e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.42639739411743e-10\n",
      "  batch 101 loss: 2.4776464038200883e-08\n",
      "LOSS train 2.274213023991926e-08 valid 4.987314383697594e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.4638806550569824e-10\n",
      "  batch 101 loss: 2.493967682370979e-08\n",
      "LOSS train 2.1661237785979237e-08 valid 4.397729114202775e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.9396150463535377e-10\n",
      "  batch 101 loss: 2.3990309314658732e-08\n",
      "LOSS train 2.1329367556993858e-08 valid 4.4567336487943976e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.991618070462664e-10\n",
      "  batch 101 loss: 2.3377877835795146e-08\n",
      "LOSS train 2.167787141390041e-08 valid 3.8003896918326063e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.005747371912002563\n",
      "  batch 101 loss: 0.00554128729475309\n",
      "LOSS train 0.005730368585689733 valid 2.7138113978253386e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.034286930718281e-11\n",
      "  batch 101 loss: 2.8016928375063088e-08\n",
      "LOSS train 2.1917517543688002e-08 valid 2.725299275141424e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.269205681836865e-11\n",
      "  batch 101 loss: 2.807689925443668e-08\n",
      "LOSS train 2.4475084905191847e-08 valid 2.8827459530589294e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.971595745142167e-11\n",
      "  batch 101 loss: 2.5620312928964496e-08\n",
      "LOSS train 2.4019560388043976e-08 valid 4.773494310938986e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.2726531767602864e-10\n",
      "  batch 101 loss: 2.470283410715979e-08\n",
      "LOSS train 2.2859077298837103e-08 valid 5.0873907753157255e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.5537486791904483e-10\n",
      "  batch 101 loss: 2.5036856328153334e-08\n",
      "LOSS train 2.1775321085470076e-08 valid 4.4155260781053585e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.955288375654618e-10\n",
      "  batch 101 loss: 2.4059811831733667e-08\n",
      "LOSS train 2.1292350097394276e-08 valid 4.465552194687916e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.999401710861548e-10\n",
      "  batch 101 loss: 2.3520494216722197e-08\n",
      "LOSS train 2.1691936587630007e-08 valid 3.927364033984304e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 6.484566458188068e-05\n",
      "LOSS train 3.2936733340913424e-05 valid 1.6291983229166362e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5391950114462816e-09\n",
      "  batch 101 loss: 8.724547989391596e-08\n",
      "LOSS train 6.037761759368691e-08 valid 1.6327339835697785e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.542880454508122e-09\n",
      "  batch 101 loss: 9.643946196291608e-08\n",
      "LOSS train 6.188615216562161e-08 valid 4.311463186468245e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.657102271541589e-10\n",
      "  batch 101 loss: 1.542105683949302e-07\n",
      "LOSS train 1.2409942332979863e-07 valid 5.547791204207897e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.0238514742441114e-10\n",
      "  batch 101 loss: 7.136823384268353e-08\n",
      "LOSS train 7.308476398013823e-08 valid 3.924785474396231e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2196834592591587e-10\n",
      "  batch 101 loss: 1.1251271335499524e-07\n",
      "LOSS train 1.202225075047636e-07 valid 4.7909562539416584e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.288227740621096e-10\n",
      "  batch 101 loss: 2.0354335726890939e-07\n",
      "LOSS train 1.4458170020492937e-07 valid 5.576764294801251e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9960183667299133e-10\n",
      "  batch 101 loss: 1.3484949927144995e-07\n",
      "LOSS train 1.497750838321911e-07 valid 1.3454170755267114e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.003935991823673248\n",
      "  batch 101 loss: 0.3133948616452835\n",
      "LOSS train 0.16114576627671506 valid 0.00023742289340589195\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.795366988517344e-06\n",
      "  batch 101 loss: 0.00010063827056455921\n",
      "LOSS train 6.0539018812646486e-05 valid 2.1903302695136517e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.0072347979294135e-07\n",
      "  batch 101 loss: 1.3908171311811657e-05\n",
      "LOSS train 1.4316381393764967e-05 valid 8.782009899732657e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.9055878510698677e-07\n",
      "  batch 101 loss: 2.3394285387894343e-05\n",
      "LOSS train 3.4068196054466694e-05 valid 3.242298771510832e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1759219341911376e-06\n",
      "  batch 101 loss: 7.659459066871932e-05\n",
      "LOSS train 8.448867778460582e-05 valid 8.455165516352281e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.1521892884047702e-06\n",
      "  batch 101 loss: 0.0010681067278073897\n",
      "LOSS train 0.0018372626393683817 valid 0.0007213409407995641\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 4.525045806076378e-06\n",
      "  batch 101 loss: 0.010397566829924472\n",
      "LOSS train 0.008060391536955326 valid 0.002091072266921401\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.1255129724740984e-05\n",
      "  batch 101 loss: 0.019868785093713086\n",
      "LOSS train 0.02467396060960086 valid 0.016009481623768806\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0013833679258823395\n",
      "  batch 101 loss: 0.2261943026931027\n",
      "LOSS train 0.11552243272059745 valid 3.1119149923597433e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2620131606411177e-10\n",
      "  batch 101 loss: 2.5804060377332406e-08\n",
      "LOSS train 2.0197148149086484e-08 valid 2.735446180679446e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.457455097892307e-11\n",
      "  batch 101 loss: 2.869341584110785e-08\n",
      "LOSS train 2.3739888642608798e-08 valid 2.698577894477694e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.346962955205981e-11\n",
      "  batch 101 loss: 2.6994094421972647e-08\n",
      "LOSS train 2.4436200391985224e-08 valid 3.452257857361474e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.1299459146130176e-10\n",
      "  batch 101 loss: 2.486605651608187e-08\n",
      "LOSS train 2.3625908471565645e-08 valid 5.220561405394619e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.673656673835012e-10\n",
      "  batch 101 loss: 2.4984947992745532e-08\n",
      "LOSS train 2.2647340176923577e-08 valid 4.86888289685794e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.3578266450385853e-10\n",
      "  batch 101 loss: 2.4838882053401788e-08\n",
      "LOSS train 2.1585459674613033e-08 valid 4.397075770157244e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.9390405725516757e-10\n",
      "  batch 101 loss: 2.398244398627014e-08\n",
      "LOSS train 2.1320764067950277e-08 valid 4.463351999106635e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.006079567670822144\n",
      "  batch 101 loss: 0.01247263900674419\n",
      "LOSS train 0.00941760019016621 valid 5.246555900839667e-08\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.697104584115095e-10\n",
      "  batch 101 loss: 2.472908371187188e-08\n",
      "LOSS train 2.2237846526429293e-08 valid 3.365289202861277e-08\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0590728294346263e-10\n",
      "  batch 101 loss: 2.0667659719197928e-08\n",
      "LOSS train 2.0419609741381133e-08 valid 2.701263746018867e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.295809207301772e-11\n",
      "  batch 101 loss: 1.973644699226895e-08\n",
      "LOSS train 2.101090111032267e-08 valid 2.697227508008382e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.520670225995673e-11\n",
      "  batch 101 loss: 2.2794609437770673e-08\n",
      "LOSS train 2.770735265249919e-08 valid 2.8320298994799487e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.957830033295977e-11\n",
      "  batch 101 loss: 2.8571389347753496e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 3.1348523867012226e-08 valid 8.671942453020165e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.383974320873676e-10\n",
      "  batch 101 loss: 4.78532554271105e-08\n",
      "LOSS train 3.547618783060008e-08 valid 1.2761873335875862e-07\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.1701207114356294e-09\n",
      "  batch 101 loss: 5.679290262294856e-08\n",
      "LOSS train 3.829038648925322e-08 valid 1.7089178072637878e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.0010746598370171335\n",
      "LOSS train 0.0005455558131975714 valid 1.551010342382142e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.457648011182755e-09\n",
      "  batch 101 loss: 7.921439272973174e-08\n",
      "LOSS train 5.5102622062501104e-08 valid 1.740781669923308e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.655409107570449e-09\n",
      "  batch 101 loss: 9.318774020883324e-08\n",
      "LOSS train 6.000231088637311e-08 valid 4.004807152568901e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.310797064808412e-10\n",
      "  batch 101 loss: 1.584751797389927e-07\n",
      "LOSS train 1.3084249057593987e-07 valid 5.937912561648773e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.448884283192456e-10\n",
      "  batch 101 loss: 6.988984737610338e-08\n",
      "LOSS train 6.89106050620198e-08 valid 3.153174077397125e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.3130286191653795e-10\n",
      "  batch 101 loss: 1.0337004188176735e-07\n",
      "LOSS train 1.1752829395199965e-07 valid 3.8930412671334125e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.5006285991603364e-10\n",
      "  batch 101 loss: 2.1310668842033918e-07\n",
      "LOSS train 1.4935812216789778e-07 valid 6.094759896768664e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.468338860557196e-10\n",
      "  batch 101 loss: 1.3568153296228402e-07\n",
      "LOSS train 1.4850488999713797e-07 valid 2.429392793601437e-07\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(3)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [1,2,4,8]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,1,2,3]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            m = create_model(nb_hidden,input_size=d_ft_in['train'].shape[1])\n",
    "            m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "            \n",
    "            saved_models = dict()\n",
    "            \n",
    "            for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                path = f\"trained_models/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "                \n",
    "                model = m\n",
    "                m.load_state_dict(torch.load(path))\n",
    "                m.eval()\n",
    "\n",
    "                test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "                \n",
    "                train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "                \n",
    "                validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                validation_loss = loss_fn(validation_prediction,d_ft_out[\"val\"])\n",
    "\n",
    "                if mt == \"min_val\": \n",
    "                    min_val = True\n",
    "                else: \n",
    "                    min_val = False\n",
    "\n",
    "                r = pd.DataFrame({\"Model_type\": nb_hidden,\"Min_val\":min_val,\"Epochs\": nb_e,\"Lr\":lr, \"Tr_l\":train_loss.item(),\"Te_l\":test_loss.item(),\"V_l\": validation_loss.item()},index = [i]\n",
    "                )\n",
    "                i+=1\n",
    "                results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21e22ff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m f \u001b[38;5;241m=\u001b[39m (results\u001b[38;5;241m.\u001b[39mModel_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m  (results\u001b[38;5;241m.\u001b[39mMin_val \u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m&\u001b[39m (results\u001b[38;5;241m.\u001b[39mEpochs \u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboxplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTe_l\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTr_l\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV_l\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mby\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlayout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msharey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\plotting\\_core.py:507\u001b[0m, in \u001b[0;36mboxplot_frame\u001b[1;34m(self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, backend, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, backend\u001b[38;5;241m=\u001b[39m_backend_doc)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_boxplot_doc)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxplot_frame\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    505\u001b[0m ):\n\u001b[0;32m    506\u001b[0m     plot_backend \u001b[38;5;241m=\u001b[39m _get_plot_backend(backend)\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m plot_backend\u001b[38;5;241m.\u001b[39mboxplot_frame(\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    509\u001b[0m         column\u001b[38;5;241m=\u001b[39mcolumn,\n\u001b[0;32m    510\u001b[0m         by\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m    511\u001b[0m         ax\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m    512\u001b[0m         fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[0;32m    513\u001b[0m         rot\u001b[38;5;241m=\u001b[39mrot,\n\u001b[0;32m    514\u001b[0m         grid\u001b[38;5;241m=\u001b[39mgrid,\n\u001b[0;32m    515\u001b[0m         figsize\u001b[38;5;241m=\u001b[39mfigsize,\n\u001b[0;32m    516\u001b[0m         layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[0;32m    517\u001b[0m         return_type\u001b[38;5;241m=\u001b[39mreturn_type,\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    519\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\boxplot.py:469\u001b[0m, in \u001b[0;36mboxplot_frame\u001b[1;34m(self, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mboxplot_frame\u001b[39m(\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    456\u001b[0m     column\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m    466\u001b[0m ):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     ax \u001b[38;5;241m=\u001b[39m boxplot(\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    471\u001b[0m         column\u001b[38;5;241m=\u001b[39mcolumn,\n\u001b[0;32m    472\u001b[0m         by\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m    473\u001b[0m         ax\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m    474\u001b[0m         fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[0;32m    475\u001b[0m         grid\u001b[38;5;241m=\u001b[39mgrid,\n\u001b[0;32m    476\u001b[0m         rot\u001b[38;5;241m=\u001b[39mrot,\n\u001b[0;32m    477\u001b[0m         figsize\u001b[38;5;241m=\u001b[39mfigsize,\n\u001b[0;32m    478\u001b[0m         layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[0;32m    479\u001b[0m         return_type\u001b[38;5;241m=\u001b[39mreturn_type,\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m    481\u001b[0m     )\n\u001b[0;32m    482\u001b[0m     plt\u001b[38;5;241m.\u001b[39mdraw_if_interactive()\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ax\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\boxplot.py:415\u001b[0m, in \u001b[0;36mboxplot\u001b[1;34m(data, column, by, ax, fontsize, rot, grid, figsize, layout, return_type, **kwds)\u001b[0m\n\u001b[0;32m    410\u001b[0m         columns \u001b[38;5;241m=\u001b[39m [column]\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;66;03m# Prefer array return type for 2-D plots to match the subplot layout\u001b[39;00m\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;66;03m# https://github.com/pandas-dev/pandas/pull/12216#issuecomment-241175580\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m     result \u001b[38;5;241m=\u001b[39m _grouped_plot_by_column(\n\u001b[0;32m    416\u001b[0m         plot_group,\n\u001b[0;32m    417\u001b[0m         data,\n\u001b[0;32m    418\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    419\u001b[0m         by\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m    420\u001b[0m         grid\u001b[38;5;241m=\u001b[39mgrid,\n\u001b[0;32m    421\u001b[0m         figsize\u001b[38;5;241m=\u001b[39mfigsize,\n\u001b[0;32m    422\u001b[0m         ax\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m    423\u001b[0m         layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[0;32m    424\u001b[0m         return_type\u001b[38;5;241m=\u001b[39mreturn_type,\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[0;32m    426\u001b[0m     )\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\boxplot.py:256\u001b[0m, in \u001b[0;36m_grouped_plot_by_column\u001b[1;34m(plotf, data, columns, by, numeric_only, grid, figsize, ax, layout, return_type, **kwargs)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_grouped_plot_by_column\u001b[39m(\n\u001b[0;32m    244\u001b[0m     plotf,\n\u001b[0;32m    245\u001b[0m     data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    255\u001b[0m ):\n\u001b[1;32m--> 256\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(by, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\core\\frame.py:8262\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8260\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m-> 8262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   8263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8265\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8268\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   8272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32mC:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dor'"
     ]
    }
   ],
   "source": [
    "f = (results.Model_type == 0) &  (results.Min_val ==False) & (results.Epochs !=8)\n",
    "results[f].boxplot(column = [\"Te_l\", \"Tr_l\",\"V_l\"],by = [\"dor\"],layout = (3,1),sharey = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c9493ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0038035571575164794\n",
      "  batch 101 loss: 0.013667512624961091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.009400308943126967 valid 0.00186417275108397\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.1709101535379885e-05\n",
      "  batch 101 loss: 0.0010250554697267943\n",
      "LOSS train 0.0007213617721570671 valid 0.0003256538475397974\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0036404710263014e-05\n",
      "  batch 101 loss: 0.00034494912834816206\n",
      "LOSS train 0.0002561983209982961 valid 0.0001011294370982796\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.001771063078195e-06\n",
      "  batch 101 loss: 0.00013385735032898083\n",
      "LOSS train 0.00010164761680511597 valid 4.466002792469226e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.6963283491786567e-07\n",
      "  batch 101 loss: 4.432829742654576e-05\n",
      "LOSS train 3.3683990387154214e-05 valid 1.3112870874465443e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.586595110187773e-07\n",
      "  batch 101 loss: 1.183692930851521e-05\n",
      "LOSS train 8.55809655774631e-06 valid 2.6632019398675766e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([874])) that is different to the input size (torch.Size([874, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([6289])) that is different to the input size (torch.Size([6289, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1573])) that is different to the input size (torch.Size([1573, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0025280815362930297\n",
      "  batch 101 loss: 0.006121075835480951\n",
      "LOSS train 0.004567627332375574 valid 0.0005244357162155211\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.57816999591887e-06\n",
      "  batch 101 loss: 0.0001134927920793416\n",
      "LOSS train 0.00010672157611968268 valid 0.00029401297797448933\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.487695892341435e-06\n",
      "  batch 101 loss: 6.968760035306332e-05\n",
      "LOSS train 6.956902177592794e-05 valid 0.0001237175747519359\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 9.162959759123623e-07\n",
      "  batch 101 loss: 3.7333458942612195e-05\n",
      "LOSS train 3.902155037907007e-05 valid 7.477763574570417e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.546194294467568e-07\n",
      "  batch 101 loss: 3.699844604625469e-05\n",
      "LOSS train 3.394841876700776e-05 valid 2.667807348188944e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.650074202392716e-07\n",
      "  batch 101 loss: 1.4129496828445553e-05\n",
      "LOSS train 1.2387289968713505e-05 valid 1.0868719982681796e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.651137351989747e-05\n",
      "  batch 101 loss: 0.0015278573525574757\n",
      "LOSS train 0.0008623920313748109 valid 6.853169907117262e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.8653855881420897e-07\n",
      "  batch 101 loss: 6.042454956514121e-05\n",
      "LOSS train 4.826988220205573e-05 valid 2.3474191038985737e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.7223857867065818e-07\n",
      "  batch 101 loss: 0.0001191289356438574\n",
      "LOSS train 0.0007541053862207938 valid 0.004587485920637846\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.335911151021719e-05\n",
      "  batch 101 loss: 0.001685822834933788\n",
      "LOSS train 0.0009412780637244695 valid 3.150786869809963e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.992073081666604e-07\n",
      "  batch 101 loss: 4.470006096539692e-05\n",
      "LOSS train 3.11922525307281e-05 valid 1.2282983334443998e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.277055616810685e-07\n",
      "  batch 101 loss: 4.598169630867233e-06\n",
      "LOSS train 7.063270977700279e-06 valid 1.303957287746016e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.002898261547088623\n",
      "  batch 101 loss: 0.010560910447529749\n",
      "LOSS train 0.007092827188621806 valid 0.0007006147061474621\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.10375902801752e-06\n",
      "  batch 101 loss: 0.000490916969756654\n",
      "LOSS train 0.0003328130267544822 valid 0.00048803837853483856\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.2535830319393427e-06\n",
      "  batch 101 loss: 0.0002050043623239617\n",
      "LOSS train 0.00015146375501664004 valid 0.00026585551677271724\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0228015162283554e-06\n",
      "  batch 101 loss: 8.287282293167664e-05\n",
      "LOSS train 7.044624251112661e-05 valid 6.804627628298476e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.489722101017833e-07\n",
      "  batch 101 loss: 2.9272305923768726e-05\n",
      "LOSS train 2.7707261379795186e-05 valid 4.398666351335123e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.6418718082131816e-07\n",
      "  batch 101 loss: 1.4893751703084491e-05\n",
      "LOSS train 1.4793743707395415e-05 valid 1.889228769869078e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.006990601420402527\n",
      "  batch 101 loss: 0.023349554411834107\n",
      "LOSS train 0.01591339762645997 valid 0.002362219849601388\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.079126425087452e-05\n",
      "  batch 101 loss: 0.0010586749510548543\n",
      "LOSS train 0.0006917464008696937 valid 0.000353143346728757\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.594014756847173e-06\n",
      "  batch 101 loss: 0.0002136059403346735\n",
      "LOSS train 0.0001538191475627145 valid 8.135878306347877e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.351127715082839e-07\n",
      "  batch 101 loss: 5.8252364924555876e-05\n",
      "LOSS train 4.615641789088176e-05 valid 2.7219695766689256e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.5831799323204904e-07\n",
      "  batch 101 loss: 2.1781713362543085e-05\n",
      "LOSS train 1.7861130867098893e-05 valid 1.0916556675510947e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.819808838074096e-08\n",
      "  batch 101 loss: 7.416356919520695e-06\n",
      "LOSS train 5.849818611240231e-06 valid 4.497991540119983e-06\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.251823596656323e-05\n",
      "  batch 101 loss: 0.029580837434477873\n",
      "LOSS train 0.015342565937206174 valid 0.002194831846281886\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.3139406219124794e-05\n",
      "  batch 101 loss: 0.0005829106677720119\n",
      "LOSS train 0.0004237640006173707 valid 0.00016194618365261704\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0198226664215325e-06\n",
      "  batch 101 loss: 0.00011396926202905888\n",
      "LOSS train 0.00012925162563480285 valid 9.949470404535532e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.5677112464327365e-07\n",
      "  batch 101 loss: 0.00042610704638718744\n",
      "LOSS train 0.0008237026089551678 valid 0.0012960920576006174\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.73535799048841e-06\n",
      "  batch 101 loss: 0.006260913933729171\n",
      "LOSS train 0.03957960884266877 valid 0.0627823993563652\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0001671280711889267\n",
      "  batch 101 loss: 0.013155041770078242\n",
      "LOSS train 0.007677242356922175 valid 0.0005063836579211056\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0023416638374328612\n",
      "  batch 101 loss: 0.010694233114409145\n",
      "LOSS train 0.007050593969606457 valid 0.0004702787264250219\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.2103460903745145e-06\n",
      "  batch 101 loss: 0.000397070401199926\n",
      "LOSS train 0.0004561813332883258 valid 0.0005170569638721645\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.3902319187764078e-06\n",
      "  batch 101 loss: 0.0015107974499551347\n",
      "LOSS train 0.003331647861902874 valid 0.00873512402176857\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2580623626708983e-05\n",
      "  batch 101 loss: 0.04423126669760677\n",
      "LOSS train 0.02450616971490199 valid 0.0005413896287791431\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3009958202019334e-05\n",
      "  batch 101 loss: 0.0010307632763579021\n",
      "LOSS train 0.0006403937666175525 valid 5.114195300848223e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.333852459443733e-07\n",
      "  batch 101 loss: 2.645090675969186e-05\n",
      "LOSS train 2.9628616448001035e-05 valid 3.726667637238279e-05\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00041841201484203337\n",
      "  batch 101 loss: 0.01931575185575639\n",
      "LOSS train 0.010146793191387157 valid 0.00013909029075875878\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 8.623073517810553e-07\n",
      "  batch 101 loss: 0.00013458368383226116\n",
      "LOSS train 7.727636828552341e-05 valid 2.8017737349728122e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.6985109798260963e-07\n",
      "  batch 101 loss: 1.967879813946638e-05\n",
      "LOSS train 2.042419794756386e-05 valid 1.2007527402602136e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.917978781624697e-07\n",
      "  batch 101 loss: 2.974051134629008e-05\n",
      "LOSS train 4.533109076677887e-05 valid 4.6311335609061643e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.7792903236113488e-06\n",
      "  batch 101 loss: 9.953033038527792e-05\n",
      "LOSS train 0.00011962262973043502 valid 0.00020019584917463362\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2110529496567324e-06\n",
      "  batch 101 loss: 0.0016472866882031667\n",
      "LOSS train 0.002406183447608893 valid 0.00019344527390785515\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.000881679356098175\n",
      "  batch 101 loss: 0.013177677657658933\n",
      "LOSS train 0.007459323322598785 valid 0.0006891173543408513\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.446727154776454e-06\n",
      "  batch 101 loss: 0.00027653871060010713\n",
      "LOSS train 0.00032076797731233134 valid 0.00016376566782128066\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.232755160657689e-07\n",
      "  batch 101 loss: 0.0007068571495983633\n",
      "LOSS train 0.0010128283833238051 valid 0.0009308142471127212\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.5066880043596028e-05\n",
      "  batch 101 loss: 0.003220842907758197\n",
      "LOSS train 0.02254173143976309 valid 0.05856086686253548\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 0.00018143007531762123\n",
      "  batch 101 loss: 0.04500333185889758\n",
      "LOSS train 0.025092110995792425 valid 0.0015256042825058103\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.089517217129469e-05\n",
      "  batch 101 loss: 0.002399170993376174\n",
      "LOSS train 0.0015473262156880765 valid 0.00019079813500866294\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.004294447898864746\n",
      "  batch 101 loss: 0.014587521803114213\n",
      "LOSS train 0.009648494344985593 valid 3.5911481973016635e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.6170042474404906e-07\n",
      "  batch 101 loss: 3.978715250241294e-05\n",
      "LOSS train 2.5565560457953972e-05 valid 3.874330104736146e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.3120635458108154e-08\n",
      "  batch 101 loss: 2.1346278152236665e-05\n",
      "LOSS train 3.0551404395028666e-05 valid 2.4567150830989704e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.568687578896061e-07\n",
      "  batch 101 loss: 7.147015986447514e-05\n",
      "LOSS train 0.0001020754768391557 valid 0.00011465730494819582\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.255131352692843e-06\n",
      "  batch 101 loss: 0.00026201715463685104\n",
      "LOSS train 0.0003395895006261863 valid 0.0002454563800711185\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 4.416235606186092e-06\n",
      "  batch 101 loss: 0.0022944555337744534\n",
      "LOSS train 0.0032663015665398256 valid 0.001991280587390065\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(2)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [6]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [0]\n",
    "dors = [None,0,0.2,0.4,0.6]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            for dor in dors:\n",
    "                m = create_model(nb_hidden,d_ft_in['train'].shape[1],dor)\n",
    "                if dor is None:\n",
    "                    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "                else:                 \n",
    "                    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor\"\n",
    "                optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "\n",
    "                saved_models = dict()\n",
    "\n",
    "                for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                    path = f\"trained_models/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                    model = m\n",
    "                    m.load_state_dict(torch.load(path))\n",
    "                    m.eval()\n",
    "\n",
    "                    test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                    test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "\n",
    "                    train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                    train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "\n",
    "                    validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                    validation_loss = loss_fn(validation_prediction,d_ft_out[\"val\"])\n",
    "\n",
    "                    if mt == \"min_val\": \n",
    "                        min_val = True\n",
    "                    else: \n",
    "                        min_val = False\n",
    "\n",
    "                    r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                      \"Min_val\":min_val,\n",
    "                                      \"Epochs\": nb_e,\n",
    "                                      \"Lr\":lr,\n",
    "                                      \"Dor\": dor,\n",
    "                                      \"Tr_l\":train_loss.item(),\n",
    "                                      \"Te_l\":test_loss.item(),\n",
    "                                      \"V_l\": validation_loss.item()}\n",
    "                                     ,index = [i]\n",
    "                    )\n",
    "                    i+=1\n",
    "                    results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84991e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      True\n",
       "1      True\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10     True\n",
       "11     True\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17    False\n",
       "18    False\n",
       "19    False\n",
       "Name: Dor, dtype: bool"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "021c515c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'Te_l'}, xlabel='[Min_val]'>,\n",
       "        <Axes: title={'center': 'V_l'}, xlabel='[Min_val]'>],\n",
       "       [<Axes: title={'center': 'Tr_l'}, xlabel='[Min_val]'>, <Axes: >]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = (results.Dor).isnull() |(results.Dor) == 0\n",
    "results[f].boxplot(column = [\"Te_l\",\"V_l\",\"Tr_l\"],by = [\"Min_val\"],sharey = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
