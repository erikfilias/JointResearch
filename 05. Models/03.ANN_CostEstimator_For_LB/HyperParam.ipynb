{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b42ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NN_classes\n",
    "from torchvision import datasets, transforms\n",
    "import training_methods\n",
    "import DataLoading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cca8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_101_N_105_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_102_N_106_cac1\",\"Network_Line_Out_N_103_N_109_cac1\"]\n",
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\"]\n",
    "#executions = [\"Network_Full_Generation_Full\",\"Network_Line_In_N_101_N_102_cac1\",\"Network_Line_In_N_101_N_103_cac1\",\"Network_Line_In_N_101_N_105_cac1\"]\n",
    "\n",
    "executions = [\"Network_Existing_Generation_Full\"]\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"Samples_3-bus_ACOPF\"\n",
    "te_s = 0.1\n",
    "val_s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f16d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/input_f_sc01_Network_Existing_Generation_Full_2030.csv\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te(dfs_in,dfs_out,executions,te_s,val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4032fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out = DataLoading.concat_and_normalize(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da40d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "training_loader = DataLoader(train,batch_size=32)\n",
    "validation_loader = DataLoader(train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b1e7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6289, 23])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_ft_in['train'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fb281c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0021936340630054475\n",
      "  batch 101 loss: 0.008084873525513103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.005398804827254843 valid 0.00046850694343447685\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.848475367296487e-06\n",
      "  batch 101 loss: 0.000179266222962724\n",
      "LOSS train 0.00014913108567169114 valid 0.0004656228993553668\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.460045697167516e-06\n",
      "  batch 101 loss: 8.090905772746737e-05\n",
      "LOSS train 7.070910374244851e-05 valid 0.00017688451043795794\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.352863666601479e-06\n",
      "  batch 101 loss: 5.0711962428522385e-05\n",
      "LOSS train 4.5106938424041015e-05 valid 8.6059226305224e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([874])) that is different to the input size (torch.Size([874, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([6289])) that is different to the input size (torch.Size([6289, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 3.5267525583027036e-06\n",
      "LOSS train 1.8042565276598634e-06 valid 1.8270878854309558e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7451772293952671e-09\n",
      "  batch 101 loss: 1.1701225131055537e-07\n",
      "LOSS train 7.893608026330289e-08 valid 1.0667744732018036e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.498042174982402e-10\n",
      "  batch 101 loss: 9.928076639464934e-08\n",
      "LOSS train 6.448141113781532e-08 valid 4.4329382831165276e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7932307133937684e-10\n",
      "  batch 101 loss: 1.5157064648718687e-07\n",
      "LOSS train 1.2346435350678596e-07 valid 5.392543656057569e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.974652849137783e-05\n",
      "  batch 101 loss: 0.028787242122270983\n",
      "LOSS train 0.014833312932474137 valid 0.00039810611633583903\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.707629559561611e-06\n",
      "  batch 101 loss: 0.0002526248555705024\n",
      "LOSS train 0.00014245426134355254 valid 2.056378980341833e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 6.815429514972493e-08\n",
      "  batch 101 loss: 2.508340661620423e-05\n",
      "LOSS train 4.460710375222306e-05 valid 5.5906395573401824e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.294550747843459e-07\n",
      "  batch 101 loss: 0.00021505901177988564\n",
      "LOSS train 0.00035936391089257233 valid 0.00029035229817964137\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 6.484566458188068e-05\n",
      "LOSS train 3.2936733340913424e-05 valid 1.6291983229166362e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5391950114462816e-09\n",
      "  batch 101 loss: 8.724547989391596e-08\n",
      "LOSS train 6.037761759368691e-08 valid 1.6327339835697785e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.542880454508122e-09\n",
      "  batch 101 loss: 9.643946196291608e-08\n",
      "LOSS train 6.188615216562161e-08 valid 4.311463186468245e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.657102271541589e-10\n",
      "  batch 101 loss: 1.542105683949302e-07\n",
      "LOSS train 1.2409942332979863e-07 valid 5.547791204207897e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0020209568738937377\n",
      "  batch 101 loss: 0.3616099057792235\n",
      "LOSS train 0.18462286554719132 valid 3.540963007253595e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.3382144465576857e-07\n",
      "  batch 101 loss: 2.4640781515472553e-05\n",
      "LOSS train 1.5022578533542742e-05 valid 9.920799129758961e-06\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.945595148659777e-08\n",
      "  batch 101 loss: 1.9714334343490236e-06\n",
      "LOSS train 2.4640220409424973e-06 valid 2.7208388928556815e-06\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.2577578445416293e-08\n",
      "  batch 101 loss: 7.594375025519185e-06\n",
      "LOSS train 1.0106531278652625e-05 valid 9.885352483252063e-06\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.0010746598370171335\n",
      "LOSS train 0.0005455558131975714 valid 1.551010342382142e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.457648011182755e-09\n",
      "  batch 101 loss: 7.921439272973174e-08\n",
      "LOSS train 5.5102622062501104e-08 valid 1.740781669923308e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.655409107570449e-09\n",
      "  batch 101 loss: 9.318774020883324e-08\n",
      "LOSS train 6.000231088637311e-08 valid 4.004807152568901e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.310797064808412e-10\n",
      "  batch 101 loss: 1.584751797389927e-07\n",
      "LOSS train 1.3084249057593987e-07 valid 5.937912561648773e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.461014203727245e-05\n",
      "  batch 101 loss: 7.925635736305266\n",
      "LOSS train 4.045526163619151 valid 0.11080850660800934\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0007726238667964936\n",
      "  batch 101 loss: 0.017759496534526988\n",
      "LOSS train 0.009440395377687125 valid 5.5321597756119445e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.883520912495442e-07\n",
      "  batch 101 loss: 3.617331709733662e-05\n",
      "LOSS train 3.446528358457505e-05 valid 2.151284206775017e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.39305721456185e-08\n",
      "  batch 101 loss: 9.206410784258879e-05\n",
      "LOSS train 0.0001578842211500409 valid 0.00015680823707953095\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.017347024958151394\n",
      "LOSS train 0.008805919796393853 valid 1.5021299759609974e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.406616263466276e-09\n",
      "  batch 101 loss: 7.724726081903555e-08\n",
      "LOSS train 5.380958643694721e-08 valid 1.7618140191189013e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.6772938238318602e-09\n",
      "  batch 101 loss: 9.218936349864038e-08\n",
      "LOSS train 5.9491301396714756e-08 valid 3.89911498643869e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.1903787228438887e-10\n",
      "  batch 101 loss: 1.589186907136053e-07\n",
      "LOSS train 1.319555559191621e-07 valid 5.923027757148702e-08\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0024374157190322878\n",
      "  batch 101 loss: 0.008643947196032968\n",
      "LOSS train 0.005892578947667265 valid 0.0006130983238108456\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.6869830433279275e-05\n",
      "  batch 101 loss: 0.0005359067896461056\n",
      "LOSS train 0.0004461569013092507 valid 0.00034126461832784116\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.1036930372938513e-05\n",
      "  batch 101 loss: 0.00026614446498570034\n",
      "LOSS train 0.0002043637394188427 valid 9.593411232344806e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.221888327971101e-06\n",
      "  batch 101 loss: 9.41810014796829e-05\n",
      "LOSS train 6.842946815830949e-05 valid 5.338714981917292e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.3248466711957008e-06\n",
      "  batch 101 loss: 3.2329281266356704e-05\n",
      "LOSS train 2.3074179400310145e-05 valid 9.86209670372773e-06\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.103214865201153e-07\n",
      "  batch 101 loss: 8.852454480461346e-06\n",
      "LOSS train 6.8516011375382725e-06 valid 3.0843764307064703e-06\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.775508503167657e-08\n",
      "  batch 101 loss: 1.5960251979407759e-06\n",
      "LOSS train 1.5115667622395163e-06 valid 1.3200701687310357e-06\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0873153541979264e-08\n",
      "  batch 101 loss: 5.524222715180826e-07\n",
      "LOSS train 7.276916489982267e-07 valid 9.869105497273267e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 3.5267525583027036e-06\n",
      "LOSS train 1.8042565276598634e-06 valid 1.8270878854309558e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.7451772293952671e-09\n",
      "  batch 101 loss: 1.1701225131055537e-07\n",
      "LOSS train 7.893608026330289e-08 valid 1.0667744732018036e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.498042174982402e-10\n",
      "  batch 101 loss: 9.928076639464934e-08\n",
      "LOSS train 6.448141113781532e-08 valid 4.4329382831165276e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.7932307133937684e-10\n",
      "  batch 101 loss: 1.5157064648718687e-07\n",
      "LOSS train 1.2346435350678596e-07 valid 5.392543656057569e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.854053431950888e-10\n",
      "  batch 101 loss: 7.029986571982505e-08\n",
      "LOSS train 5.995282369256046e-08 valid 2.733861670378701e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 7.428877957238455e-11\n",
      "  batch 101 loss: 7.518641056192265e-08\n",
      "LOSS train 9.876818862380644e-08 valid 2.9021140335316886e-07\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.857171921277768e-09\n",
      "  batch 101 loss: 1.3596063913601242e-07\n",
      "LOSS train 1.4475787920074338e-07 valid 1.757144474368033e-07\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.6724354168218268e-09\n",
      "  batch 101 loss: 2.5508923273687145e-07\n",
      "LOSS train 2.0523051534132445e-07 valid 1.2188803566459683e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0018680933117866515\n",
      "  batch 101 loss: 0.009113366890014732\n",
      "LOSS train 0.005672433272491912 valid 9.563320054439828e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.590567060513422e-07\n",
      "  batch 101 loss: 2.913399426461183e-05\n",
      "LOSS train 3.102428174343419e-05 valid 1.8256620023748837e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.0804290620144455e-07\n",
      "  batch 101 loss: 1.953895844053477e-05\n",
      "LOSS train 4.0621261319262754e-05 valid 4.209112375974655e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.2654963433742523e-07\n",
      "  batch 101 loss: 0.0002285511808531737\n",
      "LOSS train 0.00018390549923405792 valid 0.00013123535609338433\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.6578849428915417e-07\n",
      "  batch 101 loss: 0.002063292056809587\n",
      "LOSS train 0.002917242730369521 valid 6.135980220278725e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.7892642063088715e-06\n",
      "  batch 101 loss: 0.00017879494212365898\n",
      "LOSS train 0.00020686968624376354 valid 0.00011061725672334433\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.296383561566472e-06\n",
      "  batch 101 loss: 0.00047079396111257663\n",
      "LOSS train 0.0005028123283977371 valid 3.964794086641632e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.3862946070730687e-06\n",
      "  batch 101 loss: 0.001305325943076241\n",
      "LOSS train 0.00243018246437991 valid 0.001671843696385622\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 6.484566458188068e-05\n",
      "LOSS train 3.2936733340913424e-05 valid 1.6291983229166362e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5391950114462816e-09\n",
      "  batch 101 loss: 8.724547989391596e-08\n",
      "LOSS train 6.037761759368691e-08 valid 1.6327339835697785e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.542880454508122e-09\n",
      "  batch 101 loss: 9.643946196291608e-08\n",
      "LOSS train 6.188615216562161e-08 valid 4.311463186468245e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.657102271541589e-10\n",
      "  batch 101 loss: 1.542105683949302e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 1.2409942332979863e-07 valid 5.547791204207897e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.0238514742441114e-10\n",
      "  batch 101 loss: 7.136823384268353e-08\n",
      "LOSS train 7.308476398013823e-08 valid 3.924785474396231e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2196834592591587e-10\n",
      "  batch 101 loss: 1.1251271335499524e-07\n",
      "LOSS train 1.202225075047636e-07 valid 4.7909562539416584e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.288227740621096e-10\n",
      "  batch 101 loss: 2.0354335726890939e-07\n",
      "LOSS train 1.4458170020492937e-07 valid 5.576764294801251e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9960183667299133e-10\n",
      "  batch 101 loss: 1.3484949927144995e-07\n",
      "LOSS train 1.497750838321911e-07 valid 1.3454170755267114e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.012243911027908325\n",
      "  batch 101 loss: 0.20678476940327528\n",
      "LOSS train 0.111556607448615 valid 0.0005304967053234577\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.073441428365186e-06\n",
      "  batch 101 loss: 0.0001968661233331659\n",
      "LOSS train 0.00011516872118671775 valid 1.856757990026381e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.3906805179431103e-07\n",
      "  batch 101 loss: 1.749761617702461e-05\n",
      "LOSS train 1.73304622666974e-05 valid 2.309350747964345e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.347557246044744e-08\n",
      "  batch 101 loss: 3.364424393510035e-05\n",
      "LOSS train 3.398904835864696e-05 valid 2.3454807887901552e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.811539813294076e-07\n",
      "  batch 101 loss: 0.00024524712459879084\n",
      "LOSS train 0.0004360584941889729 valid 1.7213376850122586e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.7022948415833527e-07\n",
      "  batch 101 loss: 0.0002912084467607201\n",
      "LOSS train 0.0003269869741951727 valid 0.0004258367989677936\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.257363230455667e-06\n",
      "  batch 101 loss: 0.0028029521989992646\n",
      "LOSS train 0.015929062865309093 valid 0.04298991337418556\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.00048822037875652315\n",
      "  batch 101 loss: 0.04036455450754147\n",
      "LOSS train 0.025435482738388887 valid 0.00029744170024059713\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.0010746598370171335\n",
      "LOSS train 0.0005455558131975714 valid 1.551010342382142e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.457648011182755e-09\n",
      "  batch 101 loss: 7.921439272973174e-08\n",
      "LOSS train 5.5102622062501104e-08 valid 1.740781669923308e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.655409107570449e-09\n",
      "  batch 101 loss: 9.318774020883324e-08\n",
      "LOSS train 6.000231088637311e-08 valid 4.004807152568901e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.310797064808412e-10\n",
      "  batch 101 loss: 1.584751797389927e-07\n",
      "LOSS train 1.3084249057593987e-07 valid 5.937912561648773e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.448884283192456e-10\n",
      "  batch 101 loss: 6.988984737610338e-08\n",
      "LOSS train 6.89106050620198e-08 valid 3.153174077397125e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.3130286191653795e-10\n",
      "  batch 101 loss: 1.0337004188176735e-07\n",
      "LOSS train 1.1752829395199965e-07 valid 3.8930412671334125e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.5006285991603364e-10\n",
      "  batch 101 loss: 2.1310668842033918e-07\n",
      "LOSS train 1.4935812216789778e-07 valid 6.094759896768664e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.468338860557196e-10\n",
      "  batch 101 loss: 1.3568153296228402e-07\n",
      "LOSS train 1.4850488999713797e-07 valid 2.429392793601437e-07\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0006510955095291138\n",
      "  batch 101 loss: 7.657909907195717\n",
      "LOSS train 3.9048403783476506 valid 0.09021532535552979\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0007082667946815491\n",
      "  batch 101 loss: 0.017034509050390625\n",
      "LOSS train 0.009116510645011667 valid 3.2195453968597576e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.694951242825482e-07\n",
      "  batch 101 loss: 2.7473058594296162e-05\n",
      "LOSS train 5.0076147900529524e-05 valid 0.00012544052151497453\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.80516534682829e-07\n",
      "  batch 101 loss: 0.000248834783201346\n",
      "LOSS train 0.0003041335123414101 valid 0.00016350900114048272\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.9753768467344343e-06\n",
      "  batch 101 loss: 0.000678132242319407\n",
      "LOSS train 0.0009234115482817066 valid 0.00104394624941051\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.5787461306899787e-05\n",
      "  batch 101 loss: 0.0020927886794379446\n",
      "LOSS train 0.0037023409239168937 valid 0.005229838192462921\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.685683149844408e-05\n",
      "  batch 101 loss: 0.005892877173318993\n",
      "LOSS train 0.006364223480208063 valid 0.0031561870127916336\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.00010767487809062004\n",
      "  batch 101 loss: 0.11486776968697086\n",
      "LOSS train 0.2059712901529454 valid 0.10963299870491028\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.598357001370459e-09\n",
      "  batch 101 loss: 0.017347024958151394\n",
      "LOSS train 0.008805919796393853 valid 1.5021299759609974e-07\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.406616263466276e-09\n",
      "  batch 101 loss: 7.724726081903555e-08\n",
      "LOSS train 5.380958643694721e-08 valid 1.7618140191189013e-07\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.6772938238318602e-09\n",
      "  batch 101 loss: 9.218936349864038e-08\n",
      "LOSS train 5.9491301396714756e-08 valid 3.89911498643869e-08\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.1903787228438887e-10\n",
      "  batch 101 loss: 1.589186907136053e-07\n",
      "LOSS train 1.319555559191621e-07 valid 5.923027757148702e-08\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.432703804013727e-10\n",
      "  batch 101 loss: 6.955992025492463e-08\n",
      "LOSS train 6.755981519739848e-08 valid 2.998643822138547e-08\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.119116088688088e-10\n",
      "  batch 101 loss: 9.978589082138001e-08\n",
      "LOSS train 1.1698638357079467e-07 valid 3.29759615169678e-08\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0046480980463458e-10\n",
      "  batch 101 loss: 2.2008828680419869e-07\n",
      "LOSS train 1.5165370330537253e-07 valid 5.043294137863086e-08\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.5141243753523667e-10\n",
      "  batch 101 loss: 1.3491252473407656e-07\n",
      "LOSS train 1.4874138317074404e-07 valid 2.0926401589349553e-07\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(4)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,3]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            if nb_hidden == 0: \n",
    "                m = NN_classes.ObjectiveEstimator_ANN_Single_layer(input_size=d_ft_in['train'].shape[1],output_size=1)\n",
    "                \n",
    "            elif nb_hidden == 3: \n",
    "                m = NN_classes.ObjectiveEstimator_ANN_3hidden_layer(input_size=d_ft_in['train'].shape[1],hidden_size1=int(d_ft_in['train'].shape[1]/4),hidden_size2=int(d_ft_in['train'].shape[1]/16),hidden_size3=int(d_ft_in['train'].shape[1]/64),output_size=1)\n",
    "            m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "            \n",
    "            test_predictions = m(d_ft_in[\"test\"].float())\n",
    "            test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "            train_predictions = m(d_ft_in[\"train\"].float())\n",
    "            train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "            \n",
    "            r = pd.DataFrame({\"Model_type\": m_name,\"Epochs\": nb_e,\"Lr\":lr, \"Tr_l\":train_loss.item(),\"Te_l\":test_loss.item()},index = [i]\n",
    "            )\n",
    "            i+=1\n",
    "            results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450d1808",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_type</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Lr</th>\n",
       "      <th>Tr_l</th>\n",
       "      <th>Te_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.995044e-02</td>\n",
       "      <td>3.113820e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.01</td>\n",
       "      <td>6.880393e-08</td>\n",
       "      <td>5.301107e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.364681e-04</td>\n",
       "      <td>2.971305e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.932472e-08</td>\n",
       "      <td>5.343779e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2.749902e-05</td>\n",
       "      <td>2.335273e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.16</td>\n",
       "      <td>6.936998e-08</td>\n",
       "      <td>5.347490e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sl</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2.291427e-04</td>\n",
       "      <td>1.987444e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3h</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>6.930760e-08</td>\n",
       "      <td>5.342376e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.233874e-06</td>\n",
       "      <td>1.735164e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2.290619e-08</td>\n",
       "      <td>3.574458e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.249689e-06</td>\n",
       "      <td>9.934462e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>5.761602e-08</td>\n",
       "      <td>8.002224e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.945337e-03</td>\n",
       "      <td>3.184862e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4.236949e-07</td>\n",
       "      <td>4.828290e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sl</td>\n",
       "      <td>8</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.091165e-01</td>\n",
       "      <td>7.309395e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3h</td>\n",
       "      <td>8</td>\n",
       "      <td>0.64</td>\n",
       "      <td>4.441067e-07</td>\n",
       "      <td>5.045854e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model_type  Epochs    Lr          Tr_l          Te_l\n",
       "0          sl       4  0.01  1.995044e-02  3.113820e-02\n",
       "1          3h       4  0.01  6.880393e-08  5.301107e-08\n",
       "2          sl       4  0.04  2.364681e-04  2.971305e-04\n",
       "3          3h       4  0.04  6.932472e-08  5.343779e-08\n",
       "4          sl       4  0.16  2.749902e-05  2.335273e-05\n",
       "5          3h       4  0.16  6.936998e-08  5.347490e-08\n",
       "6          sl       4  0.64  2.291427e-04  1.987444e-04\n",
       "7          3h       4  0.64  6.930760e-08  5.342376e-08\n",
       "8          sl       8  0.01  2.233874e-06  1.735164e-06\n",
       "9          3h       8  0.01  2.290619e-08  3.574458e-08\n",
       "10         sl       8  0.04  6.249689e-06  9.934462e-06\n",
       "11         3h       8  0.04  5.761602e-08  8.002224e-08\n",
       "12         sl       8  0.16  1.945337e-03  3.184862e-03\n",
       "13         3h       8  0.16  4.236949e-07  4.828290e-07\n",
       "14         sl       8  0.64  1.091165e-01  7.309395e-02\n",
       "15         3h       8  0.64  4.441067e-07  5.045854e-07"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
