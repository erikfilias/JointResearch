{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b42ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split,TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import NN_classes\n",
    "from torchvision import datasets, transforms\n",
    "import training_methods\n",
    "import DataLoading\n",
    "import pivottablejs\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cca8e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_101_N_105_cac1\",\"Network_Line_Out_N_102_N_104_cac1\",\"Network_Line_Out_N_102_N_106_cac1\",\"Network_Line_Out_N_103_N_109_cac1\"]\n",
    "#executions = [\"Network_Line_Out_N_101_N_102_cac1\"]\n",
    "#executions = [\"Network_Full_Generation_Full\",\"Network_Line_In_N_101_N_102_cac1\",\"Network_Line_In_N_101_N_103_cac1\",\"Network_Line_In_N_101_N_105_cac1\"]\n",
    "\n",
    "executions = [\"Network_Existing_Generation_Full\"]\n",
    "\n",
    "sc = \"sc01\"\n",
    "period = \"2030\"\n",
    "folder = \"../Data/RTS24_AC_12w\"\n",
    "all_executions = DataLoading.list_executions(folder=\"../Data/RTS24_AC_12w\",per = period,sc=sc)\n",
    "executions = all_executions[1:10]\n",
    "te_s = 0.1\n",
    "val_s = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f16d73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_f_sc01_Network_Line_In_N_101_N_102_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_103_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_101_N_105_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_104_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_102_N_106_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_103_N_124_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_104_N_109_cac1_2030.csv\n",
      "1227\n",
      "input_f_sc01_Network_Line_In_N_105_N_110_cac1_2030.csv\n",
      "1227\n"
     ]
    }
   ],
   "source": [
    "dfs_in,dfs_out = DataLoading.load_data(folder,executions,period,sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_in,ts_out =  DataLoading.split_tr_val_te(dfs_in,dfs_out,executions,te_s,val_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d4032fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ft_in, d_ft_out = DataLoading.concat_and_normalize(ts_in,ts_out,executions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da40d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TensorDataset(d_ft_in['train'].float(), d_ft_out['train'].float())\n",
    "validation = TensorDataset(d_ft_in['val'].float(), d_ft_out['val'].float())\n",
    "\n",
    "training_loader = DataLoader(train,batch_size=32)\n",
    "validation_loader = DataLoader(train,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4489ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nb_hidden, input_size, dropout_ratio):\n",
    "    hidden_sizes = []\n",
    "    if nb_hidden == 0:\n",
    "        hidden_sizes.append(input_size)\n",
    "    elif nb_hidden == 1:\n",
    "        hidden_sizes.extend([int(math.sqrt(input_size))])\n",
    "    elif nb_hidden == 2:\n",
    "        hidden_sizes.extend([int(math.sqrt(input_size)), int(math.sqrt(math.sqrt(input_size)))])\n",
    "    elif nb_hidden == 3:\n",
    "        hidden_sizes.extend([int(input_size / 4), int(input_size / 16), int(input_size / 64)])\n",
    "\n",
    "\n",
    "\n",
    "    if nb_hidden == 0:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_Single_layer\n",
    "    elif nb_hidden == 1:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_1hidden_layer\n",
    "    elif nb_hidden == 2:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_2hidden_layer\n",
    "    elif nb_hidden == 3:\n",
    "        model_class = NN_classes.ObjectiveEstimator_ANN_3hidden_layer\n",
    "    model = model_class(input_size=input_size, hidden_sizes=hidden_sizes, output_size=1, dropout_ratio=dor)\n",
    "    print(model,dor,nb_hidden)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9493ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001398173812776804\n",
      "  batch 101 loss: 0.1586406407452523\n",
      "  batch 201 loss: 0.00014118325937488409\n",
      "  batch 301 loss: 8.096177173683828e-05\n",
      "  batch 401 loss: 9.90935379536495e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([4, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.035919564745541324 valid 0.00019866936781909317\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.000007124384865e-07\n",
      "  batch 101 loss: 3.7357704330815976e-05\n",
      "  batch 201 loss: 5.330982817213226e-05\n",
      "  batch 301 loss: 2.894053589898249e-05\n",
      "  batch 401 loss: 8.906744471573802e-05\n",
      "LOSS train 5.947155338625661e-05 valid 0.00016058000619523227\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.6121218979824337e-07\n",
      "  batch 101 loss: 3.930450857865253e-05\n",
      "  batch 201 loss: 0.00010180937398445167\n",
      "  batch 301 loss: 0.00023323037463796937\n",
      "  batch 401 loss: 0.00015954672208863486\n",
      "LOSS train 0.00013164083147102642 valid 0.00036362078390084207\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.656290598679334e-06\n",
      "  batch 101 loss: 0.00018224253232801857\n",
      "  batch 201 loss: 6.99857623965272e-05\n",
      "  batch 301 loss: 4.5193153365516995e-05\n",
      "  batch 401 loss: 0.0001264547496043633\n",
      "LOSS train 0.00019055059289864443 valid 0.012457850389182568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1971])) that is different to the input size (torch.Size([1971, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([14148])) that is different to the input size (torch.Size([14148, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Workdir\\Programs\\Miniconda\\envs\\jr23\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([3537])) that is different to the input size (torch.Size([3537, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.913058787584305e-05\n",
      "  batch 101 loss: 0.03511278539280283\n",
      "  batch 201 loss: 5.5124865866673645e-05\n",
      "  batch 301 loss: 3.598213260431748e-05\n",
      "  batch 401 loss: 2.1471341743790616e-05\n",
      "LOSS train 0.007972459852391306 valid 5.6656110245967284e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.4097095117904247e-07\n",
      "  batch 101 loss: 4.177051236240459e-06\n",
      "  batch 201 loss: 5.723134957236198e-06\n",
      "  batch 301 loss: 6.433188012238134e-06\n",
      "  batch 401 loss: 5.9980331582210055e-06\n",
      "LOSS train 5.594165436293119e-06 valid 4.825448922929354e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.1961312793573598e-08\n",
      "  batch 101 loss: 4.804748200228914e-06\n",
      "  batch 201 loss: 4.293244986683931e-06\n",
      "  batch 301 loss: 4.780525641763234e-06\n",
      "  batch 401 loss: 4.653659205757776e-06\n",
      "LOSS train 4.544848347232548e-06 valid 5.25660980201792e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.6429635151289404e-08\n",
      "  batch 101 loss: 3.7356480470407404e-06\n",
      "  batch 201 loss: 3.2095203647486413e-06\n",
      "  batch 301 loss: 2.923374512420196e-06\n",
      "  batch 401 loss: 2.824673755412732e-06\n",
      "LOSS train 3.1551369838154196e-06 valid 5.1555274694692343e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.000796569287776947\n",
      "  batch 101 loss: 0.024184570604002146\n",
      "  batch 201 loss: 9.80432194239711e-05\n",
      "  batch 301 loss: 0.00010863370923686943\n",
      "  batch 401 loss: 9.915681581333046e-05\n",
      "LOSS train 0.005714486268190547 valid 0.00021980819292366505\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.134780272375792e-06\n",
      "  batch 101 loss: 8.373018157726619e-05\n",
      "  batch 201 loss: 7.419063088605071e-05\n",
      "  batch 301 loss: 6.548338307993618e-05\n",
      "  batch 401 loss: 6.092783344001873e-05\n",
      "LOSS train 6.961117407650781e-05 valid 0.0001600875984877348\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.2679920948576184e-06\n",
      "  batch 101 loss: 6.0656258896187864e-05\n",
      "  batch 201 loss: 5.92787917199189e-05\n",
      "  batch 301 loss: 5.750455071279248e-05\n",
      "  batch 401 loss: 5.7936866855357036e-05\n",
      "LOSS train 5.8702175122777094e-05 valid 0.00015387148596346378\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.173500106437132e-06\n",
      "  batch 101 loss: 6.121073346974981e-05\n",
      "  batch 201 loss: 6.050416080313425e-05\n",
      "  batch 301 loss: 5.892476324618201e-05\n",
      "  batch 401 loss: 5.873652927675721e-05\n",
      "LOSS train 5.987542334195635e-05 valid 0.00011073270434280857\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.755889211897739e-08\n",
      "  batch 101 loss: 0.015822196390868157\n",
      "  batch 201 loss: 0.00010208491919456719\n",
      "  batch 301 loss: 3.6615070505376935e-05\n",
      "  batch 401 loss: 2.6381451177712734e-05\n",
      "LOSS train 0.0036119765040923493 valid 0.00016983327805064619\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1339323100401089e-06\n",
      "  batch 101 loss: 2.9770016111285714e-05\n",
      "  batch 201 loss: 1.0365563217646923e-05\n",
      "  batch 301 loss: 1.0293662468825459e-05\n",
      "  batch 401 loss: 2.9875962401320066e-05\n",
      "LOSS train 1.946851148345733e-05 valid 0.00010015031875809655\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 5.375852560973726e-07\n",
      "  batch 101 loss: 1.1995142396585833e-05\n",
      "  batch 201 loss: 6.105412650185826e-06\n",
      "  batch 301 loss: 7.805317259226286e-06\n",
      "  batch 401 loss: 1.2965418232511183e-05\n",
      "LOSS train 9.611969967401397e-06 valid 8.502120908815414e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.6891746276523916e-07\n",
      "  batch 101 loss: 8.745667130369839e-06\n",
      "  batch 201 loss: 6.9738551474074486e-06\n",
      "  batch 301 loss: 5.89239175297962e-06\n",
      "  batch 401 loss: 4.5959442121557e-06\n",
      "LOSS train 6.550987581291588e-06 valid 6.04587075940799e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00029342692345380784\n",
      "  batch 101 loss: 2.7477309821074596\n",
      "  batch 201 loss: 0.0018003452084667515\n",
      "  batch 301 loss: 0.000508770750820986\n",
      "  batch 401 loss: 0.0002758379264560062\n",
      "LOSS train 0.6209162686505859 valid 0.0008300840272568166\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.6822675358271227e-07\n",
      "  batch 101 loss: 0.00014016108835676277\n",
      "  batch 201 loss: 8.749629571298102e-05\n",
      "  batch 301 loss: 3.6285663898070195e-05\n",
      "  batch 401 loss: 0.00015992581574323595\n",
      "LOSS train 0.00011206656275660752 valid 0.0013258956605568528\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.987354575656354e-06\n",
      "  batch 101 loss: 0.0002215622812991569\n",
      "  batch 201 loss: 0.00020059274417235428\n",
      "  batch 301 loss: 5.912853660220208e-05\n",
      "  batch 401 loss: 0.0005893613437410749\n",
      "LOSS train 0.00045237567543989583 valid 0.003145806025713682\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.754947006702423e-05\n",
      "  batch 101 loss: 0.00029248505753912466\n",
      "  batch 201 loss: 0.00019055819128425356\n",
      "  batch 301 loss: 5.524947408957814e-05\n",
      "  batch 401 loss: 0.0004259073741013708\n",
      "LOSS train 0.0002445171475594906 valid 0.002377424854785204\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0003962477669119835\n",
      "  batch 101 loss: 0.49233932675737835\n",
      "  batch 201 loss: 0.02015476420030609\n",
      "  batch 301 loss: 0.0001103042385398112\n",
      "  batch 401 loss: 9.975584027642981e-05\n",
      "LOSS train 0.11583243024943336 valid 0.00013020139886066318\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.802406186470762e-06\n",
      "  batch 101 loss: 0.00010672801993564462\n",
      "  batch 201 loss: 0.00011000998207379098\n",
      "  batch 301 loss: 0.00010883870585573163\n",
      "  batch 401 loss: 0.0001051676077531738\n",
      "LOSS train 0.00010475213027790775 valid 0.00022352844825945795\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.1869192025624217e-06\n",
      "  batch 101 loss: 9.879267559625759e-05\n",
      "  batch 201 loss: 9.323651120254794e-05\n",
      "  batch 301 loss: 8.448109044707053e-05\n",
      "  batch 401 loss: 7.759600406643585e-05\n",
      "LOSS train 8.609598668362618e-05 valid 0.00019277361570857465\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.75016063824296e-06\n",
      "  batch 101 loss: 7.281829839826059e-05\n",
      "  batch 201 loss: 6.882571749997624e-05\n",
      "  batch 301 loss: 6.366988351373948e-05\n",
      "  batch 401 loss: 6.0841430518507877e-05\n",
      "LOSS train 6.542633285518633e-05 valid 0.00016156231868080795\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0001028232928365469\n",
      "  batch 101 loss: 0.027981652794200045\n",
      "  batch 201 loss: 7.387982617842681e-05\n",
      "  batch 301 loss: 5.706139483891093e-05\n",
      "  batch 401 loss: 5.785298952048379e-05\n",
      "LOSS train 0.0063875568883785035 valid 0.00013833663251716644\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.9322408479638396e-06\n",
      "  batch 101 loss: 5.905533578925315e-05\n",
      "  batch 201 loss: 5.4209442978390144e-05\n",
      "  batch 301 loss: 4.970254634770299e-05\n",
      "  batch 401 loss: 4.7340458008591215e-05\n",
      "LOSS train 5.224350413155789e-05 valid 7.269349589478225e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.246596214827151e-07\n",
      "  batch 101 loss: 4.4442536478754845e-05\n",
      "  batch 201 loss: 4.514138475542495e-05\n",
      "  batch 301 loss: 4.704269038029452e-05\n",
      "  batch 401 loss: 4.917417285469128e-05\n",
      "LOSS train 4.673137803581873e-05 valid 6.317673251032829e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.19854732172098e-07\n",
      "  batch 101 loss: 4.60319166154477e-05\n",
      "  batch 201 loss: 4.622803457550617e-05\n",
      "  batch 301 loss: 4.3934650745427464e-05\n",
      "  batch 401 loss: 4.056675391694853e-05\n",
      "LOSS train 4.408154323624674e-05 valid 6.624402885790914e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002225465141236782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 8.945123964271039\n",
      "  batch 201 loss: 2.6302503148087908e-05\n",
      "  batch 301 loss: 2.19729291643489e-05\n",
      "  batch 401 loss: 1.4866874900576477e-05\n",
      "LOSS train 2.019280530516372 valid 4.188019738649018e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.688259539558203e-08\n",
      "  batch 101 loss: 5.881314745295185e-06\n",
      "  batch 201 loss: 4.411972436457745e-06\n",
      "  batch 301 loss: 5.112290766930982e-06\n",
      "  batch 401 loss: 4.789388825372498e-06\n",
      "LOSS train 4.925854465453364e-06 valid 5.9439862525323406e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.6177342483133545e-08\n",
      "  batch 101 loss: 4.965640918328517e-06\n",
      "  batch 201 loss: 5.812016117943131e-06\n",
      "  batch 301 loss: 6.838375041695599e-06\n",
      "  batch 401 loss: 6.469398819461958e-06\n",
      "LOSS train 5.9526129298491516e-06 valid 5.8650031860452145e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.3816466637072154e-07\n",
      "  batch 101 loss: 1.1817582943365324e-05\n",
      "  batch 201 loss: 1.0867928694437978e-05\n",
      "  batch 301 loss: 1.3423143478803467e-05\n",
      "  batch 401 loss: 7.122111482544824e-06\n",
      "LOSS train 1.095074825596325e-05 valid 8.931792399380356e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00014142868109047414\n",
      "  batch 101 loss: 45.80776677718386\n",
      "  batch 201 loss: 0.024640014900360257\n",
      "  batch 301 loss: 0.005696055528533179\n",
      "  batch 401 loss: 0.0022012896132400784\n",
      "LOSS train 10.347864009862061 valid 0.0018106781644746661\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.078709408640861e-05\n",
      "  batch 101 loss: 0.0008299801136763562\n",
      "  batch 201 loss: 0.000508895049581497\n",
      "  batch 301 loss: 8.227400675082209e-05\n",
      "  batch 401 loss: 3.4643964935412443e-05\n",
      "LOSS train 0.00035417081124663785 valid 0.0011431947350502014\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1423762664198876e-05\n",
      "  batch 101 loss: 0.0005092021752102483\n",
      "  batch 201 loss: 0.0004604540029004056\n",
      "  batch 301 loss: 0.00011546555721110963\n",
      "  batch 401 loss: 0.00013344387754386843\n",
      "LOSS train 0.0003221888438452637 valid 0.0010281599825248122\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.6004821993410587e-05\n",
      "  batch 101 loss: 0.0019323988157702843\n",
      "  batch 201 loss: 0.00089787411701991\n",
      "  batch 301 loss: 0.00011362747114276316\n",
      "  batch 401 loss: 0.0001310595220718369\n",
      "LOSS train 0.0007041517926137033 valid 0.0011326659005135298\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 8.781313896179199e-05\n",
      "  batch 101 loss: 54.03841131091118\n",
      "  batch 201 loss: 0.022389212710222636\n",
      "  batch 301 loss: 8.055451929976698e-05\n",
      "  batch 401 loss: 7.411898368445691e-05\n",
      "LOSS train 12.203403118563921 valid 6.603067595278844e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.339151175576262e-07\n",
      "  batch 101 loss: 7.776609752909281e-05\n",
      "  batch 201 loss: 8.017215668587596e-05\n",
      "  batch 301 loss: 8.256743311449099e-05\n",
      "  batch 401 loss: 8.585295854118157e-05\n",
      "LOSS train 8.184560845109375e-05 valid 7.863915379857644e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.649651863379404e-07\n",
      "  batch 101 loss: 8.9788487646274e-05\n",
      "  batch 201 loss: 9.236421170498943e-05\n",
      "  batch 301 loss: 9.494414736309409e-05\n",
      "  batch 401 loss: 9.833940210000947e-05\n",
      "LOSS train 9.340380390680423e-05 valid 0.00010892656428040937\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.4471598842646928e-06\n",
      "  batch 101 loss: 0.00010210500507128017\n",
      "  batch 201 loss: 0.00010459626341798866\n",
      "  batch 301 loss: 0.0001062877385959382\n",
      "  batch 401 loss: 0.00010823753633872002\n",
      "LOSS train 0.00010370265003761701 valid 0.00016470516857225448\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0008771500736474991\n",
      "  batch 101 loss: 13.45268089371757\n",
      "  batch 201 loss: 0.00011421786983191851\n",
      "  batch 301 loss: 8.366421820483084e-05\n",
      "  batch 401 loss: 9.076045274014177e-05\n",
      "LOSS train 3.0369939907858754 valid 9.512010001344606e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1984415323240682e-06\n",
      "  batch 101 loss: 9.832349900193549e-05\n",
      "  batch 201 loss: 0.00010319572312710079\n",
      "  batch 301 loss: 0.00010658644386694504\n",
      "  batch 401 loss: 0.0001090443222756221\n",
      "LOSS train 0.00010253872203117678 valid 0.00018297047063242644\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.607812930364162e-06\n",
      "  batch 101 loss: 0.00010936092836345779\n",
      "  batch 201 loss: 0.00010856133667402901\n",
      "  batch 301 loss: 0.00010357484851965637\n",
      "  batch 401 loss: 9.801714925060879e-05\n",
      "LOSS train 0.0001018875394638011 valid 0.0002230508252978325\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.180236672051251e-06\n",
      "  batch 101 loss: 9.190440690701961e-05\n",
      "  batch 201 loss: 8.700000028738942e-05\n",
      "  batch 301 loss: 7.942811215798429e-05\n",
      "  batch 401 loss: 7.383574410255278e-05\n",
      "LOSS train 8.097929723218315e-05 valid 0.00018657222972251475\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.7953676655888555e-05\n",
      "  batch 101 loss: 100126.6928911039\n",
      "  batch 201 loss: 0.525316059589386\n",
      "  batch 301 loss: 0.4517441713809967\n",
      "  batch 401 loss: 0.3735024455189705\n",
      "LOSS train 22602.297520916785 valid 0.30313822627067566\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.0029751086235046387\n",
      "  batch 101 loss: 0.26734509870409967\n",
      "  batch 201 loss: 0.20319077715277672\n",
      "  batch 301 loss: 0.14938024252653123\n",
      "  batch 401 loss: 0.10629394948482514\n",
      "LOSS train 0.1723891593700876 valid 0.07499746233224869\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0007218214869499207\n",
      "  batch 101 loss: 0.06159184891730547\n",
      "  batch 201 loss: 0.04040858998894691\n",
      "  batch 301 loss: 0.025598362442106008\n",
      "  batch 401 loss: 0.015666060484945774\n",
      "LOSS train 0.033541125255903866 valid 0.009542141109704971\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 8.518773131072522e-05\n",
      "  batch 101 loss: 0.00725893972441554\n",
      "  batch 201 loss: 0.00407167146448046\n",
      "  batch 301 loss: 0.002202684785006568\n",
      "  batch 401 loss: 0.0011600826180074365\n",
      "LOSS train 0.003409642351446484 valid 0.0006218557828105986\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0004018294811248779\n",
      "  batch 101 loss: 0.14249471422699572\n",
      "  batch 201 loss: 0.00014058383314477396\n",
      "  batch 301 loss: 6.191770022041965e-05\n",
      "  batch 401 loss: 0.00028028587364133274\n",
      "LOSS train 0.03237735308459807 valid 0.0003704955743160099\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.359116014209576e-08\n",
      "  batch 101 loss: 4.7178416066344654e-05\n",
      "  batch 201 loss: 9.361735040556595e-05\n",
      "  batch 301 loss: 4.016739217149734e-05\n",
      "  batch 401 loss: 0.00043043122572271385\n",
      "LOSS train 0.00016307418089396116 valid 0.00036110321525484324\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.213541276636533e-07\n",
      "  batch 101 loss: 4.4736162941489965e-05\n",
      "  batch 201 loss: 0.00021008338800129422\n",
      "  batch 301 loss: 0.00017766064537227066\n",
      "  batch 401 loss: 0.0005701543251007025\n",
      "LOSS train 0.00028800133564009045 valid 0.0008696793811395764\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.874153623357415e-06\n",
      "  batch 101 loss: 0.00013618760590645708\n",
      "  batch 201 loss: 0.0002315037829544053\n",
      "  batch 301 loss: 0.0002203547007366069\n",
      "  batch 401 loss: 0.0029785775049640507\n",
      "LOSS train 0.0012398993547458662 valid 0.0032438451889902353\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.3632235825061796e-05\n",
      "  batch 101 loss: 0.004442576925175672\n",
      "  batch 201 loss: 0.0011932990945570056\n",
      "  batch 301 loss: 0.00941410307699698\n",
      "  batch 401 loss: 0.009303794963889231\n",
      "LOSS train 0.005559501596741218 valid 0.00046299942187033594\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.923164386942517e-08\n",
      "  batch 101 loss: 0.000125651543830827\n",
      "  batch 201 loss: 0.00015105392425311947\n",
      "  batch 301 loss: 0.0003648008038669559\n",
      "  batch 401 loss: 0.009598784452518884\n",
      "LOSS train 0.0024422735099915165 valid 0.0007076191250234842\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1243392509641126e-06\n",
      "  batch 101 loss: 0.0015194937164415023\n",
      "  batch 201 loss: 0.0009128044980752747\n",
      "  batch 301 loss: 0.005582151175763102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 0.0034162008258408604\n",
      "LOSS train 0.002922235282826898 valid 0.0074731092900037766\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 5.22959278896451e-05\n",
      "  batch 101 loss: 0.003975816387392115\n",
      "  batch 201 loss: 0.003946711393664373\n",
      "  batch 301 loss: 0.0035537241381416607\n",
      "  batch 401 loss: 0.002439428385368956\n",
      "LOSS train 0.003298366599610909 valid 0.0013076363829895854\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.114022314548492e-05\n",
      "  batch 101 loss: 0.06495047225108465\n",
      "  batch 201 loss: 0.0020124564783736785\n",
      "  batch 301 loss: 7.689639202908438e-05\n",
      "  batch 401 loss: 8.671985319779197e-05\n",
      "LOSS train 0.015167882382221224 valid 8.456564683001488e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 9.916511771734804e-07\n",
      "  batch 101 loss: 9.368188086227746e-05\n",
      "  batch 201 loss: 9.828956000546895e-05\n",
      "  batch 301 loss: 0.00010222691074432077\n",
      "  batch 401 loss: 0.0001060903108225375\n",
      "LOSS train 9.89589446840616e-05 valid 0.00015214459563139826\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.1470563660841435e-06\n",
      "  batch 101 loss: 0.00010896324198910179\n",
      "  batch 201 loss: 0.00011043087716075206\n",
      "  batch 301 loss: 0.00010875753209802497\n",
      "  batch 401 loss: 0.0001062005912638142\n",
      "LOSS train 0.00010583288663081374 valid 0.00021963028120808303\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 3.1322840368375184e-06\n",
      "  batch 101 loss: 0.00010222155307189951\n",
      "  batch 201 loss: 9.876765541946497e-05\n",
      "  batch 301 loss: 9.175274090722496e-05\n",
      "  batch 401 loss: 8.576477898202484e-05\n",
      "LOSS train 9.204682786294174e-05 valid 0.00020868980209343135\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.9778838506899773e-06\n",
      "  batch 101 loss: 8.094255883861479e-05\n",
      "  batch 201 loss: 7.691941039070116e-05\n",
      "  batch 301 loss: 7.090749059443624e-05\n",
      "  batch 401 loss: 6.69693765064494e-05\n",
      "LOSS train 7.240282679630423e-05 valid 0.00017369736451655626\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.4714827304705977e-06\n",
      "  batch 101 loss: 6.560289320418633e-05\n",
      "  batch 201 loss: 6.330317668926e-05\n",
      "  batch 301 loss: 5.991109480191881e-05\n",
      "  batch 401 loss: 5.8479741472154956e-05\n",
      "LOSS train 6.106850326700628e-05 valid 0.0001581912802066654\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.2392757819034157e-06\n",
      "  batch 101 loss: 6.0222790089596856e-05\n",
      "  batch 201 loss: 5.938126863497928e-05\n",
      "  batch 301 loss: 5.7745357416933984e-05\n",
      "  batch 401 loss: 5.799886653903741e-05\n",
      "LOSS train 5.861544804122665e-05 valid 0.00015619340410921723\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.208918012911454e-06\n",
      "  batch 101 loss: 6.113504647260016e-05\n",
      "  batch 201 loss: 6.066225611419895e-05\n",
      "  batch 301 loss: 5.9398533393846266e-05\n",
      "  batch 401 loss: 5.987157251752251e-05\n",
      "LOSS train 6.033641809817679e-05 valid 0.00013461304479278624\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005720805004239082\n",
      "  batch 101 loss: 0.000956655354006557\n",
      "  batch 201 loss: 7.39121613230509e-05\n",
      "  batch 301 loss: 5.791706629509008e-05\n",
      "  batch 401 loss: 5.71728388331394e-05\n",
      "LOSS train 0.0003928593485270324 valid 0.00015037723642308265\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.119905693689361e-06\n",
      "  batch 101 loss: 6.103472673189003e-05\n",
      "  batch 201 loss: 5.880312423869327e-05\n",
      "  batch 301 loss: 5.5474082452064976e-05\n",
      "  batch 401 loss: 5.3048013598981924e-05\n",
      "LOSS train 5.679253892168582e-05 valid 8.091030031209812e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 9.147173841483891e-07\n",
      "  batch 101 loss: 4.742707511582012e-05\n",
      "  batch 201 loss: 4.605164107204018e-05\n",
      "  batch 301 loss: 4.5363383106291624e-05\n",
      "  batch 401 loss: 4.585021299192249e-05\n",
      "LOSS train 4.6194235987039567e-05 valid 7.203859422588721e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 7.079916395014152e-07\n",
      "  batch 101 loss: 4.485681232154093e-05\n",
      "  batch 201 loss: 4.582789105143092e-05\n",
      "  batch 301 loss: 4.731624779708454e-05\n",
      "  batch 401 loss: 4.887945303721608e-05\n",
      "LOSS train 4.691230726690433e-05 valid 6.619752093683928e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 5.395504558691755e-07\n",
      "  batch 101 loss: 4.682816439128601e-05\n",
      "  batch 201 loss: 4.827804617036691e-05\n",
      "  batch 301 loss: 4.8911530036548357e-05\n",
      "  batch 401 loss: 4.833795571016708e-05\n",
      "LOSS train 4.81099928337635e-05 valid 6.165344530018046e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.4292045054608023e-07\n",
      "  batch 101 loss: 4.392455735768408e-05\n",
      "  batch 201 loss: 4.504924864363602e-05\n",
      "  batch 301 loss: 4.447571568789499e-05\n",
      "  batch 401 loss: 4.290649521863088e-05\n",
      "LOSS train 4.41292448100879e-05 valid 6.514812412206084e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.1745833035092801e-07\n",
      "  batch 101 loss: 3.933958004438409e-05\n",
      "  batch 201 loss: 3.991912967990174e-05\n",
      "  batch 301 loss: 3.939873715921749e-05\n",
      "  batch 401 loss: 3.8285381955986964e-05\n",
      "LOSS train 3.946234088013903e-05 valid 6.587837560800835e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0610130630084313e-07\n",
      "  batch 101 loss: 3.655637805067613e-05\n",
      "  batch 201 loss: 3.673061805727684e-05\n",
      "  batch 301 loss: 3.672138957028892e-05\n",
      "  batch 401 loss: 3.6205200228209836e-05\n",
      "LOSS train 3.693753376333325e-05 valid 6.399783887900412e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 2.5248299352824687e-05\n",
      "  batch 101 loss: 0.04757177687121384\n",
      "  batch 201 loss: 4.244856152809007e-05\n",
      "  batch 301 loss: 7.095725459294044e-05\n",
      "  batch 401 loss: 6.125555786184123e-05\n",
      "LOSS train 0.010786337888062655 valid 4.7097786591621116e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.108744618250057e-07\n",
      "  batch 101 loss: 1.9532657078684678e-05\n",
      "  batch 201 loss: 1.6181147245220018e-05\n",
      "  batch 301 loss: 1.2535824082817727e-05\n",
      "  batch 401 loss: 7.366105729147421e-06\n",
      "LOSS train 1.3152925494035406e-05 valid 5.925788718741387e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.1329339940857608e-07\n",
      "  batch 101 loss: 4.325887581444476e-06\n",
      "  batch 201 loss: 4.2667194782097794e-06\n",
      "  batch 301 loss: 5.750861944591179e-06\n",
      "  batch 401 loss: 4.696781241761983e-06\n",
      "LOSS train 4.745666073747579e-06 valid 6.266449054237455e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.1580453247006516e-07\n",
      "  batch 101 loss: 4.625977968544248e-06\n",
      "  batch 201 loss: 5.3243062504293445e-06\n",
      "  batch 301 loss: 6.469494794885122e-06\n",
      "  batch 401 loss: 4.8616741528917375e-06\n",
      "LOSS train 5.286438663058105e-06 valid 5.893198976991698e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 7.773696779622696e-08\n",
      "  batch 101 loss: 5.217691107475275e-06\n",
      "  batch 201 loss: 6.52498307630367e-06\n",
      "  batch 301 loss: 5.873107120919485e-06\n",
      "  batch 401 loss: 4.786106295284753e-06\n",
      "LOSS train 5.595221147986755e-06 valid 4.8441768740303814e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.096034207483172e-09\n",
      "  batch 101 loss: 5.8981392105295075e-06\n",
      "  batch 201 loss: 6.391286304499033e-06\n",
      "  batch 301 loss: 6.3266206043977035e-06\n",
      "  batch 401 loss: 5.135534798199615e-06\n",
      "LOSS train 5.828803057834098e-06 valid 4.8487927415408194e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.720634241399238e-09\n",
      "  batch 101 loss: 5.6869764983957795e-06\n",
      "  batch 201 loss: 6.008558865744362e-06\n",
      "  batch 301 loss: 5.579526410315339e-06\n",
      "  batch 401 loss: 3.425969215555824e-06\n",
      "LOSS train 4.931075189271506e-06 valid 5.049653191235848e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 4.6670593292219565e-08\n",
      "  batch 101 loss: 4.339519431368899e-06\n",
      "  batch 201 loss: 4.389410204339583e-06\n",
      "  batch 301 loss: 2.9515989801609523e-06\n",
      "  batch 401 loss: 2.8629759871989792e-06\n",
      "LOSS train 3.616649355149401e-06 valid 5.023961057304405e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 4.9289874732494355e-06\n",
      "  batch 101 loss: 2.713568072130438\n",
      "  batch 201 loss: 0.0013846315658884124\n",
      "  batch 301 loss: 0.00039652297622524204\n",
      "  batch 401 loss: 0.0004358022939231887\n",
      "LOSS train 0.6130636441053747 valid 0.0009280563681386411\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.1822203416377305e-05\n",
      "  batch 101 loss: 0.00022660530368739273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 3.807152044828399e-05\n",
      "  batch 301 loss: 0.00012364143941908878\n",
      "  batch 401 loss: 0.0004570087195634187\n",
      "LOSS train 0.00021965102837974868 valid 0.00048164138570427895\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.5557380286045374e-06\n",
      "  batch 101 loss: 0.00016554037119703935\n",
      "  batch 201 loss: 5.744326876993e-05\n",
      "  batch 301 loss: 0.00017930369380678713\n",
      "  batch 401 loss: 0.001953342294909817\n",
      "LOSS train 0.0005569338806288043 valid 0.0013259568950161338\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.68813269212842e-05\n",
      "  batch 101 loss: 0.0008317526069276937\n",
      "  batch 201 loss: 0.00011950586862781165\n",
      "  batch 301 loss: 0.00014718886840000777\n",
      "  batch 401 loss: 0.0009810677595123707\n",
      "LOSS train 0.0007059266131638577 valid 0.0007906716200523078\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.2211976572871208e-05\n",
      "  batch 101 loss: 0.01786712179375172\n",
      "  batch 201 loss: 0.116812201179564\n",
      "  batch 301 loss: 0.01172009497997351\n",
      "  batch 401 loss: 0.1778469452614081\n",
      "LOSS train 0.09010198907306827 valid 0.02989695593714714\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.00014064542949199677\n",
      "  batch 101 loss: 0.053402796022128315\n",
      "  batch 201 loss: 0.007645512721064733\n",
      "  batch 301 loss: 0.012081350417611247\n",
      "  batch 401 loss: 0.07688109032918873\n",
      "LOSS train 0.044696927332773875 valid 0.23694318532943726\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.001393159031867981\n",
      "  batch 101 loss: 0.05313460315373959\n",
      "  batch 201 loss: 0.004153167882977868\n",
      "  batch 301 loss: 0.03836549596693658\n",
      "  batch 401 loss: 0.12391126371425344\n",
      "LOSS train 0.05757657890302669 valid 0.19824516773223877\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0009014328569173813\n",
      "  batch 101 loss: 0.05562507173948689\n",
      "  batch 201 loss: 0.021983054754091427\n",
      "  batch 301 loss: 0.03589675237213669\n",
      "  batch 401 loss: 0.05811555413863971\n",
      "LOSS train 0.05264006372673963 valid 0.03514568507671356\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005458686128258705\n",
      "  batch 101 loss: 0.5927961273562187\n",
      "  batch 201 loss: 7.470029474234252e-05\n",
      "  batch 301 loss: 8.889674636293421e-05\n",
      "  batch 401 loss: 9.70993001556053e-05\n",
      "LOSS train 0.13400450179022905 valid 0.00011784233356593177\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5992840053513647e-06\n",
      "  batch 101 loss: 0.00010461226867050755\n",
      "  batch 201 loss: 0.00010876927078129484\n",
      "  batch 301 loss: 0.00010945880237756\n",
      "  batch 401 loss: 0.00010790425457969377\n",
      "LOSS train 0.00010504249505920834 valid 0.0002170016523450613\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.0953335226513447e-06\n",
      "  batch 101 loss: 0.0001033596498280076\n",
      "  batch 201 loss: 9.889719463899382e-05\n",
      "  batch 301 loss: 9.059129764438012e-05\n",
      "  batch 401 loss: 8.345032875297421e-05\n",
      "LOSS train 9.138273791398464e-05 valid 0.00020350991690065712\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.904198772739619e-06\n",
      "  batch 101 loss: 7.775807708014782e-05\n",
      "  batch 201 loss: 7.323125385710227e-05\n",
      "  batch 301 loss: 6.721581714373314e-05\n",
      "  batch 401 loss: 6.3551219279816e-05\n",
      "LOSS train 6.907684226441108e-05 valid 0.00016645577852614224\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.3637515550944953e-06\n",
      "  batch 101 loss: 6.2857420272735e-05\n",
      "  batch 201 loss: 6.0930075698024666e-05\n",
      "  batch 301 loss: 5.819052435185767e-05\n",
      "  batch 401 loss: 5.749375002949364e-05\n",
      "LOSS train 5.93458466755384e-05 valid 0.00015691656153649092\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.2199194063432516e-06\n",
      "  batch 101 loss: 6.022724087529241e-05\n",
      "  batch 201 loss: 5.9785432217722703e-05\n",
      "  batch 301 loss: 5.859949637624595e-05\n",
      "  batch 401 loss: 5.930616924274545e-05\n",
      "LOSS train 5.950099838167469e-05 valid 0.0001476497418479994\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.0778144244104625e-06\n",
      "  batch 101 loss: 6.124864922128381e-05\n",
      "  batch 201 loss: 5.999083610845446e-05\n",
      "  batch 301 loss: 5.798509719170397e-05\n",
      "  batch 401 loss: 5.7210300827250645e-05\n",
      "LOSS train 5.904067760439865e-05 valid 9.920985758071765e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.2741671525873245e-06\n",
      "  batch 101 loss: 5.303483066086301e-05\n",
      "  batch 201 loss: 5.106563156459743e-05\n",
      "  batch 301 loss: 4.908453364066645e-05\n",
      "  batch 401 loss: 4.81117654544505e-05\n",
      "LOSS train 5.015954453265695e-05 valid 7.470109994756058e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0012883967161178588\n",
      "  batch 101 loss: 0.41750688471539205\n",
      "  batch 201 loss: 5.150049247177435e-05\n",
      "  batch 301 loss: 4.674276682919753e-05\n",
      "  batch 401 loss: 4.9054152684959716e-05\n",
      "LOSS train 0.09457389971378982 valid 6.54309696983546e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.1285237633273937e-07\n",
      "  batch 101 loss: 3.827697850908862e-05\n",
      "  batch 201 loss: 3.649271492463413e-05\n",
      "  batch 301 loss: 3.6158970717963254e-05\n",
      "  batch 401 loss: 3.666858051303734e-05\n",
      "LOSS train 3.7364755267752755e-05 valid 6.332522025331855e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 4.271282159606926e-07\n",
      "  batch 101 loss: 3.9735280271884224e-05\n",
      "  batch 201 loss: 4.221544100090568e-05\n",
      "  batch 301 loss: 4.862183294051192e-05\n",
      "  batch 401 loss: 5.456286602282035e-05\n",
      "LOSS train 4.7323594901126645e-05 valid 0.0001222632417920977\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.6728602349758148e-06\n",
      "  batch 101 loss: 6.206796411049709e-05\n",
      "  batch 201 loss: 6.14600738038007e-05\n",
      "  batch 301 loss: 5.781192264748824e-05\n",
      "  batch 401 loss: 5.3154359821974137e-05\n",
      "LOSS train 5.801867839300358e-05 valid 7.003968494245782e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 6.549895624630153e-07\n",
      "  batch 101 loss: 5.0242809325027336e-05\n",
      "  batch 201 loss: 5.514084448634549e-05\n",
      "  batch 301 loss: 5.747146107893286e-05\n",
      "  batch 401 loss: 6.195735003473147e-05\n",
      "LOSS train 5.6651165045087443e-05 valid 6.757397932233289e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 8.50590458867373e-08\n",
      "  batch 101 loss: 5.281783966211151e-05\n",
      "  batch 201 loss: 5.35791467530089e-05\n",
      "  batch 301 loss: 4.8875182495748956e-05\n",
      "  batch 401 loss: 4.5441293851240515e-05\n",
      "LOSS train 4.9914546291829966e-05 valid 6.20177379460074e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.0952127670170738e-07\n",
      "  batch 101 loss: 4.1799445470189766e-05\n",
      "  batch 201 loss: 4.268716280989793e-05\n",
      "  batch 301 loss: 4.3970991041817344e-05\n",
      "  batch 401 loss: 4.5013784186096475e-05\n",
      "LOSS train 4.351613716023755e-05 valid 6.812496576458216e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.00322637183126e-07\n",
      "  batch 101 loss: 4.383157104768998e-05\n",
      "  batch 201 loss: 4.507365215260961e-05\n",
      "  batch 301 loss: 4.7512760272638846e-05\n",
      "  batch 401 loss: 5.162004984754276e-05\n",
      "LOSS train 4.7651105331664453e-05 valid 6.526582001242787e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0002865655720233917\n",
      "  batch 101 loss: 2.961333857633854\n",
      "  batch 201 loss: 0.00011196276162991126\n",
      "  batch 301 loss: 0.00013093944577747152\n",
      "  batch 401 loss: 1.683239936028258e-05\n",
      "LOSS train 0.6685988956704139 valid 0.00028523735818453133\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.5976448776200415e-06\n",
      "  batch 101 loss: 2.272042314700684e-05\n",
      "  batch 201 loss: 2.2257399366765183e-05\n",
      "  batch 301 loss: 6.977150737341731e-06\n",
      "  batch 401 loss: 1.8703598712477287e-05\n",
      "LOSS train 1.803139189923691e-05 valid 6.2700426497031e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.7598158087348565e-07\n",
      "  batch 101 loss: 4.0937485928225214e-05\n",
      "  batch 201 loss: 0.00032961996467406607\n",
      "  batch 301 loss: 6.507553104711405e-05\n",
      "  batch 401 loss: 7.408840061771116e-05\n",
      "LOSS train 0.00011719167248640021 valid 0.00010651063348632306\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2545928004547023e-07\n",
      "  batch 101 loss: 1.8009943675281194e-05\n",
      "  batch 201 loss: 1.3716809746853187e-05\n",
      "  batch 301 loss: 8.529543557074248e-06\n",
      "  batch 401 loss: 6.551178669553793e-05\n",
      "LOSS train 2.7164913875878295e-05 valid 4.9371210479876027e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.9558732674340718e-07\n",
      "  batch 101 loss: 5.0066815741161006e-05\n",
      "  batch 201 loss: 1.1576049998893723e-05\n",
      "  batch 301 loss: 9.478327556280419e-06\n",
      "  batch 401 loss: 2.85217919332581e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 2.4868761934829188e-05 valid 7.626844308106229e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 6.696018681395799e-07\n",
      "  batch 101 loss: 4.4201487815485054e-05\n",
      "  batch 201 loss: 1.5485209585222036e-05\n",
      "  batch 301 loss: 9.099422741201124e-06\n",
      "  batch 401 loss: 1.0429019212097047e-05\n",
      "LOSS train 1.9211765013746524e-05 valid 0.00010953203309327364\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.934983785962686e-07\n",
      "  batch 101 loss: 1.9398668691792408e-05\n",
      "  batch 201 loss: 1.5760619649540786e-05\n",
      "  batch 301 loss: 9.412846303007427e-06\n",
      "  batch 401 loss: 1.8184269984971026e-05\n",
      "LOSS train 1.5791652426207005e-05 valid 4.676835305872373e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 8.058316780079622e-08\n",
      "  batch 101 loss: 2.7431195333633697e-05\n",
      "  batch 201 loss: 1.2923038654548691e-05\n",
      "  batch 301 loss: 1.0648817446536896e-05\n",
      "  batch 401 loss: 1.574266384892553e-05\n",
      "LOSS train 1.6467116820395286e-05 valid 0.00012130599498050287\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.8744907942600546e-06\n",
      "  batch 101 loss: 46.08946832534857\n",
      "  batch 201 loss: 0.02295787740731612\n",
      "  batch 301 loss: 0.006088576636975631\n",
      "  batch 401 loss: 0.00266157891790499\n",
      "LOSS train 10.411342813908563 valid 0.004792987369000912\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 6.556383334100247e-05\n",
      "  batch 101 loss: 0.0007075724390597316\n",
      "  batch 201 loss: 0.0003627503040024749\n",
      "  batch 301 loss: 0.0001153310194331425\n",
      "  batch 401 loss: 0.000137790134763236\n",
      "LOSS train 0.0003298235661695306 valid 0.0021249025594443083\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.7581242136657238e-05\n",
      "  batch 101 loss: 0.0003504029147666188\n",
      "  batch 201 loss: 0.0004251612502412172\n",
      "  batch 301 loss: 7.871735910612187e-05\n",
      "  batch 401 loss: 0.0005792337949606008\n",
      "LOSS train 0.0004749288714804198 valid 0.0005449464078992605\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.0610858337022363e-06\n",
      "  batch 101 loss: 0.001956401862371422\n",
      "  batch 201 loss: 0.0009947719873980533\n",
      "  batch 301 loss: 8.755752657748417e-05\n",
      "  batch 401 loss: 0.00014923850988793675\n",
      "LOSS train 0.0007755687857079096 valid 0.0013798055006191134\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 9.507190552540124e-06\n",
      "  batch 101 loss: 0.004165186766122133\n",
      "  batch 201 loss: 2.89755557074197\n",
      "  batch 301 loss: 1.6683619699953125\n",
      "  batch 401 loss: 0.11492882382473908\n",
      "LOSS train 1.0671741066065945 valid 0.13375312089920044\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.0005425361171364784\n",
      "  batch 101 loss: 0.16235840278473915\n",
      "  batch 201 loss: 0.978813351476565\n",
      "  batch 301 loss: 0.6151082124328241\n",
      "  batch 401 loss: 1.9145311170897912\n",
      "LOSS train 0.8489779520532958 valid 0.5283994674682617\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.002325999140739441\n",
      "  batch 101 loss: 0.5750860405806452\n",
      "  batch 201 loss: 1.0814387900009752\n",
      "  batch 301 loss: 0.2146201146254316\n",
      "  batch 401 loss: 2.5359055305551736\n",
      "LOSS train 1.0260536197497263 valid 0.24350763857364655\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0006634601205587387\n",
      "  batch 101 loss: 0.27002208842895925\n",
      "  batch 201 loss: 0.22873493848834187\n",
      "  batch 301 loss: 1.3320918905292638\n",
      "  batch 401 loss: 0.973906578021124\n",
      "LOSS train 0.7161842785627449 valid 0.6105364561080933\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00033124007284641267\n",
      "  batch 101 loss: 42.27376639949158\n",
      "  batch 201 loss: 0.0021910179244898132\n",
      "  batch 301 loss: 7.297340425793664e-05\n",
      "  batch 401 loss: 7.615250096478121e-05\n",
      "LOSS train 9.543221491319379 valid 6.782313721487299e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.912548658670857e-07\n",
      "  batch 101 loss: 8.039355619985145e-05\n",
      "  batch 201 loss: 8.322762148964102e-05\n",
      "  batch 301 loss: 8.605770858594042e-05\n",
      "  batch 401 loss: 8.977768139629916e-05\n",
      "LOSS train 8.498595525195772e-05 valid 8.639352017780766e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.0289153578924014e-06\n",
      "  batch 101 loss: 9.416450752269156e-05\n",
      "  batch 201 loss: 9.70622837189694e-05\n",
      "  batch 301 loss: 9.976858189929772e-05\n",
      "  batch 401 loss: 0.00010307667495794704\n",
      "LOSS train 9.766480447039856e-05 valid 0.00012956076534464955\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.7920619575306773e-06\n",
      "  batch 101 loss: 0.000106380857968702\n",
      "  batch 201 loss: 0.00010855703679567341\n",
      "  batch 301 loss: 0.00010909444206561148\n",
      "  batch 401 loss: 0.0001094596701270234\n",
      "LOSS train 0.00010619799295637075 valid 0.00019384817278478295\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.765662793535739e-06\n",
      "  batch 101 loss: 0.00010878924578378246\n",
      "  batch 201 loss: 0.00010797934738434379\n",
      "  batch 301 loss: 0.00010367449891930391\n",
      "  batch 401 loss: 9.935019161275705e-05\n",
      "LOSS train 0.00010212857274807237 valid 0.00022373025421984494\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.189744602423161e-06\n",
      "  batch 101 loss: 9.478543609134249e-05\n",
      "  batch 201 loss: 9.104001153275475e-05\n",
      "  batch 301 loss: 8.422312667562437e-05\n",
      "  batch 401 loss: 7.891026407151002e-05\n",
      "LOSS train 8.504180173027988e-05 valid 0.0001971980818780139\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.8138639754615725e-06\n",
      "  batch 101 loss: 7.530074251974384e-05\n",
      "  batch 201 loss: 7.187794062360809e-05\n",
      "  batch 301 loss: 6.673737308119598e-05\n",
      "  batch 401 loss: 6.361564830285715e-05\n",
      "LOSS train 6.811403334070503e-05 valid 0.00016721450083423406\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.3750949185341596e-06\n",
      "  batch 101 loss: 6.3229062346295e-05\n",
      "  batch 201 loss: 6.138815306030665e-05\n",
      "  batch 301 loss: 5.857428578565305e-05\n",
      "  batch 401 loss: 5.769248406068073e-05\n",
      "LOSS train 5.963106881525176e-05 valid 0.00015719894145149738\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0007881355285644531\n",
      "  batch 101 loss: 3.5469956752170764\n",
      "  batch 201 loss: 0.0017225532968967627\n",
      "  batch 301 loss: 0.00010505342294948151\n",
      "  batch 401 loss: 0.00010894166530334815\n",
      "LOSS train 0.801298740099357 valid 0.00021961798483971506\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.132111742161214e-06\n",
      "  batch 101 loss: 0.00010138014705944442\n",
      "  batch 201 loss: 9.257816453256851e-05\n",
      "  batch 301 loss: 8.080297046490159e-05\n",
      "  batch 401 loss: 7.232578304183334e-05\n",
      "LOSS train 8.420405094514245e-05 valid 0.00018027774058282375\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.568403724581003e-06\n",
      "  batch 101 loss: 6.734934403425541e-05\n",
      "  batch 201 loss: 6.360227788832163e-05\n",
      "  batch 301 loss: 5.945861110717487e-05\n",
      "  batch 401 loss: 5.7870026025170774e-05\n",
      "LOSS train 6.133175154525336e-05 valid 0.0001569904707139358\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.2210433962754906e-06\n",
      "  batch 101 loss: 6.006303126440571e-05\n",
      "  batch 201 loss: 5.9529392572130747e-05\n",
      "  batch 301 loss: 5.833558495965008e-05\n",
      "  batch 401 loss: 5.9108500501565686e-05\n",
      "LOSS train 5.928508848628461e-05 valid 0.00014811864821240306\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.0850678265560417e-06\n",
      "  batch 101 loss: 6.124040562099253e-05\n",
      "  batch 201 loss: 5.985965173977093e-05\n",
      "  batch 301 loss: 5.7688365050694304e-05\n",
      "  batch 401 loss: 5.665682025323804e-05\n",
      "LOSS train 5.875585633448364e-05 valid 9.533964475849643e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2025609612464905e-06\n",
      "  batch 101 loss: 5.197021739377305e-05\n",
      "  batch 201 loss: 4.994315568751517e-05\n",
      "  batch 301 loss: 4.8056721074090095e-05\n",
      "  batch 401 loss: 4.724964594117864e-05\n",
      "LOSS train 4.9157846259258006e-05 valid 7.363926124526188e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 7.482178625650704e-07\n",
      "  batch 101 loss: 4.46758192560992e-05\n",
      "  batch 201 loss: 4.472012842654749e-05\n",
      "  batch 301 loss: 4.545395358093174e-05\n",
      "  batch 401 loss: 4.70603682458659e-05\n",
      "LOSS train 4.566099937542595e-05 valid 6.980996113270521e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.486588245024905e-07\n",
      "  batch 101 loss: 4.635017288990184e-05\n",
      "  batch 201 loss: 4.795839502548915e-05\n",
      "  batch 301 loss: 4.913773256504328e-05\n",
      "  batch 401 loss: 4.834053885531375e-05\n",
      "LOSS train 4.7982738994606376e-05 valid 6.219916394911706e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 5.0093382596969605e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 85891.86369325181\n",
      "  batch 201 loss: 0.5512992471456528\n",
      "  batch 301 loss: 0.4684519079327583\n",
      "  batch 401 loss: 0.38145128220319746\n",
      "LOSS train 19389.02947625466 valid 0.30437031388282776\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.002987316846847534\n",
      "  batch 101 loss: 0.2657986703515053\n",
      "  batch 201 loss: 0.19759531036019326\n",
      "  batch 301 loss: 0.14170437887310983\n",
      "  batch 401 loss: 0.0980952589213848\n",
      "LOSS train 0.1664424307526638 valid 0.0672483891248703\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.0006458085775375367\n",
      "  batch 101 loss: 0.05438256781548262\n",
      "  batch 201 loss: 0.03447965705767274\n",
      "  batch 301 loss: 0.021048620389774442\n",
      "  batch 401 loss: 0.012380391554906965\n",
      "LOSS train 0.028548740300415064 valid 0.007239541504532099\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.344644352793694e-05\n",
      "  batch 101 loss: 0.00539127089548856\n",
      "  batch 201 loss: 0.0028912643948569893\n",
      "  batch 301 loss: 0.0014955052256118507\n",
      "  batch 401 loss: 0.0007576201917254366\n",
      "LOSS train 0.0024405609779916577 valid 0.0003966710064560175\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.7654738621786237e-06\n",
      "  batch 101 loss: 0.00028136743036156987\n",
      "  batch 201 loss: 0.00015561953772476046\n",
      "  batch 301 loss: 9.888784199233668e-05\n",
      "  batch 401 loss: 7.620110676612057e-05\n",
      "LOSS train 0.0001461374323656769 valid 6.684216350549832e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.33812862058403e-08\n",
      "  batch 101 loss: 6.411013650449604e-05\n",
      "  batch 201 loss: 6.283514207098051e-05\n",
      "  batch 301 loss: 6.221050822205143e-05\n",
      "  batch 401 loss: 6.260376553655079e-05\n",
      "LOSS train 6.335475216938138e-05 valid 6.155303708510473e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 2.7574904379434886e-07\n",
      "  batch 101 loss: 6.253536085750966e-05\n",
      "  batch 201 loss: 6.280444453295787e-05\n",
      "  batch 301 loss: 6.265823133617233e-05\n",
      "  batch 401 loss: 6.309860447800019e-05\n",
      "LOSS train 6.324936139228679e-05 valid 6.161107012303546e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.9926392016932366e-07\n",
      "  batch 101 loss: 6.312190468634071e-05\n",
      "  batch 201 loss: 6.332200077849848e-05\n",
      "  batch 301 loss: 6.31710865036439e-05\n",
      "  batch 401 loss: 6.362227484714822e-05\n",
      "LOSS train 6.37792248447238e-05 valid 6.165225204313174e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0018133552372455598\n",
      "  batch 101 loss: 0.10003411160359973\n",
      "  batch 201 loss: 0.0003537663188490114\n",
      "  batch 301 loss: 0.00012710201643585607\n",
      "  batch 401 loss: 0.0001839391288490333\n",
      "LOSS train 0.023152016409008273 valid 0.0003842971345875412\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8998642847000157e-08\n",
      "  batch 101 loss: 2.3340786755170483e-05\n",
      "  batch 201 loss: 0.0001726430462611006\n",
      "  batch 301 loss: 0.00014413490207289216\n",
      "  batch 401 loss: 0.0008935131987072964\n",
      "LOSS train 0.0004041900038000943 valid 0.0008135593379847705\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.921944542322308e-06\n",
      "  batch 101 loss: 0.00268663367631234\n",
      "  batch 201 loss: 0.0008713437511687517\n",
      "  batch 301 loss: 0.0003448641258353291\n",
      "  batch 401 loss: 0.0002799758788228246\n",
      "LOSS train 0.0009624690543348046 valid 0.0010060555068776011\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.5518200816586612e-05\n",
      "  batch 101 loss: 0.0005872258870294899\n",
      "  batch 201 loss: 0.0009132264083746122\n",
      "  batch 301 loss: 0.004684989850711645\n",
      "  batch 401 loss: 0.0024557143329548125\n",
      "LOSS train 0.002265566144070069 valid 0.002726331353187561\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.6803788281977174e-05\n",
      "  batch 101 loss: 0.004672299068552092\n",
      "  batch 201 loss: 0.0032284888915819466\n",
      "  batch 301 loss: 0.0014857815404320717\n",
      "  batch 401 loss: 0.007015243610512698\n",
      "LOSS train 0.004114473055998619 valid 0.0012129152892157435\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 9.228409180650487e-07\n",
      "  batch 101 loss: 0.004216795510646989\n",
      "  batch 201 loss: 0.0019522066717945563\n",
      "  batch 301 loss: 0.0013889130281404506\n",
      "  batch 401 loss: 0.004808445796443266\n",
      "LOSS train 0.0030092081343677276 valid 0.0018067359924316406\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 6.752840126864612e-06\n",
      "  batch 101 loss: 0.0037589787414071905\n",
      "  batch 201 loss: 0.0025552051138947717\n",
      "  batch 301 loss: 0.004528296526286795\n",
      "  batch 401 loss: 0.0016491201438202552\n",
      "LOSS train 0.003059301179068897 valid 0.02951951138675213\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.00038484428077936175\n",
      "  batch 101 loss: 0.01191803908572183\n",
      "  batch 201 loss: 0.0015660459155878925\n",
      "  batch 301 loss: 0.0018267392693815054\n",
      "  batch 401 loss: 0.002617791958145972\n",
      "LOSS train 0.004218030408807717 valid 0.003758178325369954\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.998947329819203e-05\n",
      "  batch 101 loss: 0.005526546212749963\n",
      "  batch 201 loss: 0.0033047261341471314\n",
      "  batch 301 loss: 0.0021773588464293423\n",
      "  batch 401 loss: 0.00737627259844885\n",
      "LOSS train 0.005451046286570815 valid 0.005651191342622042\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.3491889480501415e-05\n",
      "  batch 101 loss: 0.0036072633882577067\n",
      "  batch 201 loss: 0.0018984483759959404\n",
      "  batch 301 loss: 0.003733512291137231\n",
      "  batch 401 loss: 0.0029431778847538227\n",
      "LOSS train 0.0028535256810149883 valid 0.00116264121606946\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.030415391549468e-06\n",
      "  batch 101 loss: 0.0029787029043109215\n",
      "  batch 201 loss: 0.001265695747861173\n",
      "  batch 301 loss: 0.0038726868375670164\n",
      "  batch 401 loss: 0.005081877407101274\n",
      "LOSS train 0.0033991194235543764 valid 0.0021208564285188913\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.4325225492939353e-06\n",
      "  batch 101 loss: 0.0048267777287401255\n",
      "  batch 201 loss: 0.004610848841075495\n",
      "  batch 301 loss: 0.003425769289569871\n",
      "  batch 401 loss: 0.0052123912944443875\n",
      "LOSS train 0.004625099203388039 valid 0.021266339346766472\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 3.3182438928633926e-05\n",
      "  batch 101 loss: 0.024122579734400348\n",
      "  batch 201 loss: 0.00020679017695329093\n",
      "  batch 301 loss: 9.591768911832333e-05\n",
      "  batch 401 loss: 0.00010473681247049172\n",
      "LOSS train 0.005553049457005832 valid 0.00015918503049761057\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.254335704492405e-06\n",
      "  batch 101 loss: 0.00010921330274840101\n",
      "  batch 201 loss: 0.00010957346646847554\n",
      "  batch 301 loss: 0.00010416275428838162\n",
      "  batch 401 loss: 9.70582619987681e-05\n",
      "LOSS train 0.00010179315682319464 valid 0.00022170302690938115\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.161357599310577e-06\n",
      "  batch 101 loss: 8.905601708249833e-05\n",
      "  batch 201 loss: 8.300690584860604e-05\n",
      "  batch 301 loss: 7.49626797073688e-05\n",
      "  batch 401 loss: 6.945073442238937e-05\n",
      "LOSS train 7.721602740028679e-05 valid 0.00017731489788275212\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.524874289520085e-06\n",
      "  batch 101 loss: 6.671970010984297e-05\n",
      "  batch 201 loss: 6.382777859471389e-05\n",
      "  batch 301 loss: 6.003792014439568e-05\n",
      "  batch 401 loss: 5.8418316610868716e-05\n",
      "LOSS train 6.146665707618929e-05 valid 0.00015791269834153354\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.235050342278555e-06\n",
      "  batch 101 loss: 6.015571839469658e-05\n",
      "  batch 201 loss: 5.935585724387238e-05\n",
      "  batch 301 loss: 5.782142572115845e-05\n",
      "  batch 401 loss: 5.8222200430577686e-05\n",
      "LOSS train 5.872757872806016e-05 valid 0.00015527044888585806\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.194857515860349e-06\n",
      "  batch 101 loss: 6.132965554257908e-05\n",
      "  batch 201 loss: 6.0766619966443614e-05\n",
      "  batch 301 loss: 5.9404946813970124e-05\n",
      "  batch 401 loss: 5.969157641800393e-05\n",
      "LOSS train 6.0368411586286383e-05 valid 0.00012638756015803665\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.7405507969669998e-06\n",
      "  batch 101 loss: 5.860012643211121e-05\n",
      "  batch 201 loss: 5.689898046171038e-05\n",
      "  batch 301 loss: 5.477837177977562e-05\n",
      "  batch 401 loss: 5.371624646613782e-05\n",
      "LOSS train 5.586025154670534e-05 valid 8.64209359860979e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.0294694220647217e-06\n",
      "  batch 101 loss: 4.966281292468011e-05\n",
      "  batch 201 loss: 4.848042542960229e-05\n",
      "  batch 301 loss: 4.730734955899152e-05\n",
      "  batch 401 loss: 4.698849988926668e-05\n",
      "LOSS train 4.803699495756328e-05 valid 7.380713213933632e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 7.523403473896905e-07\n",
      "  batch 101 loss: 4.476171861682588e-05\n",
      "  batch 201 loss: 4.476263908486544e-05\n",
      "  batch 301 loss: 4.5170599609036796e-05\n",
      "  batch 401 loss: 4.6274331882329986e-05\n",
      "LOSS train 4.5358505573584484e-05 valid 7.164878479670733e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 6.979211320867762e-07\n",
      "  batch 101 loss: 4.538112425052532e-05\n",
      "  batch 201 loss: 4.6403747044791996e-05\n",
      "  batch 301 loss: 4.775407633133e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 401 loss: 4.906824096735818e-05\n",
      "LOSS train 4.730711033538411e-05 valid 6.600574124604464e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 5.330640851752833e-07\n",
      "  batch 101 loss: 4.6832771537026475e-05\n",
      "  batch 201 loss: 4.829083518472998e-05\n",
      "  batch 301 loss: 4.900351536463177e-05\n",
      "  batch 401 loss: 4.8650406863117724e-05\n",
      "LOSS train 4.8222280970098655e-05 valid 6.155604205559939e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 2.6838795747607946e-07\n",
      "  batch 101 loss: 4.4460508017181156e-05\n",
      "  batch 201 loss: 4.5750511758626545e-05\n",
      "  batch 301 loss: 4.538771353480797e-05\n",
      "  batch 401 loss: 4.3960645975005264e-05\n",
      "LOSS train 4.492041701207582e-05 valid 6.446228508139029e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.000415131039917469\n",
      "  batch 101 loss: 0.032988357532121884\n",
      "  batch 201 loss: 6.51529945389484e-05\n",
      "  batch 301 loss: 3.183032619176629e-05\n",
      "  batch 401 loss: 1.0459680420353833e-05\n",
      "LOSS train 0.007565064768632751 valid 4.6028293581912294e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.3758053581987045e-08\n",
      "  batch 101 loss: 4.8211195462499745e-06\n",
      "  batch 201 loss: 5.8620786546725865e-06\n",
      "  batch 301 loss: 4.9606478256691846e-06\n",
      "  batch 401 loss: 5.513399624703652e-06\n",
      "LOSS train 5.220632183149984e-06 valid 5.086668170406483e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.679273959278362e-08\n",
      "  batch 101 loss: 6.2704699786308994e-06\n",
      "  batch 201 loss: 4.974019225869597e-06\n",
      "  batch 301 loss: 4.41441650878005e-06\n",
      "  batch 401 loss: 4.545304053067412e-06\n",
      "LOSS train 5.061863525844719e-06 valid 6.218171620275825e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.4441100322292185e-07\n",
      "  batch 101 loss: 8.120050017055292e-06\n",
      "  batch 201 loss: 8.393624090103913e-06\n",
      "  batch 301 loss: 8.224465982493712e-06\n",
      "  batch 401 loss: 5.102492899027311e-06\n",
      "LOSS train 7.442607083819475e-06 valid 5.02566181239672e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.574738348608662e-08\n",
      "  batch 101 loss: 1.1768789931068114e-05\n",
      "  batch 201 loss: 2.443754882392568e-05\n",
      "  batch 301 loss: 2.9848007010286892e-05\n",
      "  batch 401 loss: 2.977964059482474e-05\n",
      "LOSS train 2.430894530549425e-05 valid 5.1572937081800774e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 5.823308129038196e-08\n",
      "  batch 101 loss: 1.7611267550421418e-05\n",
      "  batch 201 loss: 3.279357478334077e-05\n",
      "  batch 301 loss: 2.8708306356577395e-05\n",
      "  batch 401 loss: 8.831773985775726e-06\n",
      "LOSS train 2.0798894610344747e-05 valid 5.184169640415348e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.4327890716667754e-08\n",
      "  batch 101 loss: 8.474488402185897e-06\n",
      "  batch 201 loss: 9.45518574894777e-06\n",
      "  batch 301 loss: 8.545871386900216e-06\n",
      "  batch 401 loss: 1.4110885952618446e-05\n",
      "LOSS train 1.0433518933275643e-05 valid 0.00015516221174038947\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 9.977599984267726e-07\n",
      "  batch 101 loss: 1.8964465026556355e-05\n",
      "  batch 201 loss: 9.68352045958909e-06\n",
      "  batch 301 loss: 1.2661337606800771e-05\n",
      "  batch 401 loss: 1.4808131618337939e-05\n",
      "LOSS train 1.431112984918406e-05 valid 5.111319478601217e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.082499233642011e-08\n",
      "  batch 101 loss: 6.013537657167944e-06\n",
      "  batch 201 loss: 1.0985227698938615e-05\n",
      "  batch 301 loss: 1.0569852103898825e-05\n",
      "  batch 401 loss: 5.974775011310385e-06\n",
      "LOSS train 8.000077078420377e-06 valid 5.118780245538801e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 9.158451916846389e-09\n",
      "  batch 101 loss: 5.502314974137335e-06\n",
      "  batch 201 loss: 5.438383985705286e-06\n",
      "  batch 301 loss: 3.8445816979049145e-06\n",
      "  batch 401 loss: 4.63937256093061e-06\n",
      "LOSS train 4.877743946070608e-06 valid 5.3005242079962045e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.6648256860207765e-07\n",
      "  batch 101 loss: 7.436766927142457e-06\n",
      "  batch 201 loss: 5.828855509975029e-06\n",
      "  batch 301 loss: 6.589309265621068e-06\n",
      "  batch 401 loss: 4.461078511610595e-06\n",
      "LOSS train 5.820465530919429e-06 valid 5.7020959502551705e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 7.171606739575509e-09\n",
      "  batch 101 loss: 3.1238874368000324e-06\n",
      "  batch 201 loss: 1.0980007580485562e-05\n",
      "  batch 301 loss: 1.8076331866865302e-05\n",
      "  batch 401 loss: 1.216507128901867e-05\n",
      "LOSS train 1.1552980621441535e-05 valid 4.280764551367611e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00013814846985042096\n",
      "  batch 101 loss: 0.019789853125571424\n",
      "  batch 201 loss: 4.327293130302223e-05\n",
      "  batch 301 loss: 4.9127336623087104e-05\n",
      "  batch 401 loss: 3.135832618340828e-05\n",
      "LOSS train 0.004528453097287134 valid 5.4460877436213195e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.3044013940088917e-08\n",
      "  batch 101 loss: 1.9941459119081628e-05\n",
      "  batch 201 loss: 1.5347234086675598e-05\n",
      "  batch 301 loss: 9.371022437107967e-06\n",
      "  batch 401 loss: 5.477233687400939e-06\n",
      "LOSS train 1.1683000562793208e-05 valid 5.0129070586990565e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.928332494571805e-08\n",
      "  batch 101 loss: 5.44386919898443e-06\n",
      "  batch 201 loss: 6.858740658230999e-06\n",
      "  batch 301 loss: 7.391825779876626e-06\n",
      "  batch 401 loss: 5.345528991682613e-06\n",
      "LOSS train 6.1001882785779345e-06 valid 4.7809913667151704e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.065763278049417e-08\n",
      "  batch 101 loss: 5.798222768760297e-06\n",
      "  batch 201 loss: 6.425290786467031e-06\n",
      "  batch 301 loss: 5.341700985752595e-06\n",
      "  batch 401 loss: 4.152575614853049e-06\n",
      "LOSS train 5.388612101137472e-06 valid 4.772049214807339e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 9.794956667974476e-09\n",
      "  batch 101 loss: 6.835138491112502e-06\n",
      "  batch 201 loss: 9.0141800608734e-06\n",
      "  batch 301 loss: 5.983547212338181e-06\n",
      "  batch 401 loss: 4.0800907146376634e-06\n",
      "LOSS train 6.386782534424579e-06 valid 4.802969124284573e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.2504098094723305e-08\n",
      "  batch 101 loss: 8.931779232739245e-06\n",
      "  batch 201 loss: 6.4786300444552584e-06\n",
      "  batch 301 loss: 2.118683385106124e-06\n",
      "  batch 401 loss: 3.5531205789141042e-06\n",
      "LOSS train 5.405302308662013e-06 valid 7.30634928913787e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.0604158887872473e-07\n",
      "  batch 101 loss: 6.778241830858178e-06\n",
      "  batch 201 loss: 4.0041075436647585e-06\n",
      "  batch 301 loss: 2.0267320678613033e-06\n",
      "  batch 401 loss: 2.7501997647050303e-06\n",
      "LOSS train 3.932035729482272e-06 valid 5.281508856569417e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 6.267371190915582e-09\n",
      "  batch 101 loss: 6.826280657321604e-06\n",
      "  batch 201 loss: 4.148790426228288e-06\n",
      "  batch 301 loss: 2.052056796770785e-06\n",
      "  batch 401 loss: 2.3302840735084375e-06\n",
      "LOSS train 3.865842744094912e-06 valid 6.169525295263156e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.418551048322115e-08\n",
      "  batch 101 loss: 6.551059245367696e-06\n",
      "  batch 201 loss: 3.8939614694299965e-06\n",
      "  batch 301 loss: 2.131272111114413e-06\n",
      "  batch 401 loss: 2.824600844917313e-06\n",
      "LOSS train 3.963495657995565e-06 valid 7.516243931604549e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 7.474580797861563e-08\n",
      "  batch 101 loss: 7.130980698377698e-06\n",
      "  batch 201 loss: 4.145958718595466e-06\n",
      "  batch 301 loss: 2.719999719076327e-06\n",
      "  batch 401 loss: 2.735319161217831e-06\n",
      "LOSS train 4.201423236877464e-06 valid 0.0004163281118962914\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.494177606422454e-06\n",
      "  batch 101 loss: 4.4277536009644794e-05\n",
      "  batch 201 loss: 4.1940203520596245e-06\n",
      "  batch 301 loss: 4.361754039763355e-06\n",
      "  batch 401 loss: 4.645946560515313e-06\n",
      "LOSS train 1.3665181218770164e-05 valid 5.091932325740345e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.122333848499693e-08\n",
      "  batch 101 loss: 1.4108674112378595e-06\n",
      "  batch 201 loss: 2.7896888404654873e-06\n",
      "  batch 301 loss: 6.616891830475425e-06\n",
      "  batch 401 loss: 8.652439896223996e-06\n",
      "LOSS train 6.548963347496708e-06 valid 6.2316503317561e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00011328750289976597\n",
      "  batch 101 loss: 2.802706332432572\n",
      "  batch 201 loss: 0.0014317402851884253\n",
      "  batch 301 loss: 0.00044348852959956274\n",
      "  batch 401 loss: 0.00027053246869400025\n",
      "LOSS train 0.6331828178421076 valid 0.0007003687205724418\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 3.043654724024236e-06\n",
      "  batch 101 loss: 0.00010870142126805149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 9.637970977564691e-05\n",
      "  batch 301 loss: 0.00012255521784368284\n",
      "  batch 401 loss: 0.00010209320199578543\n",
      "LOSS train 0.00010313126434302174 valid 0.0006259115762077272\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 3.243551473133266e-06\n",
      "  batch 101 loss: 0.00010875933905253988\n",
      "  batch 201 loss: 7.600264835787129e-05\n",
      "  batch 301 loss: 0.00017010434688700116\n",
      "  batch 401 loss: 0.0012912975934068526\n",
      "LOSS train 0.0004740850495964523 valid 0.0012272900203242898\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.0843286290764808e-05\n",
      "  batch 101 loss: 0.002008457598531095\n",
      "  batch 201 loss: 0.0006309439550295792\n",
      "  batch 301 loss: 0.0001574600755282063\n",
      "  batch 401 loss: 8.99953977750556e-05\n",
      "LOSS train 0.0006616636024395199 valid 0.00044425606029108167\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.4560125418938696e-06\n",
      "  batch 101 loss: 0.0021493020378397887\n",
      "  batch 201 loss: 0.2889185733496561\n",
      "  batch 301 loss: 0.03340547026862623\n",
      "  batch 401 loss: 0.0025584542282376787\n",
      "LOSS train 0.07404342714703716 valid 0.00866194162517786\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.766422625631094e-05\n",
      "  batch 101 loss: 0.003743245722089341\n",
      "  batch 201 loss: 0.03285214256502513\n",
      "  batch 301 loss: 0.05472226190729998\n",
      "  batch 401 loss: 0.12779245439334772\n",
      "LOSS train 0.05103203185323803 valid 0.01636335626244545\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.8379384204745295e-05\n",
      "  batch 101 loss: 0.03439239366998663\n",
      "  batch 201 loss: 0.05012227298808284\n",
      "  batch 301 loss: 0.004450894937108387\n",
      "  batch 401 loss: 0.30209214899863585\n",
      "LOSS train 0.09257580923083994 valid 0.08980388194322586\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0007460298389196395\n",
      "  batch 101 loss: 0.02117881772282999\n",
      "  batch 201 loss: 0.0032667259955087504\n",
      "  batch 301 loss: 0.01601927197623809\n",
      "  batch 401 loss: 0.173979164439952\n",
      "LOSS train 0.054784349318307037 valid 0.0342196486890316\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.000113737927749753\n",
      "  batch 101 loss: 0.013855184425337938\n",
      "  batch 201 loss: 0.0023072781936934916\n",
      "  batch 301 loss: 0.017069130743620917\n",
      "  batch 401 loss: 0.2356004524533637\n",
      "LOSS train 0.06483040034882191 valid 0.0350252240896225\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.00012487313710153104\n",
      "  batch 101 loss: 0.020067092031240463\n",
      "  batch 201 loss: 0.003154500186610676\n",
      "  batch 301 loss: 0.01844979838369909\n",
      "  batch 401 loss: 0.19230146416375646\n",
      "LOSS train 0.055629709719942515 valid 0.05309423804283142\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.0008998798578977585\n",
      "  batch 101 loss: 0.03077077142232156\n",
      "  batch 201 loss: 0.0019941482598005676\n",
      "  batch 301 loss: 0.029829632743931144\n",
      "  batch 401 loss: 0.2547593578521628\n",
      "LOSS train 0.07786459770408159 valid 0.05073971301317215\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.7999877454712986e-05\n",
      "  batch 101 loss: 0.03631325622380246\n",
      "  batch 201 loss: 0.012654234582441859\n",
      "  batch 301 loss: 0.022226370452553965\n",
      "  batch 401 loss: 0.18842435896745882\n",
      "LOSS train 0.06266880804884645 valid 0.048557814210653305\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00011652648448944092\n",
      "  batch 101 loss: 0.27479281604648575\n",
      "  batch 201 loss: 8.934636126923578e-05\n",
      "  batch 301 loss: 0.0001028650158423261\n",
      "  batch 401 loss: 0.00010898223960680298\n",
      "LOSS train 0.062132152691881014 valid 0.00020111662161070853\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 2.8700195252895354e-06\n",
      "  batch 101 loss: 0.00010681279674145117\n",
      "  batch 201 loss: 0.00010113555187047041\n",
      "  batch 301 loss: 9.02836628316095e-05\n",
      "  batch 401 loss: 8.087228944532399e-05\n",
      "LOSS train 9.177578598348107e-05 valid 0.00019634021737147123\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.801537630148232e-06\n",
      "  batch 101 loss: 7.375520644245626e-05\n",
      "  batch 201 loss: 6.873402349242496e-05\n",
      "  batch 301 loss: 6.304119583774082e-05\n",
      "  batch 401 loss: 6.006761803575955e-05\n",
      "LOSS train 6.528911797775658e-05 valid 0.00015988013183232397\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 2.264853101223707e-06\n",
      "  batch 101 loss: 6.064447461781697e-05\n",
      "  batch 201 loss: 5.943326217618505e-05\n",
      "  batch 301 loss: 5.761498204947202e-05\n",
      "  batch 401 loss: 5.7881612615346964e-05\n",
      "LOSS train 5.868511506236082e-05 valid 0.000155676098074764\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.20104047912173e-06\n",
      "  batch 101 loss: 6.121308453373331e-05\n",
      "  batch 201 loss: 6.0690308819459917e-05\n",
      "  batch 301 loss: 5.930826504254583e-05\n",
      "  batch 401 loss: 5.9481988083973645e-05\n",
      "LOSS train 6.0243817463272596e-05 valid 0.00012039535795338452\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 1.6419078747276217e-06\n",
      "  batch 101 loss: 5.738064402237342e-05\n",
      "  batch 201 loss: 5.5108399176333476e-05\n",
      "  batch 301 loss: 5.257243760752317e-05\n",
      "  batch 401 loss: 5.105567535338196e-05\n",
      "LOSS train 5.38090207739287e-05 valid 7.890338019933552e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 8.708421955816448e-07\n",
      "  batch 101 loss: 4.68560008155805e-05\n",
      "  batch 201 loss: 4.591958887999681e-05\n",
      "  batch 301 loss: 4.5388498671741216e-05\n",
      "  batch 401 loss: 4.586586619780064e-05\n",
      "LOSS train 4.603381898722542e-05 valid 7.211699994513765e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 7.10004533175379e-07\n",
      "  batch 101 loss: 4.481540253266303e-05\n",
      "  batch 201 loss: 4.5822180343861877e-05\n",
      "  batch 301 loss: 4.749141703712212e-05\n",
      "  batch 401 loss: 4.9179248085238216e-05\n",
      "LOSS train 4.705034974078945e-05 valid 6.418193515855819e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.6517332521034403e-07\n",
      "  batch 101 loss: 4.648940963477344e-05\n",
      "  batch 201 loss: 4.7478464231289764e-05\n",
      "  batch 301 loss: 4.651213872165272e-05\n",
      "  batch 401 loss: 4.3724547348347185e-05\n",
      "LOSS train 4.5955026625327305e-05 valid 6.54994510114193e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.1177853593835607e-07\n",
      "  batch 101 loss: 3.873936790796506e-05\n",
      "  batch 201 loss: 3.846764947184056e-05\n",
      "  batch 301 loss: 3.751122938865592e-05\n",
      "  batch 401 loss: 3.6389040513427066e-05\n",
      "LOSS train 3.804511577176009e-05 valid 6.38053534203209e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.4419057151826563e-07\n",
      "  batch 101 loss: 3.5743703442818745e-05\n",
      "  batch 201 loss: 3.565040356519944e-05\n",
      "  batch 301 loss: 3.633628679381218e-05\n",
      "  batch 401 loss: 3.679687204169113e-05\n",
      "LOSS train 3.663938924413338e-05 valid 6.307403964456171e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.146670107729733e-07\n",
      "  batch 101 loss: 3.859777614195536e-05\n",
      "  batch 201 loss: 3.909257818236256e-05\n",
      "  batch 301 loss: 4.1900940671553145e-05\n",
      "  batch 401 loss: 4.4393117015602e-05\n",
      "LOSS train 4.169163410080418e-05 valid 8.992944640340284e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00018391069024801255\n",
      "  batch 101 loss: 0.25735187264316667\n",
      "  batch 201 loss: 0.0010605009603523285\n",
      "  batch 301 loss: 0.0024159770089318047\n",
      "  batch 401 loss: 2.722196526406151e-05\n",
      "LOSS train 0.058927323367953026 valid 4.7935001930454746e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.0579567970125936e-07\n",
      "  batch 101 loss: 1.6007323563371755e-05\n",
      "  batch 201 loss: 8.780024536463316e-06\n",
      "  batch 301 loss: 4.791192163793312e-06\n",
      "  batch 401 loss: 3.3462161110264786e-06\n",
      "LOSS train 7.72213465746609e-06 valid 4.799257294507697e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 1.1933630958083086e-07\n",
      "  batch 101 loss: 2.8077558457084705e-06\n",
      "  batch 201 loss: 2.720908197488825e-06\n",
      "  batch 301 loss: 3.600641115895087e-06\n",
      "  batch 401 loss: 3.876742203203775e-06\n",
      "LOSS train 3.2867557319726243e-06 valid 5.256668373476714e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.6453410353278742e-07\n",
      "  batch 101 loss: 3.6141923098398366e-06\n",
      "  batch 201 loss: 4.334075079839295e-06\n",
      "  batch 301 loss: 6.071895050467902e-06\n",
      "  batch 401 loss: 6.105770153794765e-06\n",
      "LOSS train 5.069418770958476e-06 valid 4.867296956945211e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.993105570363696e-08\n",
      "  batch 101 loss: 4.264270639282586e-06\n",
      "  batch 201 loss: 4.2370038877947995e-06\n",
      "  batch 301 loss: 4.663860876803483e-06\n",
      "  batch 401 loss: 4.7391808891461554e-06\n",
      "LOSS train 4.430312100484245e-06 valid 4.961457307217643e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 4.257850832800614e-08\n",
      "  batch 101 loss: 4.5742705475504405e-06\n",
      "  batch 201 loss: 4.069693501378424e-06\n",
      "  batch 301 loss: 3.5976037182194885e-06\n",
      "  batch 401 loss: 3.550969743173482e-06\n",
      "LOSS train 3.949495042161597e-06 valid 5.4512638598680496e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.4907176591805184e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 101 loss: 5.494172249882467e-06\n",
      "  batch 201 loss: 5.155620009986706e-06\n",
      "  batch 301 loss: 6.553394239006138e-06\n",
      "  batch 401 loss: 6.475903609555189e-06\n",
      "LOSS train 5.910139967556223e-06 valid 4.77788235002663e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.055725417449139e-08\n",
      "  batch 101 loss: 5.058066015806162e-06\n",
      "  batch 201 loss: 8.239438552095635e-06\n",
      "  batch 301 loss: 8.973240646810154e-06\n",
      "  batch 401 loss: 5.695125104239196e-06\n",
      "LOSS train 7.123384385610024e-06 valid 5.507864261744544e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 4.607603386830306e-08\n",
      "  batch 101 loss: 9.027460635593342e-06\n",
      "  batch 201 loss: 1.511575421886846e-05\n",
      "  batch 301 loss: 1.5114172830976713e-05\n",
      "  batch 401 loss: 6.117890155223904e-06\n",
      "LOSS train 1.175351694842886e-05 valid 4.6175875468179584e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 9.57798192757764e-09\n",
      "  batch 101 loss: 1.5133134172629071e-05\n",
      "  batch 201 loss: 1.2997901511653254e-05\n",
      "  batch 301 loss: 9.795881831564657e-06\n",
      "  batch 401 loss: 5.617684852836646e-06\n",
      "LOSS train 1.04821626010702e-05 valid 4.93748375447467e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.1329093467793427e-08\n",
      "  batch 101 loss: 6.360810943988326e-06\n",
      "  batch 201 loss: 1.0875612612721853e-05\n",
      "  batch 301 loss: 1.0493200903169964e-05\n",
      "  batch 401 loss: 4.881695208780457e-06\n",
      "LOSS train 7.784584566371126e-06 valid 5.312356006470509e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.0222738637821749e-07\n",
      "  batch 101 loss: 4.686294336124774e-06\n",
      "  batch 201 loss: 5.660054785039392e-06\n",
      "  batch 301 loss: 6.915376090148584e-06\n",
      "  batch 401 loss: 4.316194577853594e-06\n",
      "LOSS train 5.302542757481278e-06 valid 6.364436558214948e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 7.166875526309014e-05\n",
      "  batch 101 loss: 5.8988818545194865\n",
      "  batch 201 loss: 0.0005208063124337059\n",
      "  batch 301 loss: 2.045552829542885e-05\n",
      "  batch 401 loss: 1.597764962468773e-05\n",
      "LOSS train 1.3317186046041152 valid 7.030337292235345e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 1.4482374353974593e-07\n",
      "  batch 101 loss: 6.163499836873143e-06\n",
      "  batch 201 loss: 7.354744991232564e-06\n",
      "  batch 301 loss: 6.342015693689973e-06\n",
      "  batch 401 loss: 7.056153204985094e-06\n",
      "LOSS train 6.932495151430768e-06 valid 0.00014244105841498822\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.045805589063094e-07\n",
      "  batch 101 loss: 1.4392631787529808e-05\n",
      "  batch 201 loss: 1.051846565417236e-05\n",
      "  batch 301 loss: 1.200490638524343e-05\n",
      "  batch 401 loss: 1.5153191025092383e-05\n",
      "LOSS train 1.3199794927508425e-05 valid 0.00013320319703780115\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 6.96007555234246e-07\n",
      "  batch 101 loss: 2.1517522112048936e-05\n",
      "  batch 201 loss: 3.4141818098873954e-05\n",
      "  batch 301 loss: 3.701409196651184e-05\n",
      "  batch 401 loss: 5.062696682955448e-05\n",
      "LOSS train 3.4937342951954644e-05 valid 8.031565084820613e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 3.338863461976871e-07\n",
      "  batch 101 loss: 3.7478047207741835e-05\n",
      "  batch 201 loss: 3.521180657173772e-05\n",
      "  batch 301 loss: 2.9568817387257694e-05\n",
      "  batch 401 loss: 4.025870710449908e-05\n",
      "LOSS train 3.4491447855686495e-05 valid 8.011083991732448e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.0291554392315447e-07\n",
      "  batch 101 loss: 4.572065067833364e-05\n",
      "  batch 201 loss: 2.780361750353677e-05\n",
      "  batch 301 loss: 1.2047279686271394e-05\n",
      "  batch 401 loss: 2.7865461424312342e-05\n",
      "LOSS train 2.862544785858501e-05 valid 8.721677295397967e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 1.6126392438309267e-07\n",
      "  batch 101 loss: 0.001313161127200999\n",
      "  batch 201 loss: 4.4467352128094715e-05\n",
      "  batch 301 loss: 4.927023249592821e-05\n",
      "  batch 401 loss: 5.1452281298338675e-05\n",
      "LOSS train 0.0003342239073258595 valid 0.00010879835463128984\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 1.4449305308517068e-06\n",
      "  batch 101 loss: 5.643387385219967e-05\n",
      "  batch 201 loss: 5.6151049131614174e-05\n",
      "  batch 301 loss: 5.9453104544786587e-05\n",
      "  batch 401 loss: 5.908261959518768e-05\n",
      "LOSS train 5.7973793541351745e-05 valid 0.00011942569835809991\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 1.6257638344541192e-06\n",
      "  batch 101 loss: 6.0966511985327545e-05\n",
      "  batch 201 loss: 5.911793858345504e-05\n",
      "  batch 301 loss: 6.063578751323462e-05\n",
      "  batch 401 loss: 5.880700790442006e-05\n",
      "LOSS train 5.983290783759323e-05 valid 0.00011772623838623986\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 1.597337395651266e-06\n",
      "  batch 101 loss: 5.942700806770063e-05\n",
      "  batch 201 loss: 5.6649354941811225e-05\n",
      "  batch 301 loss: 5.76028794296235e-05\n",
      "  batch 401 loss: 5.574028164389233e-05\n",
      "LOSS train 5.72918574961781e-05 valid 0.00011169861681992188\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.4950468903407454e-06\n",
      "  batch 101 loss: 5.6154594330593e-05\n",
      "  batch 201 loss: 5.344312630583659e-05\n",
      "  batch 301 loss: 5.395445199383175e-05\n",
      "  batch 401 loss: 5.289591153314177e-05\n",
      "LOSS train 5.407110962629616e-05 valid 0.00010338851279811934\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.349635422229767e-06\n",
      "  batch 101 loss: 5.261465926395203e-05\n",
      "  batch 201 loss: 5.0366665690546596e-05\n",
      "  batch 301 loss: 4.8415559435852626e-05\n",
      "  batch 401 loss: 4.589185685638597e-05\n",
      "LOSS train 4.917490067682616e-05 valid 8.662467735121027e-05\n",
      "ObjectiveEstimator_ANN_Single_layer(\n",
      "  (output_layer): Linear(in_features=1227, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      ") 0 0\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00012305391021072865\n",
      "  batch 101 loss: 45.79574585102498\n",
      "  batch 201 loss: 0.02390516092069447\n",
      "  batch 301 loss: 0.006241148997214623\n",
      "  batch 401 loss: 0.002527875076630153\n",
      "LOSS train 10.345250199770438 valid 0.0017358660697937012\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 7.3900900315493345e-06\n",
      "  batch 101 loss: 0.0005863935895104078\n",
      "  batch 201 loss: 0.0002669651619999058\n",
      "  batch 301 loss: 0.00012597278030625602\n",
      "  batch 401 loss: 9.063983921350882e-05\n",
      "LOSS train 0.00024847242882099533 valid 0.00033071383950300515\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 2.9507270664907994e-06\n",
      "  batch 101 loss: 0.000195126507162513\n",
      "  batch 201 loss: 0.00022476705575172673\n",
      "  batch 301 loss: 0.00014757273273630745\n",
      "  batch 401 loss: 0.00010861949243917479\n",
      "LOSS train 0.00019053918044133365 valid 0.0003491597599349916\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 4.227679164614529e-06\n",
      "  batch 101 loss: 0.000578811322616275\n",
      "  batch 201 loss: 0.00016157347813532396\n",
      "  batch 301 loss: 8.223306758054606e-05\n",
      "  batch 401 loss: 0.00016194224776199917\n",
      "LOSS train 0.00024487371352224893 valid 0.00029863856616429985\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 8.250848040916025e-06\n",
      "  batch 101 loss: 0.010349791921726137\n",
      "  batch 201 loss: 3.3831977975761403\n",
      "  batch 301 loss: 1.2800284790899605\n",
      "  batch 401 loss: 0.11290370903210714\n",
      "LOSS train 1.0931643196647292 valid 0.13742920756340027\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 0.00032842818647623064\n",
      "  batch 101 loss: 0.19326201943447813\n",
      "  batch 201 loss: 0.9639739453315269\n",
      "  batch 301 loss: 0.6789473070530221\n",
      "  batch 401 loss: 1.698653706535697\n",
      "LOSS train 0.8142433923190359 valid 0.47361645102500916\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 0.002005230635404587\n",
      "  batch 101 loss: 0.5966483744978904\n",
      "  batch 201 loss: 1.179727726627607\n",
      "  batch 301 loss: 0.38068152602761984\n",
      "  batch 401 loss: 1.7892906241863966\n",
      "LOSS train 0.9310766777184499 valid 0.11398077011108398\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 0.0006406785547733307\n",
      "  batch 101 loss: 0.10080892715370282\n",
      "  batch 201 loss: 1.12411448426079\n",
      "  batch 301 loss: 0.25892088244669137\n",
      "  batch 401 loss: 1.5712931347684935\n",
      "LOSS train 0.7566288383224291 valid 1.6380689144134521\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 0.011841751337051391\n",
      "  batch 101 loss: 0.7161679470923263\n",
      "  batch 201 loss: 0.663965608458966\n",
      "  batch 301 loss: 0.6805928004253655\n",
      "  batch 401 loss: 2.0465050173376222\n",
      "LOSS train 1.0293078920189038 valid 1.6308425664901733\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 0.0025323486328125\n",
      "  batch 101 loss: 0.893682338828221\n",
      "  batch 201 loss: 1.5689573606848717\n",
      "  batch 301 loss: 0.6373734821379184\n",
      "  batch 401 loss: 1.227071729968302\n",
      "LOSS train 1.021070597114138 valid 0.6445361971855164\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 0.0002660904265940189\n",
      "  batch 101 loss: 0.8058927660668269\n",
      "  batch 201 loss: 0.17012746921624056\n",
      "  batch 301 loss: 0.37854595747659914\n",
      "  batch 401 loss: 1.818078354606405\n",
      "LOSS train 0.8046476949877726 valid 0.4700021743774414\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 0.0026861608028411867\n",
      "  batch 101 loss: 1.1454332019248978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 201 loss: 0.14576332101918524\n",
      "  batch 301 loss: 0.9362481112417299\n",
      "  batch 401 loss: 1.539442822923884\n",
      "LOSS train 0.861587531193941 valid 0.5839481949806213\n",
      "ObjectiveEstimator_ANN_1hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=35, out_features=1, bias=True)\n",
      ") 0 1\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 1.980020198971033e-05\n",
      "  batch 101 loss: 72.59806510257926\n",
      "  batch 201 loss: 0.006413718519506801\n",
      "  batch 301 loss: 7.941078256408218e-05\n",
      "  batch 401 loss: 7.209772654277913e-05\n",
      "LOSS train 16.389318799362925 valid 6.459046562667936e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 4.815955617232249e-07\n",
      "  batch 101 loss: 7.512435715852916e-05\n",
      "  batch 201 loss: 7.710052531365364e-05\n",
      "  batch 301 loss: 7.904448512817908e-05\n",
      "  batch 401 loss: 8.184150081888219e-05\n",
      "LOSS train 7.863427755197155e-05 valid 7.260815618792549e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 7.22504046279937e-07\n",
      "  batch 101 loss: 8.51895705136485e-05\n",
      "  batch 201 loss: 8.735545527542854e-05\n",
      "  batch 301 loss: 8.95942189708876e-05\n",
      "  batch 401 loss: 9.275124610894637e-05\n",
      "LOSS train 8.859930310095263e-05 valid 9.18207733775489e-05\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.1357738549122587e-06\n",
      "  batch 101 loss: 9.645457182386962e-05\n",
      "  batch 201 loss: 9.88517779637732e-05\n",
      "  batch 301 loss: 0.00010103416588663094\n",
      "  batch 401 loss: 0.0001038687033496899\n",
      "LOSS train 9.907492985890656e-05 valid 0.0001321009622188285\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 1.832968118833378e-06\n",
      "  batch 101 loss: 0.0001067389711352007\n",
      "  batch 201 loss: 0.00010866284137250659\n",
      "  batch 301 loss: 0.00010906171022156741\n",
      "  batch 401 loss: 0.00010947729558324681\n",
      "LOSS train 0.00010635013692365449 valid 0.000191014027222991\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 2.7247355319559573e-06\n",
      "  batch 101 loss: 0.00010908823633712928\n",
      "  batch 201 loss: 0.00010860106791028556\n",
      "  batch 301 loss: 0.00010481450282156856\n",
      "  batch 401 loss: 0.00010102734297390726\n",
      "LOSS train 0.00010309843549378048 valid 0.00022376423294190317\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.190217830706388e-06\n",
      "  batch 101 loss: 9.693836080387541e-05\n",
      "  batch 201 loss: 9.357688912373873e-05\n",
      "  batch 301 loss: 8.698968185512968e-05\n",
      "  batch 401 loss: 8.171307659097238e-05\n",
      "LOSS train 8.750750922796989e-05 valid 0.00020249855879228562\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.8897635638713838e-06\n",
      "  batch 101 loss: 7.790742517101989e-05\n",
      "  batch 201 loss: 7.44094209130708e-05\n",
      "  batch 301 loss: 6.89787245039497e-05\n",
      "  batch 401 loss: 6.552028866735782e-05\n",
      "LOSS train 7.030815632286836e-05 valid 0.00017103982099797577\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.4320834199897945e-06\n",
      "  batch 101 loss: 6.464473522896697e-05\n",
      "  batch 201 loss: 6.255824300069435e-05\n",
      "  batch 301 loss: 5.939382146266325e-05\n",
      "  batch 401 loss: 5.815640201603856e-05\n",
      "LOSS train 6.049054695594274e-05 valid 0.00015776025247760117\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.232735714642331e-06\n",
      "  batch 101 loss: 6.012877848519338e-05\n",
      "  batch 201 loss: 5.938285051229286e-05\n",
      "  batch 301 loss: 5.786598502936613e-05\n",
      "  batch 401 loss: 5.826240677322403e-05\n",
      "LOSS train 5.875145668292709e-05 valid 0.00015525994240306318\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 2.194699191022664e-06\n",
      "  batch 101 loss: 6.135219288069039e-05\n",
      "  batch 201 loss: 6.0770646861669775e-05\n",
      "  batch 301 loss: 5.937587674083033e-05\n",
      "  batch 401 loss: 5.959031016516292e-05\n",
      "LOSS train 6.033985517412093e-05 valid 0.00012315217463765293\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.687523617874831e-06\n",
      "  batch 101 loss: 5.797100247264097e-05\n",
      "  batch 201 loss: 5.59539172309087e-05\n",
      "  batch 301 loss: 5.356514280038027e-05\n",
      "  batch 401 loss: 5.217379247142162e-05\n",
      "LOSS train 5.471965143804864e-05 valid 8.147953485604376e-05\n",
      "ObjectiveEstimator_ANN_2hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=35, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=35, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=5, out_features=1, bias=True)\n",
      ") 0 2\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.0005411854758858681\n",
      "  batch 101 loss: 52.110577786304056\n",
      "  batch 201 loss: 0.005420869652552938\n",
      "  batch 301 loss: 7.270453810633626e-05\n",
      "  batch 401 loss: 7.442618844834214e-05\n",
      "LOSS train 11.76449658404005 valid 6.626349204452708e-05\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 5.417597276391461e-07\n",
      "  batch 101 loss: 7.814090331976332e-05\n",
      "  batch 201 loss: 8.060820340688224e-05\n",
      "  batch 301 loss: 8.306675074891245e-05\n",
      "  batch 401 loss: 8.641804271519504e-05\n",
      "LOSS train 8.2297445735102e-05 valid 7.963438838487491e-05\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 8.869759767549113e-07\n",
      "  batch 101 loss: 9.042736236324345e-05\n",
      "  batch 201 loss: 9.305530469646329e-05\n",
      "  batch 301 loss: 9.566831752863436e-05\n",
      "  batch 401 loss: 9.907309124628228e-05\n",
      "LOSS train 9.404768209443928e-05 valid 0.00011167561024194583\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 1.4946518058422953e-06\n",
      "  batch 101 loss: 0.00010280622294999376\n",
      "  batch 201 loss: 0.00010527996067366985\n",
      "  batch 301 loss: 0.00010684685265459848\n",
      "  batch 401 loss: 0.00010860902594629351\n",
      "LOSS train 0.0001041942682356424 valid 0.00016922374197747558\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 2.4050698266364636e-06\n",
      "  batch 101 loss: 0.00010973272931977363\n",
      "  batch 201 loss: 0.00011034840709498894\n",
      "  batch 301 loss: 0.00010810744214438728\n",
      "  batch 401 loss: 0.0001056070946458476\n",
      "LOSS train 0.00010574158909709107 valid 0.0002195771230617538\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.1315378146246078e-06\n",
      "  batch 101 loss: 0.00010230838055235835\n",
      "  batch 201 loss: 9.953285786934885e-05\n",
      "  batch 301 loss: 9.324076405960113e-05\n",
      "  batch 401 loss: 8.782355355151595e-05\n",
      "LOSS train 9.316888472084312e-05 valid 0.00021239128545857966\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.0303033418022094e-06\n",
      "  batch 101 loss: 8.335200912028994e-05\n",
      "  batch 201 loss: 7.953850453475298e-05\n",
      "  batch 301 loss: 7.344186042018919e-05\n",
      "  batch 401 loss: 6.927013688141414e-05\n",
      "LOSS train 7.474904697757182e-05 valid 0.00017847474373411387\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 2.5419340818189085e-06\n",
      "  batch 101 loss: 6.745663334527308e-05\n",
      "  batch 201 loss: 6.491167840067647e-05\n",
      "  batch 301 loss: 6.112949398016099e-05\n",
      "  batch 401 loss: 5.930984013737372e-05\n",
      "LOSS train 6.233366134605386e-05 valid 0.00015944351616781205\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 2.2582469682674853e-06\n",
      "  batch 101 loss: 6.057070053941516e-05\n",
      "  batch 201 loss: 5.951572737870947e-05\n",
      "  batch 301 loss: 5.7653224706086805e-05\n",
      "  batch 401 loss: 5.770836723627326e-05\n",
      "LOSS train 5.857028594449264e-05 valid 0.00015664765669498593\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 2.215831045759842e-06\n",
      "  batch 101 loss: 6.089204327793141e-05\n",
      "  batch 201 loss: 6.0480022322053626e-05\n",
      "  batch 301 loss: 5.927331673149183e-05\n",
      "  batch 401 loss: 5.983214893149125e-05\n",
      "LOSS train 6.0198218533910825e-05 valid 0.00013605380081571639\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 1.896075700642541e-06\n",
      "  batch 101 loss: 5.9968286705895935e-05\n",
      "  batch 201 loss: 5.8233287100790674e-05\n",
      "  batch 301 loss: 5.5934935948016576e-05\n",
      "  batch 401 loss: 5.470558804290704e-05\n",
      "LOSS train 5.7057080799730526e-05 valid 8.842821989674121e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 1.069592108251527e-06\n",
      "  batch 101 loss: 5.0156380162889035e-05\n",
      "  batch 201 loss: 4.8604306836637076e-05\n",
      "  batch 301 loss: 4.7173177447916716e-05\n",
      "  batch 401 loss: 4.672830459213628e-05\n",
      "LOSS train 4.806839474221076e-05 valid 7.324596663238481e-05\n",
      "ObjectiveEstimator_ANN_3hidden_layer(\n",
      "  (hidden_layer1): Linear(in_features=1227, out_features=306, bias=True)\n",
      "  (hidden_layer2): Linear(in_features=306, out_features=76, bias=True)\n",
      "  (hidden_layer3): Linear(in_features=76, out_features=19, bias=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (output_layer): Linear(in_features=19, out_features=1, bias=True)\n",
      ") 0 3\n",
      "EPOCH 1:\n",
      "  batch 1 loss: 0.00018410505726933478\n",
      "  batch 101 loss: 29704.42122349481\n",
      "  batch 201 loss: 0.5951042330265045\n",
      "  batch 301 loss: 0.45093079417943954\n",
      "  batch 401 loss: 0.31743222326040266\n",
      "LOSS train 6705.617103310451 valid 0.21488498151302338\n",
      "EPOCH 2:\n",
      "  batch 1 loss: 0.002195597141981125\n",
      "  batch 101 loss: 0.1713969035446644\n",
      "  batch 201 loss: 0.10310536682605743\n",
      "  batch 301 loss: 0.058332695551216604\n",
      "  batch 401 loss: 0.031024205330759287\n",
      "LOSS train 0.08438701052260722 valid 0.01605634018778801\n",
      "EPOCH 3:\n",
      "  batch 1 loss: 0.000173071026802063\n",
      "  batch 101 loss: 0.011355886738747357\n",
      "  batch 201 loss: 0.005212100136559456\n",
      "  batch 301 loss: 0.002266102729481645\n",
      "  batch 401 loss: 0.000943103743629763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS train 0.004546416352659887 valid 0.00040020482265390456\n",
      "EPOCH 4:\n",
      "  batch 1 loss: 5.530298221856356e-06\n",
      "  batch 101 loss: 0.0002814434876927407\n",
      "  batch 201 loss: 0.00013764324290605146\n",
      "  batch 301 loss: 8.699310115844128e-05\n",
      "  batch 401 loss: 7.030992115687695e-05\n",
      "LOSS train 0.00013784978118380276 valid 6.372108327923343e-05\n",
      "EPOCH 5:\n",
      "  batch 1 loss: 4.4543081457959487e-07\n",
      "  batch 101 loss: 6.47212098010641e-05\n",
      "  batch 201 loss: 6.358291280776029e-05\n",
      "  batch 301 loss: 6.300536519120215e-05\n",
      "  batch 401 loss: 6.328601513814646e-05\n",
      "LOSS train 6.408073409404248e-05 valid 6.164169462863356e-05\n",
      "EPOCH 6:\n",
      "  batch 1 loss: 3.053294494748116e-07\n",
      "  batch 101 loss: 6.328790079351165e-05\n",
      "  batch 201 loss: 6.346885623315757e-05\n",
      "  batch 301 loss: 6.332474093142082e-05\n",
      "  batch 401 loss: 6.37891521819256e-05\n",
      "LOSS train 6.39374443822205e-05 valid 6.166748062241822e-05\n",
      "EPOCH 7:\n",
      "  batch 1 loss: 3.097015360253863e-07\n",
      "  batch 101 loss: 6.389355520695972e-05\n",
      "  batch 201 loss: 6.410621837858343e-05\n",
      "  batch 301 loss: 6.400794026376389e-05\n",
      "  batch 401 loss: 6.451536472013687e-05\n",
      "LOSS train 6.459634718930931e-05 valid 6.17418481851928e-05\n",
      "EPOCH 8:\n",
      "  batch 1 loss: 3.2021238439483567e-07\n",
      "  batch 101 loss: 6.469913321780041e-05\n",
      "  batch 201 loss: 6.492235034784244e-05\n",
      "  batch 301 loss: 6.487533086328767e-05\n",
      "  batch 401 loss: 6.543736131789046e-05\n",
      "LOSS train 6.544291969500672e-05 valid 6.18642516201362e-05\n",
      "EPOCH 9:\n",
      "  batch 1 loss: 3.339599425089546e-07\n",
      "  batch 101 loss: 6.571982495188421e-05\n",
      "  batch 201 loss: 6.5960849124167e-05\n",
      "  batch 301 loss: 6.598148188459163e-05\n",
      "  batch 401 loss: 6.66175572314387e-05\n",
      "LOSS train 6.652041837682371e-05 valid 6.206863326951861e-05\n",
      "EPOCH 10:\n",
      "  batch 1 loss: 3.522740735206753e-07\n",
      "  batch 101 loss: 6.70241521220305e-05\n",
      "  batch 201 loss: 6.729684352649201e-05\n",
      "  batch 301 loss: 6.740488056493632e-05\n",
      "  batch 401 loss: 6.814189564465777e-05\n",
      "LOSS train 6.790388439230503e-05 valid 6.241800292627886e-05\n",
      "EPOCH 11:\n",
      "  batch 1 loss: 3.7729878386016937e-07\n",
      "  batch 101 loss: 6.870633502330748e-05\n",
      "  batch 201 loss: 6.903376288391883e-05\n",
      "  batch 301 loss: 6.92529288608057e-05\n",
      "  batch 401 loss: 7.012830926214519e-05\n",
      "LOSS train 6.969568657868609e-05 valid 6.303163536358625e-05\n",
      "EPOCH 12:\n",
      "  batch 1 loss: 4.124824408791028e-07\n",
      "  batch 101 loss: 7.089684074344404e-05\n",
      "  batch 201 loss: 7.131395812393749e-05\n",
      "  batch 301 loss: 7.167496771216975e-05\n",
      "  batch 401 loss: 7.27395239755424e-05\n",
      "LOSS train 7.2036232691072e-05 valid 6.414251402020454e-05\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01*4**i for i in range(3)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [4,8,12]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,1,2,3]\n",
    "dors = [0]#,0.1,0.2,0.4]\n",
    "results = pd.DataFrame()\n",
    "folder_to_save = \"RTS24_AC_12w\"\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            for dor in dors:\n",
    "                m = create_model(nb_hidden,d_ft_in['train'].shape[1],dropout_ratio= dor)\n",
    "                if dor is None:\n",
    "                    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "                else:                 \n",
    "                    m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr_{dor}dor\"\n",
    "                optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "                train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,folder_to_save)\n",
    "\n",
    "                saved_models = dict()\n",
    "\n",
    "                for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                    path = f\"trained_models/{folder_to_save}/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "\n",
    "                    model = m\n",
    "                    m.load_state_dict(torch.load(path))\n",
    "                    m.eval()\n",
    "\n",
    "                    test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                    test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "\n",
    "                    train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                    train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "\n",
    "                    validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                    validation_loss = loss_fn(validation_prediction,d_ft_out[\"val\"])\n",
    "\n",
    "                    if mt == \"min_val\": \n",
    "                        min_val = True\n",
    "                    else: \n",
    "                        min_val = False\n",
    "\n",
    "                    r = pd.DataFrame({\"Model_type\": nb_hidden,\n",
    "                                      \"Min_val\":min_val,\n",
    "                                      \"Epochs\": nb_e,\n",
    "                                      \"Lr\":lr,\n",
    "                                      \"Dor\": dor,\n",
    "                                      \"Tr_l\":train_loss.item(),\n",
    "                                      \"Te_l\":test_loss.item(),\n",
    "                                      \"V_l\": validation_loss.item()}\n",
    "                                     ,index = [i]\n",
    "                    )\n",
    "                    i+=1\n",
    "                    results = pd.concat([results,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (results.Model_type == 1) &  (results.Min_val ==True) & (results.Epochs !=8)\n",
    "results[f].boxplot(column = [\"Te_l\", \"Tr_l\",\"V_l\"],by = [\"Dor\"],layout = (3,1),sharey = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9088ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\"Loss_results_csv/10_exec_Hyperparam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d50dc6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epochs', ylabel='Te_l'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = (results.Dor ==0) &(results.Model_type ==3)&(results.Epochs >=8)  #& (results.Min_val == True)  \n",
    "sns.boxplot(x = \"Epochs\", y = \"Te_l\",data=results[f],hue = \"Lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04f926be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epochs', ylabel='Te_l'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = (results.Lr ==0.01)#(results.Dor ==0) #& (results.Min_val == True)  \n",
    "sns.boxplot(x = \"Epochs\", y = \"Te_l\",data=results[f],hue = \"Model_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbb2ea",
   "metadata": {},
   "source": [
    "## Old code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01*4**i for i in range(3)]\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "nbs_e = [2,4]#,4,8]\n",
    "i=0\n",
    "nbs_hidden = [0,1,2,3]\n",
    "results = pd.DataFrame()\n",
    "for nb_e in nbs_e:\n",
    "    for lr in learning_rates:\n",
    "        for nb_hidden in nbs_hidden: \n",
    "            m = create_model(nb_hidden,input_size=d_ft_in['train'].shape[1])\n",
    "            m_name = f\"OE_{nb_hidden}h_{nb_e}e_{lr}lr\"\n",
    "            optimizer = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            train_loss = training_methods.train_multiple_epochs(nb_e,m,training_loader,validation_loader,loss_fn,optimizer,m_name,True)\n",
    "            \n",
    "            saved_models = dict()\n",
    "            \n",
    "            for mt in [\"min_val\",\"all_epochs\"]:\n",
    "                path = f\"trained_models/{mt}/model_{m_name}.pth\"\n",
    "\n",
    "                \n",
    "                model = m\n",
    "                m.load_state_dict(torch.load(path))\n",
    "                m.eval()\n",
    "\n",
    "                test_predictions = m(d_ft_in[\"test\"].float())\n",
    "                test_loss = loss_fn(test_predictions,d_ft_out[\"test\"])\n",
    "                \n",
    "                train_predictions = m(d_ft_in[\"train\"].float())\n",
    "                train_loss = loss_fn(train_predictions,d_ft_out[\"train\"])\n",
    "                \n",
    "                validation_prediction = m(d_ft_in[\"val\"].float())\n",
    "                validation_loss = loss_fn(validation_prediction,d_ft_out[\"val\"])\n",
    "\n",
    "                if mt == \"min_val\": \n",
    "                    min_val = True\n",
    "                else: \n",
    "                    min_val = False\n",
    "\n",
    "                r = pd.DataFrame({\"Model_type\": nb_hidden,\"Min_val\":min_val,\"Epochs\": nb_e,\"Lr\":lr, \"Tr_l\":train_loss.item(),\"Te_l\":test_loss.item(),\"V_l\": validation_loss.item()},index = [i]\n",
    "                )\n",
    "                i+=1\n",
    "                results = pd.concat([results,r])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
